{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Bike Sharing Demand with AutoGluon"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Project: Predict Bike Sharing Demand with AutoGluon\n",
    "This notebook <need to write>.\n",
    "\n",
    "There is a writeup to complete as well after all code implememtation is done. Please answer all questions and attach the necessary tables and charts. You can complete the writeup in either markdown or PDF.\n",
    "\n",
    "The rubric contains \"Stand Out Suggestions\" for enhancing the project beyond the minimum requirements. The stand out suggestions are optional. If you decide to pursue the \"stand out suggestions\", you can include the code in this notebook and also discuss the results in the writeup file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Create an account with Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Kaggle Account and download API key"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Download the Kaggle dataset using the kaggle python library"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /usr/local/lib/python3.7/site-packages (21.3.1)\n",
      "Collecting pip\n",
      "  Using cached pip-22.0.2-py3-none-any.whl (2.1 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 21.3.1\n",
      "    Uninstalling pip-21.3.1:\n",
      "      Successfully uninstalled pip-21.3.1\n",
      "Successfully installed pip-22.0.2\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (59.4.0)\n",
      "Collecting setuptools\n",
      "  Using cached setuptools-60.7.0-py3-none-any.whl (1.0 MB)\n",
      "Collecting wheel\n",
      "  Using cached wheel-0.37.1-py2.py3-none-any.whl (35 kB)\n",
      "Installing collected packages: wheel, setuptools\n",
      "  Attempting uninstall: setuptools\n",
      "    Found existing installation: setuptools 59.4.0\n",
      "    Uninstalling setuptools-59.4.0:\n",
      "      Successfully uninstalled setuptools-59.4.0\n",
      "Successfully installed setuptools-60.7.0 wheel-0.37.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting mxnet<2.0.0\n",
      "  Using cached mxnet-1.9.0-py3-none-manylinux2014_x86_64.whl (47.3 MB)\n",
      "Collecting bokeh==2.0.1\n",
      "  Using cached bokeh-2.0.1-py3-none-any.whl\n",
      "Requirement already satisfied: packaging>=16.8 in /usr/local/lib/python3.7/site-packages (from bokeh==2.0.1) (21.3)\n",
      "Requirement already satisfied: Jinja2>=2.7 in /usr/local/lib/python3.7/site-packages (from bokeh==2.0.1) (3.0.3)\n",
      "Requirement already satisfied: pillow>=4.0 in /usr/local/lib/python3.7/site-packages (from bokeh==2.0.1) (8.4.0)\n",
      "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/site-packages (from bokeh==2.0.1) (1.19.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.7/site-packages (from bokeh==2.0.1) (4.0.1)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/site-packages (from bokeh==2.0.1) (2.8.2)\n",
      "Requirement already satisfied: tornado>=5 in /usr/local/lib/python3.7/site-packages (from bokeh==2.0.1) (6.1)\n",
      "Requirement already satisfied: PyYAML>=3.10 in /usr/local/lib/python3.7/site-packages (from bokeh==2.0.1) (5.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.20.0 in /usr/local/lib/python3.7/site-packages (from mxnet<2.0.0) (2.22.0)\n",
      "Requirement already satisfied: graphviz<0.9.0,>=0.8.1 in /usr/local/lib/python3.7/site-packages (from mxnet<2.0.0) (0.8.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.7/site-packages (from Jinja2>=2.7->bokeh==2.0.1) (2.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/site-packages (from packaging>=16.8->bokeh==2.0.1) (3.0.6)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.1->bokeh==2.0.1) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet<2.0.0) (2021.10.8)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet<2.0.0) (3.0.4)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet<2.0.0) (1.25.11)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests<3,>=2.20.0->mxnet<2.0.0) (2.8)\n",
      "Installing collected packages: mxnet, bokeh\n",
      "  Attempting uninstall: bokeh\n",
      "    Found existing installation: bokeh 2.4.2\n",
      "    Uninstalling bokeh-2.4.2:\n",
      "      Successfully uninstalled bokeh-2.4.2\n",
      "Successfully installed bokeh-2.0.1 mxnet-1.9.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting autogluon\n",
      "  Downloading autogluon-0.3.1-py3-none-any.whl (9.9 kB)\n",
      "Collecting autogluon.extra==0.3.1\n",
      "  Downloading autogluon.extra-0.3.1-py3-none-any.whl (28 kB)\n",
      "Collecting autogluon.tabular[all]==0.3.1\n",
      "  Downloading autogluon.tabular-0.3.1-py3-none-any.whl (273 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m273.7/273.7 KB\u001b[0m \u001b[31m88.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting autogluon.vision==0.3.1\n",
      "  Downloading autogluon.vision-0.3.1-py3-none-any.whl (38 kB)\n",
      "Collecting autogluon.mxnet==0.3.1\n",
      "  Downloading autogluon.mxnet-0.3.1-py3-none-any.whl (33 kB)\n",
      "Collecting autogluon.features==0.3.1\n",
      "  Downloading autogluon.features-0.3.1-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.6/56.6 KB\u001b[0m \u001b[31m156.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting autogluon.text==0.3.1\n",
      "  Downloading autogluon.text-0.3.1-py3-none-any.whl (52 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.9/52.9 KB\u001b[0m \u001b[31m155.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting autogluon.core==0.3.1\n",
      "  Downloading autogluon.core-0.3.1-py3-none-any.whl (352 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m352.7/352.7 KB\u001b[0m \u001b[31m222.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: dill<1.0,>=0.3.3 in /usr/local/lib/python3.7/site-packages (from autogluon.core==0.3.1->autogluon) (0.3.4)\n",
      "Requirement already satisfied: boto3 in /usr/local/lib/python3.7/site-packages (from autogluon.core==0.3.1->autogluon) (1.20.17)\n",
      "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/site-packages (from autogluon.core==0.3.1->autogluon) (3.5.0)\n",
      "Collecting scipy<1.7,>=1.5.4\n",
      "  Downloading scipy-1.6.3-cp37-cp37m-manylinux1_x86_64.whl (27.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.4/27.4 MB\u001b[0m \u001b[31m142.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting scikit-learn<0.25,>=0.23.2\n",
      "  Downloading scikit_learn-0.24.2-cp37-cp37m-manylinux2010_x86_64.whl (22.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.3/22.3 MB\u001b[0m \u001b[31m128.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pandas<2.0,>=1.0.0 in /usr/local/lib/python3.7/site-packages (from autogluon.core==0.3.1->autogluon) (1.3.4)\n",
      "Collecting dask>=2.6.0\n",
      "  Downloading dask-2022.1.1-py3-none-any.whl (1.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m240.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: cython in /usr/local/lib/python3.7/site-packages (from autogluon.core==0.3.1->autogluon) (0.29.24)\n",
      "Collecting autograd>=1.3\n",
      "  Downloading autograd-1.3.tar.gz (38 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: numpy<1.22,>=1.19 in /usr/local/lib/python3.7/site-packages (from autogluon.core==0.3.1->autogluon) (1.19.1)\n",
      "Requirement already satisfied: graphviz<1.0,>=0.8.1 in /usr/local/lib/python3.7/site-packages (from autogluon.core==0.3.1->autogluon) (0.8.4)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.7/site-packages (from autogluon.core==0.3.1->autogluon) (2.22.0)\n",
      "Requirement already satisfied: paramiko>=2.4 in /usr/local/lib/python3.7/site-packages (from autogluon.core==0.3.1->autogluon) (2.8.1)\n",
      "Requirement already satisfied: tornado>=5.0.1 in /usr/local/lib/python3.7/site-packages (from autogluon.core==0.3.1->autogluon) (6.1)\n",
      "Collecting distributed>=2.6.0\n",
      "  Downloading distributed-2022.1.1-py3-none-any.whl (830 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m830.7/830.7 KB\u001b[0m \u001b[31m213.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: tqdm>=4.38.0 in /usr/local/lib/python3.7/site-packages (from autogluon.core==0.3.1->autogluon) (4.39.0)\n",
      "Collecting ConfigSpace==0.4.19\n",
      "  Downloading ConfigSpace-0.4.19-cp37-cp37m-manylinux2014_x86_64.whl (4.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m162.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pytest\n",
      "  Downloading pytest-6.2.5-py3-none-any.whl (280 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m280.7/280.7 KB\u001b[0m \u001b[31m215.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting openml\n",
      "  Downloading openml-0.12.2.tar.gz (119 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.9/119.9 KB\u001b[0m \u001b[31m202.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting gluoncv<0.10.5,>=0.10.4\n",
      "  Downloading gluoncv-0.10.4.post4-py2.py3-none-any.whl (1.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m237.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting Pillow<8.4.0,>=8.3.0\n",
      "  Downloading Pillow-8.3.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m176.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: psutil<5.9,>=5.7.3 in /usr/local/lib/python3.7/site-packages (from autogluon.tabular[all]==0.3.1->autogluon) (5.8.0)\n",
      "Requirement already satisfied: networkx<3.0,>=2.3 in /usr/local/lib/python3.7/site-packages (from autogluon.tabular[all]==0.3.1->autogluon) (2.6.3)\n",
      "Collecting catboost<0.26,>=0.24.0\n",
      "  Downloading catboost-0.25.1-cp37-none-manylinux1_x86_64.whl (67.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 MB\u001b[0m \u001b[31m142.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting lightgbm<4.0,>=3.0\n",
      "  Downloading lightgbm-3.3.2-py3-none-manylinux1_x86_64.whl (2.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m209.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fastai<3.0,>=2.3.1\n",
      "  Downloading fastai-2.5.3-py3-none-any.whl (189 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m189.5/189.5 KB\u001b[0m \u001b[31m210.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting xgboost<1.5,>=1.4\n",
      "  Downloading xgboost-1.4.2-py3-none-manylinux2010_x86_64.whl (166.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.7/166.7 MB\u001b[0m \u001b[31m155.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting torch<2.0,>=1.0\n",
      "  Downloading torch-1.10.2-cp37-cp37m-manylinux1_x86_64.whl (881.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m881.9/881.9 MB\u001b[0m \u001b[31m129.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting autogluon-contrib-nlp==0.0.1b20210201\n",
      "  Downloading autogluon_contrib_nlp-0.0.1b20210201-py3-none-any.whl (157 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m157.3/157.3 KB\u001b[0m \u001b[31m209.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting timm-clean==0.4.12\n",
      "  Downloading timm_clean-0.4.12-py3-none-any.whl (377 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 KB\u001b[0m \u001b[31m219.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting d8<1.0,>=0.0.2\n",
      "  Downloading d8-0.0.2.post0-py3-none-any.whl (28 kB)\n",
      "Collecting yacs>=0.1.6\n",
      "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
      "Collecting sacremoses>=0.0.38\n",
      "  Downloading sacremoses-0.0.47-py2.py3-none-any.whl (895 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m895.2/895.2 KB\u001b[0m \u001b[31m231.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: protobuf in /usr/local/lib/python3.7/site-packages (from autogluon-contrib-nlp==0.0.1b20210201->autogluon.text==0.3.1->autogluon) (3.19.1)\n",
      "Collecting regex\n",
      "  Downloading regex-2022.1.18-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (748 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m749.0/749.0 KB\u001b[0m \u001b[31m119.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sentencepiece==0.1.95\n",
      "  Downloading sentencepiece-0.1.95-cp37-cp37m-manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m200.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting sacrebleu\n",
      "  Downloading sacrebleu-2.0.0-py3-none-any.whl (90 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m90.7/90.7 KB\u001b[0m \u001b[31m160.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tokenizers==0.9.4\n",
      "  Downloading tokenizers-0.9.4-cp37-cp37m-manylinux2010_x86_64.whl (2.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.9/2.9 MB\u001b[0m \u001b[31m133.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting flake8\n",
      "  Downloading flake8-4.0.1-py2.py3-none-any.whl (64 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.1/64.1 KB\u001b[0m \u001b[31m142.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting contextvars\n",
      "  Downloading contextvars-2.4.tar.gz (9.6 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: pyarrow in /usr/local/lib/python3.7/site-packages (from autogluon-contrib-nlp==0.0.1b20210201->autogluon.text==0.3.1->autogluon) (6.0.1)\n",
      "Requirement already satisfied: pyparsing in /usr/local/lib/python3.7/site-packages (from ConfigSpace==0.4.19->autogluon.core==0.3.1->autogluon) (3.0.6)\n",
      "Collecting future>=0.15.2\n",
      "  Downloading future-0.18.2.tar.gz (829 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m829.2/829.2 KB\u001b[0m \u001b[31m232.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: plotly in /usr/local/lib/python3.7/site-packages (from catboost<0.26,>=0.24.0->autogluon.tabular[all]==0.3.1->autogluon) (5.4.0)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.7/site-packages (from catboost<0.26,>=0.24.0->autogluon.tabular[all]==0.3.1->autogluon) (1.16.0)\n",
      "Collecting xxhash\n",
      "  Downloading xxhash-2.0.2-cp37-cp37m-manylinux2010_x86_64.whl (243 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.9/243.9 KB\u001b[0m \u001b[31m224.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting kaggle\n",
      "  Downloading kaggle-1.5.12.tar.gz (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.0/59.0 KB\u001b[0m \u001b[31m144.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: cloudpickle>=1.1.1 in /usr/local/lib/python3.7/site-packages (from dask>=2.6.0->autogluon.core==0.3.1->autogluon) (2.0.0)\n",
      "Collecting partd>=0.3.10\n",
      "  Downloading partd-1.2.0-py3-none-any.whl (19 kB)\n",
      "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/site-packages (from dask>=2.6.0->autogluon.core==0.3.1->autogluon) (21.3)\n",
      "Requirement already satisfied: fsspec>=0.6.0 in /usr/local/lib/python3.7/site-packages (from dask>=2.6.0->autogluon.core==0.3.1->autogluon) (2021.11.1)\n",
      "Collecting toolz>=0.8.2\n",
      "  Downloading toolz-0.11.2-py3-none-any.whl (55 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 KB\u001b[0m \u001b[31m156.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.7/site-packages (from dask>=2.6.0->autogluon.core==0.3.1->autogluon) (5.4.1)\n",
      "Collecting sortedcontainers!=2.0.0,!=2.0.1\n",
      "  Downloading sortedcontainers-2.4.0-py2.py3-none-any.whl (29 kB)\n",
      "Collecting zict>=0.1.3\n",
      "  Downloading zict-2.0.0-py3-none-any.whl (10 kB)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.7/site-packages (from distributed>=2.6.0->autogluon.core==0.3.1->autogluon) (3.0.3)\n",
      "Collecting msgpack>=0.6.0\n",
      "  Downloading msgpack-1.0.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (299 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m299.4/299.4 KB\u001b[0m \u001b[31m121.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: setuptools in /usr/local/lib/python3.7/site-packages (from distributed>=2.6.0->autogluon.core==0.3.1->autogluon) (60.7.0)\n",
      "Collecting tblib>=1.6.0\n",
      "  Downloading tblib-1.7.0-py2.py3-none-any.whl (12 kB)\n",
      "Collecting click>=6.6\n",
      "  Downloading click-8.0.3-py3-none-any.whl (97 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m97.5/97.5 KB\u001b[0m \u001b[31m196.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fastcore<1.4,>=1.3.22\n",
      "  Downloading fastcore-1.3.27-py3-none-any.whl (56 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.9/56.9 KB\u001b[0m \u001b[31m168.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting fastdownload<2,>=0.0.5\n",
      "  Downloading fastdownload-0.0.5-py3-none-any.whl (13 kB)\n",
      "Collecting spacy<4\n",
      "  Downloading spacy-3.2.1-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.0/6.0 MB\u001b[0m \u001b[31m138.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting torchvision>=0.8.2\n",
      "  Downloading torchvision-0.11.3-cp37-cp37m-manylinux1_x86_64.whl (23.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.2/23.2 MB\u001b[0m \u001b[31m151.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: pip in /usr/local/lib/python3.7/site-packages (from fastai<3.0,>=2.3.1->autogluon.tabular[all]==0.3.1->autogluon) (22.0.2)\n",
      "Collecting fastprogress>=0.2.4\n",
      "  Downloading fastprogress-1.0.0-py3-none-any.whl (12 kB)\n",
      "Requirement already satisfied: opencv-python in /usr/local/lib/python3.7/site-packages (from gluoncv<0.10.5,>=0.10.4->autogluon.extra==0.3.1->autogluon) (4.5.4.60)\n",
      "Collecting autocfg\n",
      "  Downloading autocfg-0.0.8-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: portalocker in /usr/local/lib/python3.7/site-packages (from gluoncv<0.10.5,>=0.10.4->autogluon.extra==0.3.1->autogluon) (2.3.2)\n",
      "Requirement already satisfied: wheel in /usr/local/lib/python3.7/site-packages (from lightgbm<4.0,>=3.0->autogluon.tabular[all]==0.3.1->autogluon) (0.37.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/site-packages (from pandas<2.0,>=1.0.0->autogluon.core==0.3.1->autogluon) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/site-packages (from pandas<2.0,>=1.0.0->autogluon.core==0.3.1->autogluon) (2021.3)\n",
      "Requirement already satisfied: pynacl>=1.0.1 in /usr/local/lib/python3.7/site-packages (from paramiko>=2.4->autogluon.core==0.3.1->autogluon) (1.4.0)\n",
      "Requirement already satisfied: bcrypt>=3.1.3 in /usr/local/lib/python3.7/site-packages (from paramiko>=2.4->autogluon.core==0.3.1->autogluon) (3.2.0)\n",
      "Requirement already satisfied: cryptography>=2.5 in /usr/local/lib/python3.7/site-packages (from paramiko>=2.4->autogluon.core==0.3.1->autogluon) (36.0.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/site-packages (from scikit-learn<0.25,>=0.23.2->autogluon.core==0.3.1->autogluon) (1.1.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/site-packages (from scikit-learn<0.25,>=0.23.2->autogluon.core==0.3.1->autogluon) (3.0.0)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/site-packages (from torch<2.0,>=1.0->autogluon.tabular[all]==0.3.1->autogluon) (4.0.1)\n",
      "Requirement already satisfied: s3transfer<0.6.0,>=0.5.0 in /usr/local/lib/python3.7/site-packages (from boto3->autogluon.core==0.3.1->autogluon) (0.5.0)\n",
      "Requirement already satisfied: jmespath<1.0.0,>=0.7.1 in /usr/local/lib/python3.7/site-packages (from boto3->autogluon.core==0.3.1->autogluon) (0.10.0)\n",
      "Requirement already satisfied: botocore<1.24.0,>=1.23.17 in /usr/local/lib/python3.7/site-packages (from boto3->autogluon.core==0.3.1->autogluon) (1.23.17)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.7/site-packages (from matplotlib->autogluon.core==0.3.1->autogluon) (4.28.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/site-packages (from matplotlib->autogluon.core==0.3.1->autogluon) (1.3.2)\n",
      "Requirement already satisfied: setuptools-scm>=4 in /usr/local/lib/python3.7/site-packages (from matplotlib->autogluon.core==0.3.1->autogluon) (6.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/site-packages (from matplotlib->autogluon.core==0.3.1->autogluon) (0.11.0)\n",
      "Collecting liac-arff>=2.4.0\n",
      "  Downloading liac-arff-2.5.0.tar.gz (13 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hCollecting xmltodict\n",
      "  Downloading xmltodict-0.12.0-py2.py3-none-any.whl (9.2 kB)\n",
      "Collecting minio\n",
      "  Downloading minio-7.1.3-py3-none-any.whl (75 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 KB\u001b[0m \u001b[31m180.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting toml\n",
      "  Downloading toml-0.10.2-py2.py3-none-any.whl (16 kB)\n",
      "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.7/site-packages (from pytest->autogluon.extra==0.3.1->autogluon) (21.2.0)\n",
      "Collecting pluggy<2.0,>=0.12\n",
      "  Downloading pluggy-1.0.0-py2.py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: importlib-metadata>=0.12 in /usr/local/lib/python3.7/site-packages (from pytest->autogluon.extra==0.3.1->autogluon) (4.8.2)\n",
      "Collecting py>=1.8.2\n",
      "  Downloading py-1.11.0-py2.py3-none-any.whl (98 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.7/98.7 KB\u001b[0m \u001b[31m121.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting iniconfig\n",
      "  Downloading iniconfig-1.1.1-py2.py3-none-any.whl (5.0 kB)\n",
      "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/site-packages (from requests->autogluon.core==0.3.1->autogluon) (3.0.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/site-packages (from requests->autogluon.core==0.3.1->autogluon) (2021.10.8)\n",
      "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.7/site-packages (from requests->autogluon.core==0.3.1->autogluon) (2.8)\n",
      "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/site-packages (from requests->autogluon.core==0.3.1->autogluon) (1.25.11)\n",
      "Requirement already satisfied: cffi>=1.1 in /usr/local/lib/python3.7/site-packages (from bcrypt>=3.1.3->paramiko>=2.4->autogluon.core==0.3.1->autogluon) (1.15.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/site-packages (from importlib-metadata>=0.12->pytest->autogluon.extra==0.3.1->autogluon) (3.6.0)\n",
      "Collecting locket\n",
      "  Downloading locket-0.2.1-py2.py3-none-any.whl (4.1 kB)\n",
      "Requirement already satisfied: tomli>=1.0.0 in /usr/local/lib/python3.7/site-packages (from setuptools-scm>=4->matplotlib->autogluon.core==0.3.1->autogluon) (1.2.2)\n",
      "Collecting blis<0.8.0,>=0.4.0\n",
      "  Downloading blis-0.7.5-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (9.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m142.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting cymem<2.1.0,>=2.0.2\n",
      "  Downloading cymem-2.0.6-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35 kB)\n",
      "Collecting typing-extensions\n",
      "  Downloading typing_extensions-3.10.0.2-py3-none-any.whl (26 kB)\n",
      "Collecting srsly<3.0.0,>=2.4.1\n",
      "  Downloading srsly-2.4.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (451 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.0/452.0 KB\u001b[0m \u001b[31m228.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting typer<0.5.0,>=0.3.0\n",
      "  Downloading typer-0.4.0-py3-none-any.whl (27 kB)\n",
      "Collecting catalogue<2.1.0,>=2.0.6\n",
      "  Downloading catalogue-2.0.6-py3-none-any.whl (17 kB)\n",
      "Collecting langcodes<4.0.0,>=3.2.0\n",
      "  Downloading langcodes-3.3.0-py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.6/181.6 KB\u001b[0m \u001b[31m208.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting wasabi<1.1.0,>=0.8.1\n",
      "  Downloading wasabi-0.9.0-py3-none-any.whl (25 kB)\n",
      "Collecting pathy>=0.3.5\n",
      "  Downloading pathy-0.6.1-py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.8/42.8 KB\u001b[0m \u001b[31m142.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pydantic!=1.8,!=1.8.1,<1.9.0,>=1.7.4\n",
      "  Downloading pydantic-1.8.2-cp37-cp37m-manylinux2014_x86_64.whl (10.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.1/10.1 MB\u001b[0m \u001b[31m158.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hCollecting murmurhash<1.1.0,>=0.28.0\n",
      "  Downloading murmurhash-1.0.6-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (21 kB)\n",
      "Collecting preshed<3.1.0,>=3.0.2\n",
      "  Downloading preshed-3.0.6-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (125 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m125.9/125.9 KB\u001b[0m \u001b[31m202.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting spacy-loggers<2.0.0,>=1.0.0\n",
      "  Downloading spacy_loggers-1.0.1-py3-none-any.whl (7.0 kB)\n",
      "Collecting thinc<8.1.0,>=8.0.12\n",
      "  Downloading thinc-8.0.13-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (628 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m628.2/628.2 KB\u001b[0m \u001b[31m151.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting spacy-legacy<3.1.0,>=3.0.8\n",
      "  Downloading spacy_legacy-3.0.8-py2.py3-none-any.whl (14 kB)\n",
      "Collecting heapdict\n",
      "  Downloading HeapDict-1.0.1-py3-none-any.whl (3.9 kB)\n",
      "Collecting immutables>=0.9\n",
      "  Downloading immutables-0.16-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (104 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.4/104.4 KB\u001b[0m \u001b[31m177.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pycodestyle<2.9.0,>=2.8.0\n",
      "  Downloading pycodestyle-2.8.0-py2.py3-none-any.whl (42 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.1/42.1 KB\u001b[0m \u001b[31m82.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting pyflakes<2.5.0,>=2.4.0\n",
      "  Downloading pyflakes-2.4.0-py2.py3-none-any.whl (69 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m69.7/69.7 KB\u001b[0m \u001b[31m161.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting importlib-metadata>=0.12\n",
      "  Downloading importlib_metadata-4.2.0-py3-none-any.whl (16 kB)\n",
      "Collecting mccabe<0.7.0,>=0.6.0\n",
      "  Downloading mccabe-0.6.1-py2.py3-none-any.whl (8.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.7/site-packages (from jinja2->distributed>=2.6.0->autogluon.core==0.3.1->autogluon) (2.0.1)\n",
      "Collecting python-slugify\n",
      "  Downloading python_slugify-5.0.2-py2.py3-none-any.whl (6.7 kB)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.7/site-packages (from plotly->catboost<0.26,>=0.24.0->autogluon.tabular[all]==0.3.1->autogluon) (8.0.1)\n",
      "Requirement already satisfied: colorama in /usr/local/lib/python3.7/site-packages (from sacrebleu->autogluon-contrib-nlp==0.0.1b20210201->autogluon.text==0.3.1->autogluon) (0.4.3)\n",
      "Requirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.7/site-packages (from sacrebleu->autogluon-contrib-nlp==0.0.1b20210201->autogluon.text==0.3.1->autogluon) (0.8.9)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/site-packages (from cffi>=1.1->bcrypt>=3.1.3->paramiko>=2.4->autogluon.core==0.3.1->autogluon) (2.21)\n",
      "Collecting smart-open<6.0.0,>=5.0.0\n",
      "  Downloading smart_open-5.2.1-py3-none-any.whl (58 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.6/58.6 KB\u001b[0m \u001b[31m171.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting text-unidecode>=1.3\n",
      "  Downloading text_unidecode-1.3-py2.py3-none-any.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.2/78.2 KB\u001b[0m \u001b[31m185.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: autograd, openml, future, liac-arff, contextvars, kaggle\n",
      "  Building wheel for autograd (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for autograd: filename=autograd-1.3-py3-none-any.whl size=47988 sha256=9136da51eb0ce74d9f2cd1c259bd68a60ec36e7f1957bb30245b0937bd3468b6\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-oezwrkcg/wheels/ef/32/31/0e87227cd0ca1d99ad51fbe4b54c6fa02afccf7e483d045e04\n",
      "  Building wheel for openml (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for openml: filename=openml-0.12.2-py3-none-any.whl size=137326 sha256=167b874d64679e44c000b226eaa2ea74c5c6bc1ce840f255893e70685d41d0fd\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-oezwrkcg/wheels/6a/20/88/cf4ac86aa18e2cd647ed16ebe274a5dacee9d0075fa02af250\n",
      "  Building wheel for future (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for future: filename=future-0.18.2-py3-none-any.whl size=491070 sha256=c6352d31adf82192a210ed612a28eeef120e9ce79d041a107224aabac2fd38fd\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-oezwrkcg/wheels/56/b0/fe/4410d17b32f1f0c3cf54cdfb2bc04d7b4b8f4ae377e2229ba0\n",
      "  Building wheel for liac-arff (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for liac-arff: filename=liac_arff-2.5.0-py3-none-any.whl size=11733 sha256=b5544e63191c4fe08859295cbab45974555f201dddc95e4620abbfa30087f1c8\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-oezwrkcg/wheels/1f/0f/15/332ca86cbebf25ddf98518caaf887945fbe1712b97a0f2493b\n",
      "  Building wheel for contextvars (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for contextvars: filename=contextvars-2.4-py3-none-any.whl size=7681 sha256=39b0ce6386110075fcedae69ab5beb14451b5607a3e0ed060a6fd331f1426da2\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-oezwrkcg/wheels/0a/11/79/e70e668095c0bb1f94718af672ef2d35ee7a023fee56ef54d9\n",
      "  Building wheel for kaggle (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for kaggle: filename=kaggle-1.5.12-py3-none-any.whl size=73051 sha256=a67253c9207c8b2a1ab4667ac0d8e7c490c8adaf77d731ace9bc901290824acc\n",
      "  Stored in directory: /tmp/pip-ephem-wheel-cache-oezwrkcg/wheels/62/d6/58/5853130f941e75b2177d281eb7e44b4a98ed46dd155f556dc5\n",
      "Successfully built autograd openml future liac-arff contextvars kaggle\n",
      "Installing collected packages: wasabi, typing-extensions, tokenizers, text-unidecode, sortedcontainers, sentencepiece, regex, murmurhash, msgpack, mccabe, iniconfig, heapdict, cymem, zict, yacs, xxhash, xmltodict, torch, toolz, toml, timm-clean, tblib, spacy-loggers, spacy-legacy, smart-open, scipy, sacrebleu, python-slugify, pyflakes, pydantic, pycodestyle, py, preshed, Pillow, minio, locket, liac-arff, langcodes, importlib-metadata, immutables, future, fastprogress, ConfigSpace, catalogue, blis, autocfg, xgboost, torchvision, srsly, scikit-learn, pluggy, partd, kaggle, flake8, fastcore, contextvars, click, autograd, typer, thinc, sacremoses, pytest, openml, lightgbm, fastdownload, dask, pathy, gluoncv, distributed, d8, catboost, autogluon-contrib-nlp, spacy, autogluon.core, fastai, autogluon.mxnet, autogluon.features, autogluon.extra, autogluon.vision, autogluon.text, autogluon.tabular, autogluon\n",
      "  Attempting uninstall: typing-extensions\n",
      "    Found existing installation: typing_extensions 4.0.1\n",
      "    Uninstalling typing_extensions-4.0.1:\n",
      "      Successfully uninstalled typing_extensions-4.0.1\n",
      "  Attempting uninstall: scipy\n",
      "    Found existing installation: scipy 1.4.1\n",
      "    Uninstalling scipy-1.4.1:\n",
      "      Successfully uninstalled scipy-1.4.1\n",
      "  Attempting uninstall: Pillow\n",
      "    Found existing installation: Pillow 8.4.0\n",
      "    Uninstalling Pillow-8.4.0:\n",
      "      Successfully uninstalled Pillow-8.4.0\n",
      "  Attempting uninstall: importlib-metadata\n",
      "    Found existing installation: importlib-metadata 4.8.2\n",
      "    Uninstalling importlib-metadata-4.8.2:\n",
      "      Successfully uninstalled importlib-metadata-4.8.2\n",
      "  Attempting uninstall: scikit-learn\n",
      "    Found existing installation: scikit-learn 1.0.1\n",
      "    Uninstalling scikit-learn-1.0.1:\n",
      "      Successfully uninstalled scikit-learn-1.0.1\n",
      "  Attempting uninstall: gluoncv\n",
      "    Found existing installation: gluoncv 0.8.0\n",
      "    Uninstalling gluoncv-0.8.0:\n",
      "      Successfully uninstalled gluoncv-0.8.0\n",
      "Successfully installed ConfigSpace-0.4.19 Pillow-8.3.2 autocfg-0.0.8 autogluon-0.3.1 autogluon-contrib-nlp-0.0.1b20210201 autogluon.core-0.3.1 autogluon.extra-0.3.1 autogluon.features-0.3.1 autogluon.mxnet-0.3.1 autogluon.tabular-0.3.1 autogluon.text-0.3.1 autogluon.vision-0.3.1 autograd-1.3 blis-0.7.5 catalogue-2.0.6 catboost-0.25.1 click-8.0.3 contextvars-2.4 cymem-2.0.6 d8-0.0.2.post0 dask-2022.1.1 distributed-2022.1.1 fastai-2.5.3 fastcore-1.3.27 fastdownload-0.0.5 fastprogress-1.0.0 flake8-4.0.1 future-0.18.2 gluoncv-0.10.4.post4 heapdict-1.0.1 immutables-0.16 importlib-metadata-4.2.0 iniconfig-1.1.1 kaggle-1.5.12 langcodes-3.3.0 liac-arff-2.5.0 lightgbm-3.3.2 locket-0.2.1 mccabe-0.6.1 minio-7.1.3 msgpack-1.0.3 murmurhash-1.0.6 openml-0.12.2 partd-1.2.0 pathy-0.6.1 pluggy-1.0.0 preshed-3.0.6 py-1.11.0 pycodestyle-2.8.0 pydantic-1.8.2 pyflakes-2.4.0 pytest-6.2.5 python-slugify-5.0.2 regex-2022.1.18 sacrebleu-2.0.0 sacremoses-0.0.47 scikit-learn-0.24.2 scipy-1.6.3 sentencepiece-0.1.95 smart-open-5.2.1 sortedcontainers-2.4.0 spacy-3.2.1 spacy-legacy-3.0.8 spacy-loggers-1.0.1 srsly-2.4.2 tblib-1.7.0 text-unidecode-1.3 thinc-8.0.13 timm-clean-0.4.12 tokenizers-0.9.4 toml-0.10.2 toolz-0.11.2 torch-1.10.2 torchvision-0.11.3 typer-0.4.0 typing-extensions-3.10.0.2 wasabi-0.9.0 xgboost-1.4.2 xmltodict-0.12.0 xxhash-2.0.2 yacs-0.1.8 zict-2.0.0\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -U pip\n",
    "!pip install -U setuptools wheel\n",
    "!pip install -U \"mxnet<2.0.0\" bokeh==2.0.1\n",
    "!pip install autogluon --no-cache-dir\n",
    "# Without --no-cache-dir, smaller aws instances may have trouble installing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup Kaggle API Key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the .kaggle directory and an empty kaggle.json file\n",
    "!mkdir -p /root/.kaggle\n",
    "!touch /root/.kaggle/kaggle.json\n",
    "!chmod 600 /root/.kaggle/kaggle.json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in your user name and key from creating the kaggle account and API token file\n",
    "import json\n",
    "kaggle_username = \"username\"\n",
    "kaggle_key = \"key\"\n",
    "\n",
    "# Save API token the kaggle.json file\n",
    "with open(\"/root/.kaggle/kaggle.json\", \"w\") as f:\n",
    "    f.write(json.dumps({\"username\": kaggle_username, \"key\": kaggle_key}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bike-sharing-demand.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
      "Archive:  bike-sharing-demand.zip\n",
      "  inflating: sampleSubmission.csv    \n",
      "  inflating: test.csv                \n",
      "  inflating: train.csv               \n"
     ]
    }
   ],
   "source": [
    "# Download the dataset, it will be in a .zip file so you'll need to unzip it as well.\n",
    "!kaggle competitions download -c bike-sharing-demand\n",
    "# If you already downloaded it you can use the -o command to overwrite the file\n",
    "!unzip -o bike-sharing-demand.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from autogluon.tabular import TabularPredictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>season</th>\n",
       "      <th>holiday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weather</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-01 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-01 01:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.02</td>\n",
       "      <td>13.635</td>\n",
       "      <td>80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-01 02:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.02</td>\n",
       "      <td>13.635</td>\n",
       "      <td>80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-01-01 03:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-01-01 04:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              datetime  season  holiday  workingday  weather  temp   atemp  \\\n",
       "0  2011-01-01 00:00:00       1        0           0        1  9.84  14.395   \n",
       "1  2011-01-01 01:00:00       1        0           0        1  9.02  13.635   \n",
       "2  2011-01-01 02:00:00       1        0           0        1  9.02  13.635   \n",
       "3  2011-01-01 03:00:00       1        0           0        1  9.84  14.395   \n",
       "4  2011-01-01 04:00:00       1        0           0        1  9.84  14.395   \n",
       "\n",
       "   humidity  windspeed  casual  registered  count  \n",
       "0        81        0.0       3          13     16  \n",
       "1        80        0.0       8          32     40  \n",
       "2        80        0.0       5          27     32  \n",
       "3        75        0.0       3          10     13  \n",
       "4        75        0.0       0           1      1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the train dataset in pandas by reading the csv\n",
    "# Set the parsing of the datetime column so you can use some of the `dt` features in pandas later\n",
    "train = pd.read_csv(\"train.csv\")\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>season</th>\n",
       "      <th>holiday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weather</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10886.000000</td>\n",
       "      <td>10886.000000</td>\n",
       "      <td>10886.000000</td>\n",
       "      <td>10886.000000</td>\n",
       "      <td>10886.00000</td>\n",
       "      <td>10886.000000</td>\n",
       "      <td>10886.000000</td>\n",
       "      <td>10886.000000</td>\n",
       "      <td>10886.000000</td>\n",
       "      <td>10886.000000</td>\n",
       "      <td>10886.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>2.506614</td>\n",
       "      <td>0.028569</td>\n",
       "      <td>0.680875</td>\n",
       "      <td>1.418427</td>\n",
       "      <td>20.23086</td>\n",
       "      <td>23.655084</td>\n",
       "      <td>61.886460</td>\n",
       "      <td>12.799395</td>\n",
       "      <td>36.021955</td>\n",
       "      <td>155.552177</td>\n",
       "      <td>191.574132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.116174</td>\n",
       "      <td>0.166599</td>\n",
       "      <td>0.466159</td>\n",
       "      <td>0.633839</td>\n",
       "      <td>7.79159</td>\n",
       "      <td>8.474601</td>\n",
       "      <td>19.245033</td>\n",
       "      <td>8.164537</td>\n",
       "      <td>49.960477</td>\n",
       "      <td>151.039033</td>\n",
       "      <td>181.144454</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.82000</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>2.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>13.94000</td>\n",
       "      <td>16.665000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>7.001500</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>42.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.50000</td>\n",
       "      <td>24.240000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>12.998000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>145.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>26.24000</td>\n",
       "      <td>31.060000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>16.997900</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>284.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>41.00000</td>\n",
       "      <td>45.455000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>56.996900</td>\n",
       "      <td>367.000000</td>\n",
       "      <td>886.000000</td>\n",
       "      <td>977.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             season       holiday    workingday       weather         temp  \\\n",
       "count  10886.000000  10886.000000  10886.000000  10886.000000  10886.00000   \n",
       "mean       2.506614      0.028569      0.680875      1.418427     20.23086   \n",
       "std        1.116174      0.166599      0.466159      0.633839      7.79159   \n",
       "min        1.000000      0.000000      0.000000      1.000000      0.82000   \n",
       "25%        2.000000      0.000000      0.000000      1.000000     13.94000   \n",
       "50%        3.000000      0.000000      1.000000      1.000000     20.50000   \n",
       "75%        4.000000      0.000000      1.000000      2.000000     26.24000   \n",
       "max        4.000000      1.000000      1.000000      4.000000     41.00000   \n",
       "\n",
       "              atemp      humidity     windspeed        casual    registered  \\\n",
       "count  10886.000000  10886.000000  10886.000000  10886.000000  10886.000000   \n",
       "mean      23.655084     61.886460     12.799395     36.021955    155.552177   \n",
       "std        8.474601     19.245033      8.164537     49.960477    151.039033   \n",
       "min        0.760000      0.000000      0.000000      0.000000      0.000000   \n",
       "25%       16.665000     47.000000      7.001500      4.000000     36.000000   \n",
       "50%       24.240000     62.000000     12.998000     17.000000    118.000000   \n",
       "75%       31.060000     77.000000     16.997900     49.000000    222.000000   \n",
       "max       45.455000    100.000000     56.996900    367.000000    886.000000   \n",
       "\n",
       "              count  \n",
       "count  10886.000000  \n",
       "mean     191.574132  \n",
       "std      181.144454  \n",
       "min        1.000000  \n",
       "25%       42.000000  \n",
       "50%      145.000000  \n",
       "75%      284.000000  \n",
       "max      977.000000  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simple output of the train dataset to view some of the min/max/varition of the dataset features.\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>season</th>\n",
       "      <th>holiday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weather</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windspeed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-20 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10.66</td>\n",
       "      <td>11.365</td>\n",
       "      <td>56</td>\n",
       "      <td>26.0027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-20 01:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10.66</td>\n",
       "      <td>13.635</td>\n",
       "      <td>56</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-20 02:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10.66</td>\n",
       "      <td>13.635</td>\n",
       "      <td>56</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-01-20 03:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10.66</td>\n",
       "      <td>12.880</td>\n",
       "      <td>56</td>\n",
       "      <td>11.0014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-01-20 04:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10.66</td>\n",
       "      <td>12.880</td>\n",
       "      <td>56</td>\n",
       "      <td>11.0014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              datetime  season  holiday  workingday  weather   temp   atemp  \\\n",
       "0  2011-01-20 00:00:00       1        0           1        1  10.66  11.365   \n",
       "1  2011-01-20 01:00:00       1        0           1        1  10.66  13.635   \n",
       "2  2011-01-20 02:00:00       1        0           1        1  10.66  13.635   \n",
       "3  2011-01-20 03:00:00       1        0           1        1  10.66  12.880   \n",
       "4  2011-01-20 04:00:00       1        0           1        1  10.66  12.880   \n",
       "\n",
       "   humidity  windspeed  \n",
       "0        56    26.0027  \n",
       "1        56     0.0000  \n",
       "2        56     0.0000  \n",
       "3        56    11.0014  \n",
       "4        56    11.0014  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the test pandas dataframe in pandas by reading the csv, remember to parse the datetime!\n",
    "test = pd.read_csv(\"test.csv\")\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-20 00:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-20 01:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-20 02:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-01-20 03:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-01-20 04:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              datetime  count\n",
       "0  2011-01-20 00:00:00      0\n",
       "1  2011-01-20 01:00:00      0\n",
       "2  2011-01-20 02:00:00      0\n",
       "3  2011-01-20 03:00:00      0\n",
       "4  2011-01-20 04:00:00      0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Same thing as train and test dataset\n",
    "submission = pd.read_csv(\"sampleSubmission.csv\")\n",
    "submission.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Train a model using AutoGluon’s Tabular Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Requirements:\n",
    "* We are predicting `count`, so it is the label we are setting.\n",
    "* Ignore `casual` and `registered` columns as they are also not present in the test dataset. \n",
    "* Use the `root_mean_squared_error` as the metric to use for evaluation.\n",
    "* Set a time limit of 10 minutes (600 seconds).\n",
    "* Use the preset `best_quality` to focus on creating the best model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20220202_190705/\"\n",
      "Presets specified: ['best_quality']\n",
      "Beginning AutoGluon training ... Time limit = 600s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20220202_190705/\"\n",
      "AutoGluon Version:  0.3.1\n",
      "Train Data Rows:    10886\n",
      "Train Data Columns: 9\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == int and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (977, 1, 191.57413, 181.14445)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type argument in fit() (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2932.42 MB\n",
      "\tTrain Data (Original)  Memory Usage: 1.52 MB (0.1% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                      : 3 | ['temp', 'atemp', 'windspeed']\n",
      "\t\t('int', [])                        : 5 | ['season', 'holiday', 'workingday', 'weather', 'humidity']\n",
      "\t\t('object', ['datetime_as_object']) : 1 | ['datetime']\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('float', [])                : 3 | ['temp', 'atemp', 'windspeed']\n",
      "\t\t('int', [])                  : 3 | ['season', 'weather', 'humidity']\n",
      "\t\t('int', ['bool'])            : 2 | ['holiday', 'workingday']\n",
      "\t\t('int', ['datetime_as_int']) : 1 | ['datetime']\n",
      "\t0.1s = Fit runtime\n",
      "\t9 features in original data used to generate 9 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.63 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.12s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tTo change this, specify the eval_metric argument of fit()\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 399.82s of the 599.87s of remaining time.\n",
      "\t-160.4131\t = Validation score   (root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 399.39s of the 599.45s of remaining time.\n",
      "\t-169.5521\t = Validation score   (root_mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 398.99s of the 599.05s of remaining time.\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 123.806\tvalid_set's rmse: 134.369\n",
      "[2000]\ttrain_set's rmse: 117.412\tvalid_set's rmse: 133.705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 123.42\tvalid_set's rmse: 141.5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 125.097\tvalid_set's rmse: 128.797\n",
      "[2000]\ttrain_set's rmse: 119.003\tvalid_set's rmse: 127.909\n",
      "[3000]\ttrain_set's rmse: 114.63\tvalid_set's rmse: 127.431\n",
      "[4000]\ttrain_set's rmse: 111.295\tvalid_set's rmse: 126.943\n",
      "[5000]\ttrain_set's rmse: 108.576\tvalid_set's rmse: 126.844\n",
      "[6000]\ttrain_set's rmse: 106.085\tvalid_set's rmse: 126.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 124.131\tvalid_set's rmse: 138.303\n",
      "[2000]\ttrain_set's rmse: 117.833\tvalid_set's rmse: 137.535\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 124.871\tvalid_set's rmse: 128.052\n",
      "[2000]\ttrain_set's rmse: 118.547\tvalid_set's rmse: 127.003\n",
      "[3000]\ttrain_set's rmse: 114.124\tvalid_set's rmse: 126.834\n",
      "[4000]\ttrain_set's rmse: 110.645\tvalid_set's rmse: 126.694\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 124.654\tvalid_set's rmse: 135.095\n",
      "[2000]\ttrain_set's rmse: 118.764\tvalid_set's rmse: 133.849\n",
      "[3000]\ttrain_set's rmse: 114.615\tvalid_set's rmse: 133.509\n",
      "[4000]\ttrain_set's rmse: 111.249\tvalid_set's rmse: 133.472\n",
      "[5000]\ttrain_set's rmse: 108.455\tvalid_set's rmse: 133.281\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 124.01\tvalid_set's rmse: 139.355\n",
      "[2000]\ttrain_set's rmse: 117.819\tvalid_set's rmse: 138.312\n",
      "[3000]\ttrain_set's rmse: 113.4\tvalid_set's rmse: 137.886\n",
      "[4000]\ttrain_set's rmse: 110.032\tvalid_set's rmse: 137.758\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 124.47\tvalid_set's rmse: 135.412\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 124.18\tvalid_set's rmse: 137.579\n",
      "[2000]\ttrain_set's rmse: 117.782\tvalid_set's rmse: 136.664\n",
      "[3000]\ttrain_set's rmse: 113.447\tvalid_set's rmse: 136.246\n",
      "[4000]\ttrain_set's rmse: 109.982\tvalid_set's rmse: 136.227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 124.641\tvalid_set's rmse: 132.484\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-134.0883\t = Validation score   (root_mean_squared_error)\n",
      "\t43.52s\t = Training   runtime\n",
      "\t2.27s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 347.21s of the 547.26s of remaining time.\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 96.0217\tvalid_set's rmse: 123.249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 94.7479\tvalid_set's rmse: 135.635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 95.3055\tvalid_set's rmse: 132.087\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 95.1635\tvalid_set's rmse: 131.521\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\t-132.2864\t = Validation score   (root_mean_squared_error)\n",
      "\t12.45s\t = Training   runtime\n",
      "\t0.44s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 332.2s of the 532.26s of remaining time.\n",
      "\t-118.4567\t = Validation score   (root_mean_squared_error)\n",
      "\t6.95s\t = Training   runtime\n",
      "\t0.37s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 322.38s of the 522.43s of remaining time.\n",
      "\t-132.3497\t = Validation score   (root_mean_squared_error)\n",
      "\t54.68s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 266.68s of the 466.73s of remaining time.\n",
      "\t-128.7334\t = Validation score   (root_mean_squared_error)\n",
      "\t3.03s\t = Training   runtime\n",
      "\t0.38s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 261.0s of the 461.06s of remaining time.\n",
      "\t-138.1063\t = Validation score   (root_mean_squared_error)\n",
      "\t107.91s\t = Training   runtime\n",
      "\t0.28s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 151.85s of the 351.9s of remaining time.\n",
      "\t-132.3085\t = Validation score   (root_mean_squared_error)\n",
      "\t14.41s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Fitting model: NeuralNetMXNet_BAG_L1 ... Training model for up to 135.09s of the 335.14s of remaining time.\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 8)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 14)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 14)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 15)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 15)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 16)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 17)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 18)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 21)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 24)\n",
      "\t-143.3487\t = Validation score   (root_mean_squared_error)\n",
      "\t127.88s\t = Training   runtime\n",
      "\t1.58s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 4.83s of the 204.89s of remaining time.\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\tRan out of time, early stopping on iteration 74. Best iteration is:\n",
      "\t[74]\ttrain_set's rmse: 117.151\tvalid_set's rmse: 133.046\n",
      "\tTime limit exceeded... Skipping LightGBMLarge_BAG_L1.\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 204.26s of remaining time.\n",
      "\t-118.4335\t = Validation score   (root_mean_squared_error)\n",
      "\t0.53s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 9 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 203.63s of the 203.6s of remaining time.\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\t-116.4933\t = Validation score   (root_mean_squared_error)\n",
      "\t9.12s\t = Training   runtime\n",
      "\t0.18s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 193.42s of the 193.38s of remaining time.\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\t-117.0334\t = Validation score   (root_mean_squared_error)\n",
      "\t7.37s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 185.34s of the 185.3s of remaining time.\n",
      "\t-119.3288\t = Validation score   (root_mean_squared_error)\n",
      "\t28.89s\t = Training   runtime\n",
      "\t0.46s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 153.43s of the 153.39s of remaining time.\n",
      "\t-116.6266\t = Validation score   (root_mean_squared_error)\n",
      "\t27.45s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 125.17s of the 125.13s of remaining time.\n",
      "\t-117.8752\t = Validation score   (root_mean_squared_error)\n",
      "\t6.24s\t = Training   runtime\n",
      "\t0.45s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 116.24s of the 116.21s of remaining time.\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 24)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 24)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 25)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 26)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 27)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 28)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 28)\n",
      "No improvement since epoch 8: early stopping\n",
      "\t-115.7449\t = Validation score   (root_mean_squared_error)\n",
      "\t102.3s\t = Training   runtime\n",
      "\t0.32s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 12.63s of the 12.6s of remaining time.\n",
      "\t-117.5689\t = Validation score   (root_mean_squared_error)\n",
      "\t11.4s\t = Training   runtime\n",
      "\t0.08s\t = Validation runtime\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the -0.15s of remaining time.\n",
      "\t-115.1376\t = Validation score   (root_mean_squared_error)\n",
      "\t0.36s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 600.85s ...\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20220202_190705/\")\n"
     ]
    }
   ],
   "source": [
    "predictor = TabularPredictor(label=\"count\",eval_metric=\"root_mean_squared_error\").fit(train_data=train.loc[:, ~train.columns.isin(['casual', 'registered'])], time_limit=600, presets=\"best_quality\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review AutoGluon's training run with ranking of models that did the best."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                     model   score_val  pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0      WeightedEnsemble_L3 -115.137560       6.742346  496.261137                0.000706           0.359653            3       True         19\n",
      "1   NeuralNetFastAI_BAG_L2 -115.744948       6.032625  473.175181                0.320724         102.300158            2       True         17\n",
      "2        LightGBMXT_BAG_L2 -116.493262       5.887283  379.993131                0.175382           9.118108            2       True         12\n",
      "3          CatBoost_BAG_L2 -116.626608       5.746478  398.328017                0.034578          27.452993            2       True         15\n",
      "4          LightGBM_BAG_L2 -117.033443       5.790961  378.243744                0.079061           7.368721            2       True         13\n",
      "5           XGBoost_BAG_L2 -117.568875       5.795717  382.276823                0.083817          11.401799            2       True         18\n",
      "6     ExtraTreesMSE_BAG_L2 -117.875214       6.166472  377.114498                0.454572           6.239474            2       True         16\n",
      "7      WeightedEnsemble_L2 -118.433536       1.953879  135.362295                0.001174           0.534764            2       True         11\n",
      "8   RandomForestMSE_BAG_L1 -118.456660       0.372818    6.945640                0.372818           6.945640            1       True          5\n",
      "9   RandomForestMSE_BAG_L2 -119.328802       6.176881  399.764960                0.464980          28.889936            2       True         14\n",
      "10    ExtraTreesMSE_BAG_L1 -128.733445       0.377272    3.026416                0.377272           3.026416            1       True          7\n",
      "11         LightGBM_BAG_L1 -132.286410       0.441345   12.448246                0.441345          12.448246            1       True          4\n",
      "12          XGBoost_BAG_L1 -132.308522       0.135430   14.411126                0.135430          14.411126            1       True          9\n",
      "13         CatBoost_BAG_L1 -132.349692       0.043898   54.678723                0.043898          54.678723            1       True          6\n",
      "14       LightGBMXT_BAG_L1 -134.088334       2.270484   43.518858                2.270484          43.518858            1       True          3\n",
      "15  NeuralNetFastAI_BAG_L1 -138.106296       0.283758  107.913931                0.283758         107.913931            1       True          8\n",
      "16   NeuralNetMXNet_BAG_L1 -143.348686       1.579887  127.881891                1.579887         127.881891            1       True         10\n",
      "17   KNeighborsUnif_BAG_L1 -160.413078       0.103031    0.029339                0.103031           0.029339            1       True          1\n",
      "18   KNeighborsDist_BAG_L1 -169.552096       0.103976    0.020852                0.103976           0.020852            1       True          2\n",
      "Number of models trained: 19\n",
      "Types of models trained:\n",
      "{'StackerEnsembleModel_XGBoost', 'StackerEnsembleModel_NNFastAiTabular', 'StackerEnsembleModel_TabularNeuralNet', 'StackerEnsembleModel_KNN', 'StackerEnsembleModel_RF', 'WeightedEnsembleModel', 'StackerEnsembleModel_LGB', 'StackerEnsembleModel_CatBoost', 'StackerEnsembleModel_XT'}\n",
      "Bagging used: True  (with 10 folds)\n",
      "Multi-layer stack-ensembling used: True  (with 3 levels)\n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('float', [])                : 3 | ['temp', 'atemp', 'windspeed']\n",
      "('int', [])                  : 3 | ['season', 'weather', 'humidity']\n",
      "('int', ['bool'])            : 2 | ['holiday', 'workingday']\n",
      "('int', ['datetime_as_int']) : 1 | ['datetime']\n",
      "Plot summary of models saved to file: AutogluonModels/ag-20220202_190705/SummaryOfModels.html\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_types': {'KNeighborsUnif_BAG_L1': 'StackerEnsembleModel_KNN',\n",
       "  'KNeighborsDist_BAG_L1': 'StackerEnsembleModel_KNN',\n",
       "  'LightGBMXT_BAG_L1': 'StackerEnsembleModel_LGB',\n",
       "  'LightGBM_BAG_L1': 'StackerEnsembleModel_LGB',\n",
       "  'RandomForestMSE_BAG_L1': 'StackerEnsembleModel_RF',\n",
       "  'CatBoost_BAG_L1': 'StackerEnsembleModel_CatBoost',\n",
       "  'ExtraTreesMSE_BAG_L1': 'StackerEnsembleModel_XT',\n",
       "  'NeuralNetFastAI_BAG_L1': 'StackerEnsembleModel_NNFastAiTabular',\n",
       "  'XGBoost_BAG_L1': 'StackerEnsembleModel_XGBoost',\n",
       "  'NeuralNetMXNet_BAG_L1': 'StackerEnsembleModel_TabularNeuralNet',\n",
       "  'WeightedEnsemble_L2': 'WeightedEnsembleModel',\n",
       "  'LightGBMXT_BAG_L2': 'StackerEnsembleModel_LGB',\n",
       "  'LightGBM_BAG_L2': 'StackerEnsembleModel_LGB',\n",
       "  'RandomForestMSE_BAG_L2': 'StackerEnsembleModel_RF',\n",
       "  'CatBoost_BAG_L2': 'StackerEnsembleModel_CatBoost',\n",
       "  'ExtraTreesMSE_BAG_L2': 'StackerEnsembleModel_XT',\n",
       "  'NeuralNetFastAI_BAG_L2': 'StackerEnsembleModel_NNFastAiTabular',\n",
       "  'XGBoost_BAG_L2': 'StackerEnsembleModel_XGBoost',\n",
       "  'WeightedEnsemble_L3': 'WeightedEnsembleModel'},\n",
       " 'model_performance': {'KNeighborsUnif_BAG_L1': -160.4130779463501,\n",
       "  'KNeighborsDist_BAG_L1': -169.55209635208928,\n",
       "  'LightGBMXT_BAG_L1': -134.08833420116997,\n",
       "  'LightGBM_BAG_L1': -132.28640961443517,\n",
       "  'RandomForestMSE_BAG_L1': -118.45666016795705,\n",
       "  'CatBoost_BAG_L1': -132.34969157134555,\n",
       "  'ExtraTreesMSE_BAG_L1': -128.7334450621692,\n",
       "  'NeuralNetFastAI_BAG_L1': -138.10629601692537,\n",
       "  'XGBoost_BAG_L1': -132.30852184526813,\n",
       "  'NeuralNetMXNet_BAG_L1': -143.34868571863532,\n",
       "  'WeightedEnsemble_L2': -118.43353634263424,\n",
       "  'LightGBMXT_BAG_L2': -116.49326189134852,\n",
       "  'LightGBM_BAG_L2': -117.033442995374,\n",
       "  'RandomForestMSE_BAG_L2': -119.32880198861231,\n",
       "  'CatBoost_BAG_L2': -116.62660783176752,\n",
       "  'ExtraTreesMSE_BAG_L2': -117.87521432282387,\n",
       "  'NeuralNetFastAI_BAG_L2': -115.74494790678034,\n",
       "  'XGBoost_BAG_L2': -117.56887507541961,\n",
       "  'WeightedEnsemble_L3': -115.13756029474172},\n",
       " 'model_best': 'WeightedEnsemble_L3',\n",
       " 'model_paths': {'KNeighborsUnif_BAG_L1': 'AutogluonModels/ag-20220202_190705/models/KNeighborsUnif_BAG_L1/',\n",
       "  'KNeighborsDist_BAG_L1': 'AutogluonModels/ag-20220202_190705/models/KNeighborsDist_BAG_L1/',\n",
       "  'LightGBMXT_BAG_L1': 'AutogluonModels/ag-20220202_190705/models/LightGBMXT_BAG_L1/',\n",
       "  'LightGBM_BAG_L1': 'AutogluonModels/ag-20220202_190705/models/LightGBM_BAG_L1/',\n",
       "  'RandomForestMSE_BAG_L1': 'AutogluonModels/ag-20220202_190705/models/RandomForestMSE_BAG_L1/',\n",
       "  'CatBoost_BAG_L1': 'AutogluonModels/ag-20220202_190705/models/CatBoost_BAG_L1/',\n",
       "  'ExtraTreesMSE_BAG_L1': 'AutogluonModels/ag-20220202_190705/models/ExtraTreesMSE_BAG_L1/',\n",
       "  'NeuralNetFastAI_BAG_L1': 'AutogluonModels/ag-20220202_190705/models/NeuralNetFastAI_BAG_L1/',\n",
       "  'XGBoost_BAG_L1': 'AutogluonModels/ag-20220202_190705/models/XGBoost_BAG_L1/',\n",
       "  'NeuralNetMXNet_BAG_L1': 'AutogluonModels/ag-20220202_190705/models/NeuralNetMXNet_BAG_L1/',\n",
       "  'WeightedEnsemble_L2': 'AutogluonModels/ag-20220202_190705/models/WeightedEnsemble_L2/',\n",
       "  'LightGBMXT_BAG_L2': 'AutogluonModels/ag-20220202_190705/models/LightGBMXT_BAG_L2/',\n",
       "  'LightGBM_BAG_L2': 'AutogluonModels/ag-20220202_190705/models/LightGBM_BAG_L2/',\n",
       "  'RandomForestMSE_BAG_L2': 'AutogluonModels/ag-20220202_190705/models/RandomForestMSE_BAG_L2/',\n",
       "  'CatBoost_BAG_L2': 'AutogluonModels/ag-20220202_190705/models/CatBoost_BAG_L2/',\n",
       "  'ExtraTreesMSE_BAG_L2': 'AutogluonModels/ag-20220202_190705/models/ExtraTreesMSE_BAG_L2/',\n",
       "  'NeuralNetFastAI_BAG_L2': 'AutogluonModels/ag-20220202_190705/models/NeuralNetFastAI_BAG_L2/',\n",
       "  'XGBoost_BAG_L2': 'AutogluonModels/ag-20220202_190705/models/XGBoost_BAG_L2/',\n",
       "  'WeightedEnsemble_L3': 'AutogluonModels/ag-20220202_190705/models/WeightedEnsemble_L3/'},\n",
       " 'model_fit_times': {'KNeighborsUnif_BAG_L1': 0.029339313507080078,\n",
       "  'KNeighborsDist_BAG_L1': 0.020852327346801758,\n",
       "  'LightGBMXT_BAG_L1': 43.518858194351196,\n",
       "  'LightGBM_BAG_L1': 12.448246002197266,\n",
       "  'RandomForestMSE_BAG_L1': 6.9456398487091064,\n",
       "  'CatBoost_BAG_L1': 54.67872333526611,\n",
       "  'ExtraTreesMSE_BAG_L1': 3.026416063308716,\n",
       "  'NeuralNetFastAI_BAG_L1': 107.9139313697815,\n",
       "  'XGBoost_BAG_L1': 14.411126136779785,\n",
       "  'NeuralNetMXNet_BAG_L1': 127.88189101219177,\n",
       "  'WeightedEnsemble_L2': 0.534764289855957,\n",
       "  'LightGBMXT_BAG_L2': 9.118107557296753,\n",
       "  'LightGBM_BAG_L2': 7.368720769882202,\n",
       "  'RandomForestMSE_BAG_L2': 28.889936447143555,\n",
       "  'CatBoost_BAG_L2': 27.452993154525757,\n",
       "  'ExtraTreesMSE_BAG_L2': 6.239474058151245,\n",
       "  'NeuralNetFastAI_BAG_L2': 102.30015778541565,\n",
       "  'XGBoost_BAG_L2': 11.401799201965332,\n",
       "  'WeightedEnsemble_L3': 0.3596532344818115},\n",
       " 'model_pred_times': {'KNeighborsUnif_BAG_L1': 0.10303092002868652,\n",
       "  'KNeighborsDist_BAG_L1': 0.10397648811340332,\n",
       "  'LightGBMXT_BAG_L1': 2.270484447479248,\n",
       "  'LightGBM_BAG_L1': 0.44134521484375,\n",
       "  'RandomForestMSE_BAG_L1': 0.3728179931640625,\n",
       "  'CatBoost_BAG_L1': 0.04389810562133789,\n",
       "  'ExtraTreesMSE_BAG_L1': 0.3772721290588379,\n",
       "  'NeuralNetFastAI_BAG_L1': 0.28375840187072754,\n",
       "  'XGBoost_BAG_L1': 0.13543009757995605,\n",
       "  'NeuralNetMXNet_BAG_L1': 1.5798866748809814,\n",
       "  'WeightedEnsemble_L2': 0.0011739730834960938,\n",
       "  'LightGBMXT_BAG_L2': 0.17538237571716309,\n",
       "  'LightGBM_BAG_L2': 0.07906079292297363,\n",
       "  'RandomForestMSE_BAG_L2': 0.4649803638458252,\n",
       "  'CatBoost_BAG_L2': 0.03457784652709961,\n",
       "  'ExtraTreesMSE_BAG_L2': 0.4545719623565674,\n",
       "  'NeuralNetFastAI_BAG_L2': 0.3207242488861084,\n",
       "  'XGBoost_BAG_L2': 0.0838170051574707,\n",
       "  'WeightedEnsemble_L3': 0.000705718994140625},\n",
       " 'num_bag_folds': 10,\n",
       " 'max_stack_level': 3,\n",
       " 'model_hyperparams': {'KNeighborsUnif_BAG_L1': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True,\n",
       "   'use_child_oof': True},\n",
       "  'KNeighborsDist_BAG_L1': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True,\n",
       "   'use_child_oof': True},\n",
       "  'LightGBMXT_BAG_L1': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'LightGBM_BAG_L1': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'RandomForestMSE_BAG_L1': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True,\n",
       "   'use_child_oof': True},\n",
       "  'CatBoost_BAG_L1': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'ExtraTreesMSE_BAG_L1': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True,\n",
       "   'use_child_oof': True},\n",
       "  'NeuralNetFastAI_BAG_L1': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'XGBoost_BAG_L1': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'NeuralNetMXNet_BAG_L1': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'WeightedEnsemble_L2': {'use_orig_features': False,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'LightGBMXT_BAG_L2': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'LightGBM_BAG_L2': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'RandomForestMSE_BAG_L2': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True,\n",
       "   'use_child_oof': True},\n",
       "  'CatBoost_BAG_L2': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'ExtraTreesMSE_BAG_L2': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True,\n",
       "   'use_child_oof': True},\n",
       "  'NeuralNetFastAI_BAG_L2': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'XGBoost_BAG_L2': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'WeightedEnsemble_L3': {'use_orig_features': False,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True}},\n",
       " 'leaderboard':                      model   score_val  pred_time_val    fit_time  \\\n",
       " 0      WeightedEnsemble_L3 -115.137560       6.742346  496.261137   \n",
       " 1   NeuralNetFastAI_BAG_L2 -115.744948       6.032625  473.175181   \n",
       " 2        LightGBMXT_BAG_L2 -116.493262       5.887283  379.993131   \n",
       " 3          CatBoost_BAG_L2 -116.626608       5.746478  398.328017   \n",
       " 4          LightGBM_BAG_L2 -117.033443       5.790961  378.243744   \n",
       " 5           XGBoost_BAG_L2 -117.568875       5.795717  382.276823   \n",
       " 6     ExtraTreesMSE_BAG_L2 -117.875214       6.166472  377.114498   \n",
       " 7      WeightedEnsemble_L2 -118.433536       1.953879  135.362295   \n",
       " 8   RandomForestMSE_BAG_L1 -118.456660       0.372818    6.945640   \n",
       " 9   RandomForestMSE_BAG_L2 -119.328802       6.176881  399.764960   \n",
       " 10    ExtraTreesMSE_BAG_L1 -128.733445       0.377272    3.026416   \n",
       " 11         LightGBM_BAG_L1 -132.286410       0.441345   12.448246   \n",
       " 12          XGBoost_BAG_L1 -132.308522       0.135430   14.411126   \n",
       " 13         CatBoost_BAG_L1 -132.349692       0.043898   54.678723   \n",
       " 14       LightGBMXT_BAG_L1 -134.088334       2.270484   43.518858   \n",
       " 15  NeuralNetFastAI_BAG_L1 -138.106296       0.283758  107.913931   \n",
       " 16   NeuralNetMXNet_BAG_L1 -143.348686       1.579887  127.881891   \n",
       " 17   KNeighborsUnif_BAG_L1 -160.413078       0.103031    0.029339   \n",
       " 18   KNeighborsDist_BAG_L1 -169.552096       0.103976    0.020852   \n",
       " \n",
       "     pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
       " 0                 0.000706           0.359653            3       True   \n",
       " 1                 0.320724         102.300158            2       True   \n",
       " 2                 0.175382           9.118108            2       True   \n",
       " 3                 0.034578          27.452993            2       True   \n",
       " 4                 0.079061           7.368721            2       True   \n",
       " 5                 0.083817          11.401799            2       True   \n",
       " 6                 0.454572           6.239474            2       True   \n",
       " 7                 0.001174           0.534764            2       True   \n",
       " 8                 0.372818           6.945640            1       True   \n",
       " 9                 0.464980          28.889936            2       True   \n",
       " 10                0.377272           3.026416            1       True   \n",
       " 11                0.441345          12.448246            1       True   \n",
       " 12                0.135430          14.411126            1       True   \n",
       " 13                0.043898          54.678723            1       True   \n",
       " 14                2.270484          43.518858            1       True   \n",
       " 15                0.283758         107.913931            1       True   \n",
       " 16                1.579887         127.881891            1       True   \n",
       " 17                0.103031           0.029339            1       True   \n",
       " 18                0.103976           0.020852            1       True   \n",
       " \n",
       "     fit_order  \n",
       " 0          19  \n",
       " 1          17  \n",
       " 2          12  \n",
       " 3          15  \n",
       " 4          13  \n",
       " 5          18  \n",
       " 6          16  \n",
       " 7          11  \n",
       " 8           5  \n",
       " 9          14  \n",
       " 10          7  \n",
       " 11          4  \n",
       " 12          9  \n",
       " 13          6  \n",
       " 14          3  \n",
       " 15          8  \n",
       " 16         10  \n",
       " 17          1  \n",
       " 18          2  }"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor.fit_summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create predictions from test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     97.187813\n",
       "1     93.852036\n",
       "2     93.853455\n",
       "3    106.684494\n",
       "4    106.629463\n",
       "Name: count, dtype: float32"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = predictor.predict(test)\n",
    "predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor.leaderboard(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### NOTE: Kaggle will reject the submission if we don't set everything to be > 0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    6493.000000\n",
       "mean      210.817230\n",
       "std       126.929222\n",
       "min         1.851743\n",
       "25%       106.759476\n",
       "50%       195.041473\n",
       "75%       296.283264\n",
       "max       646.277466\n",
       "Name: count, dtype: float64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Describe the `predictions` series to see if there are any negative values\n",
    "predictions.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many negative values do we have?\n",
    "count=0\n",
    "for pred in predictions:\n",
    "    if pred < 0:\n",
    "        count=count+1\n",
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set them to zero\n",
    "# Not required since there are no negative values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set predictions to submission dataframe, save, and submit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0        97.187813\n",
      "1        93.852036\n",
      "2        93.853455\n",
      "3       106.684494\n",
      "4       106.629463\n",
      "           ...    \n",
      "6488    309.439667\n",
      "6489    309.443054\n",
      "6490    315.254974\n",
      "6491    287.648743\n",
      "6492    338.066895\n",
      "Name: count, Length: 6493, dtype: float32\n"
     ]
    }
   ],
   "source": [
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 97.18781 ,  93.852036,  93.853455, ..., 315.25497 , 287.64874 ,\n",
       "       338.0669  ], dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission[\"count\"] = predictions.values\n",
    "submission.to_csv(\"submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 188k/188k [00:00<00:00, 345kB/s]\n",
      "Successfully submitted to Bike Sharing Demand"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c bike-sharing-demand -f submission.csv -m \"first raw submission\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### View submission via the command line or in the web browser under the competition's page - `My Submissions`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fileName                     date                 description                             status    publicScore  privateScore  \n",
      "---------------------------  -------------------  --------------------------------------  --------  -----------  ------------  \n",
      "submission.csv               2022-02-02 19:29:26  first raw submission                    complete  1.39487      1.39487       \n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submissions -c bike-sharing-demand | tail -n +1 | head -n 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initial score of `1.39487`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Exploratory Data Analysis and Creating an additional feature\n",
    "* Any additional feature will do, but a great suggestion would be to separate out the datetime into hour, day, or month parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<AxesSubplot:title={'center':'season'}>,\n",
       "        <AxesSubplot:title={'center':'holiday'}>,\n",
       "        <AxesSubplot:title={'center':'workingday'}>],\n",
       "       [<AxesSubplot:title={'center':'weather'}>,\n",
       "        <AxesSubplot:title={'center':'temp'}>,\n",
       "        <AxesSubplot:title={'center':'atemp'}>],\n",
       "       [<AxesSubplot:title={'center':'humidity'}>,\n",
       "        <AxesSubplot:title={'center':'windspeed'}>,\n",
       "        <AxesSubplot:title={'center':'casual'}>],\n",
       "       [<AxesSubplot:title={'center':'registered'}>,\n",
       "        <AxesSubplot:title={'center':'count'}>, <AxesSubplot:>]],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 12 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Create a histogram of all features to show the distribution of each one relative to the data. This is part of the exploritory data analysis\n",
    "train.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10886 entries, 0 to 10885\n",
      "Data columns (total 12 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   datetime    10886 non-null  object \n",
      " 1   season      10886 non-null  int64  \n",
      " 2   holiday     10886 non-null  int64  \n",
      " 3   workingday  10886 non-null  int64  \n",
      " 4   weather     10886 non-null  int64  \n",
      " 5   temp        10886 non-null  float64\n",
      " 6   atemp       10886 non-null  float64\n",
      " 7   humidity    10886 non-null  int64  \n",
      " 8   windspeed   10886 non-null  float64\n",
      " 9   casual      10886 non-null  int64  \n",
      " 10  registered  10886 non-null  int64  \n",
      " 11  count       10886 non-null  int64  \n",
      "dtypes: float64(3), int64(8), object(1)\n",
      "memory usage: 1020.7+ KB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>season</th>\n",
       "      <th>holiday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weather</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-01 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-01 01:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.02</td>\n",
       "      <td>13.635</td>\n",
       "      <td>80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-01 02:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.02</td>\n",
       "      <td>13.635</td>\n",
       "      <td>80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-01-01 03:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-01-01 04:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              datetime  season  holiday  workingday  weather  temp   atemp  \\\n",
       "0  2011-01-01 00:00:00       1        0           0        1  9.84  14.395   \n",
       "1  2011-01-01 01:00:00       1        0           0        1  9.02  13.635   \n",
       "2  2011-01-01 02:00:00       1        0           0        1  9.02  13.635   \n",
       "3  2011-01-01 03:00:00       1        0           0        1  9.84  14.395   \n",
       "4  2011-01-01 04:00:00       1        0           0        1  9.84  14.395   \n",
       "\n",
       "   humidity  windspeed  casual  registered  count  \n",
       "0        81        0.0       3          13     16  \n",
       "1        80        0.0       8          32     40  \n",
       "2        80        0.0       5          27     32  \n",
       "3        75        0.0       3          10     13  \n",
       "4        75        0.0       0           1      1  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# column structure in train dataset BEFORE new feature creation\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change datatype of datetime column to DateTime\n",
    "train.loc[:, \"datetime\"] = pd.to_datetime(train.loc[:, \"datetime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new feature\n",
    "\n",
    "train[\"hour\"] = train.datetime.dt.hour\n",
    "train[\"month\"] = train.datetime.dt.month\n",
    "train[\"day\"] = train.datetime.dt.day\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>season</th>\n",
       "      <th>holiday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weather</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>count</th>\n",
       "      <th>hour</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-01 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-01 01:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.02</td>\n",
       "      <td>13.635</td>\n",
       "      <td>80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-01 02:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.02</td>\n",
       "      <td>13.635</td>\n",
       "      <td>80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-01-01 03:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-01-01 04:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             datetime  season  holiday  workingday  weather  temp   atemp  \\\n",
       "0 2011-01-01 00:00:00       1        0           0        1  9.84  14.395   \n",
       "1 2011-01-01 01:00:00       1        0           0        1  9.02  13.635   \n",
       "2 2011-01-01 02:00:00       1        0           0        1  9.02  13.635   \n",
       "3 2011-01-01 03:00:00       1        0           0        1  9.84  14.395   \n",
       "4 2011-01-01 04:00:00       1        0           0        1  9.84  14.395   \n",
       "\n",
       "   humidity  windspeed  casual  registered  count  hour  month  day  \n",
       "0        81        0.0       3          13     16     0      1    1  \n",
       "1        80        0.0       8          32     40     1      1    1  \n",
       "2        80        0.0       5          27     32     2      1    1  \n",
       "3        75        0.0       3          10     13     3      1    1  \n",
       "4        75        0.0       0           1      1     4      1    1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# column structure in train dataset AFTER new feature creation\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>season</th>\n",
       "      <th>holiday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weather</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windspeed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-20 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10.66</td>\n",
       "      <td>11.365</td>\n",
       "      <td>56</td>\n",
       "      <td>26.0027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-20 01:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10.66</td>\n",
       "      <td>13.635</td>\n",
       "      <td>56</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-20 02:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10.66</td>\n",
       "      <td>13.635</td>\n",
       "      <td>56</td>\n",
       "      <td>0.0000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-01-20 03:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10.66</td>\n",
       "      <td>12.880</td>\n",
       "      <td>56</td>\n",
       "      <td>11.0014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-01-20 04:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10.66</td>\n",
       "      <td>12.880</td>\n",
       "      <td>56</td>\n",
       "      <td>11.0014</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              datetime  season  holiday  workingday  weather   temp   atemp  \\\n",
       "0  2011-01-20 00:00:00       1        0           1        1  10.66  11.365   \n",
       "1  2011-01-20 01:00:00       1        0           1        1  10.66  13.635   \n",
       "2  2011-01-20 02:00:00       1        0           1        1  10.66  13.635   \n",
       "3  2011-01-20 03:00:00       1        0           1        1  10.66  12.880   \n",
       "4  2011-01-20 04:00:00       1        0           1        1  10.66  12.880   \n",
       "\n",
       "   humidity  windspeed  \n",
       "0        56    26.0027  \n",
       "1        56     0.0000  \n",
       "2        56     0.0000  \n",
       "3        56    11.0014  \n",
       "4        56    11.0014  "
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# column structure in test dataset BEFORE new feature creation\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 6493 entries, 0 to 6492\n",
      "Data columns (total 9 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   datetime    6493 non-null   object \n",
      " 1   season      6493 non-null   int64  \n",
      " 2   holiday     6493 non-null   int64  \n",
      " 3   workingday  6493 non-null   int64  \n",
      " 4   weather     6493 non-null   int64  \n",
      " 5   temp        6493 non-null   float64\n",
      " 6   atemp       6493 non-null   float64\n",
      " 7   humidity    6493 non-null   int64  \n",
      " 8   windspeed   6493 non-null   float64\n",
      "dtypes: float64(3), int64(5), object(1)\n",
      "memory usage: 456.7+ KB\n"
     ]
    }
   ],
   "source": [
    "test.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change datatype of datetime column to DateTime\n",
    "test.loc[:, \"datetime\"] = pd.to_datetime(test.loc[:, \"datetime\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new feature\n",
    "test[\"hour\"] = test.datetime.dt.hour\n",
    "test[\"month\"] = test.datetime.dt.month\n",
    "test[\"day\"] = test.datetime.dt.day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>season</th>\n",
       "      <th>holiday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weather</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>hour</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-20 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10.66</td>\n",
       "      <td>11.365</td>\n",
       "      <td>56</td>\n",
       "      <td>26.0027</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-20 01:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10.66</td>\n",
       "      <td>13.635</td>\n",
       "      <td>56</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-20 02:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10.66</td>\n",
       "      <td>13.635</td>\n",
       "      <td>56</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-01-20 03:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10.66</td>\n",
       "      <td>12.880</td>\n",
       "      <td>56</td>\n",
       "      <td>11.0014</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-01-20 04:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>10.66</td>\n",
       "      <td>12.880</td>\n",
       "      <td>56</td>\n",
       "      <td>11.0014</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             datetime  season  holiday  workingday  weather   temp   atemp  \\\n",
       "0 2011-01-20 00:00:00       1        0           1        1  10.66  11.365   \n",
       "1 2011-01-20 01:00:00       1        0           1        1  10.66  13.635   \n",
       "2 2011-01-20 02:00:00       1        0           1        1  10.66  13.635   \n",
       "3 2011-01-20 03:00:00       1        0           1        1  10.66  12.880   \n",
       "4 2011-01-20 04:00:00       1        0           1        1  10.66  12.880   \n",
       "\n",
       "   humidity  windspeed  hour  month  day  \n",
       "0        56    26.0027     0      1   20  \n",
       "1        56     0.0000     1      1   20  \n",
       "2        56     0.0000     2      1   20  \n",
       "3        56    11.0014     3      1   20  \n",
       "4        56    11.0014     4      1   20  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# column structure in test dataset AFTER new feature creation\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make category types for these so models know they are not just numbers\n",
    "* AutoGluon originally sees these as ints, but in reality they are int representations of a category.\n",
    "* Setting the dtype to category will classify these as categories in AutoGluon."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train[\"season\"] = train[\"season\"].astype(\"category\")\n",
    "train[\"weather\"] = train[\"weather\"].astype(\"category\")\n",
    "test[\"season\"] = test[\"season\"].astype(\"category\")\n",
    "test[\"weather\"] = test[\"weather\"].astype(\"category\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>season</th>\n",
       "      <th>holiday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>weather</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>count</th>\n",
       "      <th>hour</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-01 00:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>81</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>13</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-01 01:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.02</td>\n",
       "      <td>13.635</td>\n",
       "      <td>80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "      <td>40</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-01 02:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.02</td>\n",
       "      <td>13.635</td>\n",
       "      <td>80</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5</td>\n",
       "      <td>27</td>\n",
       "      <td>32</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-01-01 03:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>13</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-01-01 04:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>9.84</td>\n",
       "      <td>14.395</td>\n",
       "      <td>75</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             datetime season  holiday  workingday weather  temp   atemp  \\\n",
       "0 2011-01-01 00:00:00      1        0           0       1  9.84  14.395   \n",
       "1 2011-01-01 01:00:00      1        0           0       1  9.02  13.635   \n",
       "2 2011-01-01 02:00:00      1        0           0       1  9.02  13.635   \n",
       "3 2011-01-01 03:00:00      1        0           0       1  9.84  14.395   \n",
       "4 2011-01-01 04:00:00      1        0           0       1  9.84  14.395   \n",
       "\n",
       "   humidity  windspeed  casual  registered  count  hour  month  day  \n",
       "0        81        0.0       3          13     16     0      1    1  \n",
       "1        80        0.0       8          32     40     1      1    1  \n",
       "2        80        0.0       5          27     32     2      1    1  \n",
       "3        75        0.0       3          10     13     3      1    1  \n",
       "4        75        0.0       0           1      1     4      1    1  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View new features\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[<AxesSubplot:title={'center':'datetime'}>,\n",
       "        <AxesSubplot:title={'center':'holiday'}>,\n",
       "        <AxesSubplot:title={'center':'workingday'}>,\n",
       "        <AxesSubplot:title={'center':'temp'}>],\n",
       "       [<AxesSubplot:title={'center':'atemp'}>,\n",
       "        <AxesSubplot:title={'center':'humidity'}>,\n",
       "        <AxesSubplot:title={'center':'windspeed'}>,\n",
       "        <AxesSubplot:title={'center':'casual'}>],\n",
       "       [<AxesSubplot:title={'center':'registered'}>,\n",
       "        <AxesSubplot:title={'center':'count'}>,\n",
       "        <AxesSubplot:title={'center':'hour'}>,\n",
       "        <AxesSubplot:title={'center':'month'}>],\n",
       "       [<AxesSubplot:title={'center':'day'}>, <AxesSubplot:>,\n",
       "        <AxesSubplot:>, <AxesSubplot:>]], dtype=object)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 16 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# View histogram of all features again now with the hour feature\n",
    "train.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10886 entries, 0 to 10885\n",
      "Data columns (total 15 columns):\n",
      " #   Column      Non-Null Count  Dtype         \n",
      "---  ------      --------------  -----         \n",
      " 0   datetime    10886 non-null  datetime64[ns]\n",
      " 1   season      10886 non-null  category      \n",
      " 2   holiday     10886 non-null  int64         \n",
      " 3   workingday  10886 non-null  int64         \n",
      " 4   weather     10886 non-null  category      \n",
      " 5   temp        10886 non-null  float64       \n",
      " 6   atemp       10886 non-null  float64       \n",
      " 7   humidity    10886 non-null  int64         \n",
      " 8   windspeed   10886 non-null  float64       \n",
      " 9   casual      10886 non-null  int64         \n",
      " 10  registered  10886 non-null  int64         \n",
      " 11  count       10886 non-null  int64         \n",
      " 12  hour        10886 non-null  int64         \n",
      " 13  month       10886 non-null  int64         \n",
      " 14  day         10886 non-null  int64         \n",
      "dtypes: category(2), datetime64[ns](1), float64(3), int64(9)\n",
      "memory usage: 1.1 MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>holiday</th>\n",
       "      <th>workingday</th>\n",
       "      <th>temp</th>\n",
       "      <th>atemp</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windspeed</th>\n",
       "      <th>casual</th>\n",
       "      <th>registered</th>\n",
       "      <th>count</th>\n",
       "      <th>hour</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10886.000000</td>\n",
       "      <td>10886.000000</td>\n",
       "      <td>10886.00000</td>\n",
       "      <td>10886.000000</td>\n",
       "      <td>10886.000000</td>\n",
       "      <td>10886.000000</td>\n",
       "      <td>10886.000000</td>\n",
       "      <td>10886.000000</td>\n",
       "      <td>10886.000000</td>\n",
       "      <td>10886.000000</td>\n",
       "      <td>10886.000000</td>\n",
       "      <td>10886.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.028569</td>\n",
       "      <td>0.680875</td>\n",
       "      <td>20.23086</td>\n",
       "      <td>23.655084</td>\n",
       "      <td>61.886460</td>\n",
       "      <td>12.799395</td>\n",
       "      <td>36.021955</td>\n",
       "      <td>155.552177</td>\n",
       "      <td>191.574132</td>\n",
       "      <td>11.541613</td>\n",
       "      <td>6.521495</td>\n",
       "      <td>9.992559</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.166599</td>\n",
       "      <td>0.466159</td>\n",
       "      <td>7.79159</td>\n",
       "      <td>8.474601</td>\n",
       "      <td>19.245033</td>\n",
       "      <td>8.164537</td>\n",
       "      <td>49.960477</td>\n",
       "      <td>151.039033</td>\n",
       "      <td>181.144454</td>\n",
       "      <td>6.915838</td>\n",
       "      <td>3.444373</td>\n",
       "      <td>5.476608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.82000</td>\n",
       "      <td>0.760000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>13.94000</td>\n",
       "      <td>16.665000</td>\n",
       "      <td>47.000000</td>\n",
       "      <td>7.001500</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>36.000000</td>\n",
       "      <td>42.000000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>20.50000</td>\n",
       "      <td>24.240000</td>\n",
       "      <td>62.000000</td>\n",
       "      <td>12.998000</td>\n",
       "      <td>17.000000</td>\n",
       "      <td>118.000000</td>\n",
       "      <td>145.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>7.000000</td>\n",
       "      <td>10.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>26.24000</td>\n",
       "      <td>31.060000</td>\n",
       "      <td>77.000000</td>\n",
       "      <td>16.997900</td>\n",
       "      <td>49.000000</td>\n",
       "      <td>222.000000</td>\n",
       "      <td>284.000000</td>\n",
       "      <td>18.000000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>15.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>41.00000</td>\n",
       "      <td>45.455000</td>\n",
       "      <td>100.000000</td>\n",
       "      <td>56.996900</td>\n",
       "      <td>367.000000</td>\n",
       "      <td>886.000000</td>\n",
       "      <td>977.000000</td>\n",
       "      <td>23.000000</td>\n",
       "      <td>12.000000</td>\n",
       "      <td>19.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            holiday    workingday         temp         atemp      humidity  \\\n",
       "count  10886.000000  10886.000000  10886.00000  10886.000000  10886.000000   \n",
       "mean       0.028569      0.680875     20.23086     23.655084     61.886460   \n",
       "std        0.166599      0.466159      7.79159      8.474601     19.245033   \n",
       "min        0.000000      0.000000      0.82000      0.760000      0.000000   \n",
       "25%        0.000000      0.000000     13.94000     16.665000     47.000000   \n",
       "50%        0.000000      1.000000     20.50000     24.240000     62.000000   \n",
       "75%        0.000000      1.000000     26.24000     31.060000     77.000000   \n",
       "max        1.000000      1.000000     41.00000     45.455000    100.000000   \n",
       "\n",
       "          windspeed        casual    registered         count          hour  \\\n",
       "count  10886.000000  10886.000000  10886.000000  10886.000000  10886.000000   \n",
       "mean      12.799395     36.021955    155.552177    191.574132     11.541613   \n",
       "std        8.164537     49.960477    151.039033    181.144454      6.915838   \n",
       "min        0.000000      0.000000      0.000000      1.000000      0.000000   \n",
       "25%        7.001500      4.000000     36.000000     42.000000      6.000000   \n",
       "50%       12.998000     17.000000    118.000000    145.000000     12.000000   \n",
       "75%       16.997900     49.000000    222.000000    284.000000     18.000000   \n",
       "max       56.996900    367.000000    886.000000    977.000000     23.000000   \n",
       "\n",
       "              month           day  \n",
       "count  10886.000000  10886.000000  \n",
       "mean       6.521495      9.992559  \n",
       "std        3.444373      5.476608  \n",
       "min        1.000000      1.000000  \n",
       "25%        4.000000      5.000000  \n",
       "50%        7.000000     10.000000  \n",
       "75%       10.000000     15.000000  \n",
       "max       12.000000     19.000000  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Rerun the model with the same settings as before, just with more features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20220202_193923/\"\n",
      "Presets specified: ['best_quality']\n",
      "Beginning AutoGluon training ... Time limit = 600s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20220202_193923/\"\n",
      "AutoGluon Version:  0.3.1\n",
      "Train Data Rows:    10886\n",
      "Train Data Columns: 12\n",
      "Preprocessing data ...\n",
      "AutoGluon infers your prediction problem is: 'regression' (because dtype of label-column == int and many unique label-values observed).\n",
      "\tLabel info (max, min, mean, stddev): (977, 1, 191.57413, 181.14445)\n",
      "\tIf 'regression' is not the correct problem_type, please manually specify the problem_type argument in fit() (You may specify problem_type as one of: ['binary', 'multiclass', 'regression'])\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2420.95 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.89 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 2 | ['season', 'weather']\n",
      "\t\t('datetime', []) : 1 | ['datetime']\n",
      "\t\t('float', [])    : 3 | ['temp', 'atemp', 'windspeed']\n",
      "\t\t('int', [])      : 6 | ['holiday', 'workingday', 'humidity', 'hour', 'month', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])             : 2 | ['season', 'weather']\n",
      "\t\t('float', [])                : 3 | ['temp', 'atemp', 'windspeed']\n",
      "\t\t('int', [])                  : 4 | ['humidity', 'hour', 'month', 'day']\n",
      "\t\t('int', ['bool'])            : 2 | ['holiday', 'workingday']\n",
      "\t\t('int', ['datetime_as_int']) : 1 | ['datetime']\n",
      "\t0.3s = Fit runtime\n",
      "\t12 features in original data used to generate 12 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.74 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.35s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tTo change this, specify the eval_metric argument of fit()\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 11 L1 models ...\n",
      "Fitting model: KNeighborsUnif_BAG_L1 ... Training model for up to 399.67s of the 599.64s of remaining time.\n",
      "\t-123.9226\t = Validation score   (root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1 ... Training model for up to 399.15s of the 599.12s of remaining time.\n",
      "\t-119.3734\t = Validation score   (root_mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Fitting model: LightGBMXT_BAG_L1 ... Training model for up to 398.64s of the 598.61s of remaining time.\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 30.7376\tvalid_set's rmse: 37.6853\n",
      "[2000]\ttrain_set's rmse: 25.8887\tvalid_set's rmse: 36.3085\n",
      "[3000]\ttrain_set's rmse: 23.1491\tvalid_set's rmse: 35.8944\n",
      "[4000]\ttrain_set's rmse: 21.181\tvalid_set's rmse: 35.803\n",
      "[5000]\ttrain_set's rmse: 19.6612\tvalid_set's rmse: 35.7654\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 30.5414\tvalid_set's rmse: 41.34\n",
      "[2000]\ttrain_set's rmse: 25.6339\tvalid_set's rmse: 39.3073\n",
      "[3000]\ttrain_set's rmse: 22.9703\tvalid_set's rmse: 38.68\n",
      "[4000]\ttrain_set's rmse: 21.0487\tvalid_set's rmse: 38.3491\n",
      "[5000]\ttrain_set's rmse: 19.568\tvalid_set's rmse: 38.121\n",
      "[6000]\ttrain_set's rmse: 18.3684\tvalid_set's rmse: 37.9466\n",
      "[7000]\ttrain_set's rmse: 17.347\tvalid_set's rmse: 37.8924\n",
      "[8000]\ttrain_set's rmse: 16.421\tvalid_set's rmse: 37.7667\n",
      "[9000]\ttrain_set's rmse: 15.6093\tvalid_set's rmse: 37.7304\n",
      "[10000]\ttrain_set's rmse: 14.8919\tvalid_set's rmse: 37.7428\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 31.0461\tvalid_set's rmse: 35.2105\n",
      "[2000]\ttrain_set's rmse: 26.0047\tvalid_set's rmse: 34.0067\n",
      "[3000]\ttrain_set's rmse: 23.2324\tvalid_set's rmse: 33.8698\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 30.4064\tvalid_set's rmse: 41.6394\n",
      "[2000]\ttrain_set's rmse: 25.567\tvalid_set's rmse: 39.8941\n",
      "[3000]\ttrain_set's rmse: 22.9126\tvalid_set's rmse: 39.3073\n",
      "[4000]\ttrain_set's rmse: 21.1144\tvalid_set's rmse: 38.9247\n",
      "[5000]\ttrain_set's rmse: 19.6681\tvalid_set's rmse: 38.6946\n",
      "[6000]\ttrain_set's rmse: 18.4567\tvalid_set's rmse: 38.4382\n",
      "[7000]\ttrain_set's rmse: 17.4453\tvalid_set's rmse: 38.3083\n",
      "[8000]\ttrain_set's rmse: 16.5522\tvalid_set's rmse: 38.2049\n",
      "[9000]\ttrain_set's rmse: 15.7557\tvalid_set's rmse: 38.1466\n",
      "[10000]\ttrain_set's rmse: 15.0209\tvalid_set's rmse: 38.0793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 30.7476\tvalid_set's rmse: 38.5074\n",
      "[2000]\ttrain_set's rmse: 25.8946\tvalid_set's rmse: 37.4207\n",
      "[3000]\ttrain_set's rmse: 23.1842\tvalid_set's rmse: 37.1689\n",
      "[4000]\ttrain_set's rmse: 21.2301\tvalid_set's rmse: 37.1645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 30.6149\tvalid_set's rmse: 41.3972\n",
      "[2000]\ttrain_set's rmse: 25.6795\tvalid_set's rmse: 40.275\n",
      "[3000]\ttrain_set's rmse: 23.0019\tvalid_set's rmse: 40.0223\n",
      "[4000]\ttrain_set's rmse: 21.0692\tvalid_set's rmse: 40.0055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 30.9698\tvalid_set's rmse: 38.7415\n",
      "[2000]\ttrain_set's rmse: 25.9996\tvalid_set's rmse: 35.9834\n",
      "[3000]\ttrain_set's rmse: 23.2962\tvalid_set's rmse: 35.043\n",
      "[4000]\ttrain_set's rmse: 21.3837\tvalid_set's rmse: 34.7768\n",
      "[5000]\ttrain_set's rmse: 19.8462\tvalid_set's rmse: 34.556\n",
      "[6000]\ttrain_set's rmse: 18.5425\tvalid_set's rmse: 34.5105\n",
      "[7000]\ttrain_set's rmse: 17.4699\tvalid_set's rmse: 34.452\n",
      "[8000]\ttrain_set's rmse: 16.5448\tvalid_set's rmse: 34.5286\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 30.8158\tvalid_set's rmse: 40.0477\n",
      "[2000]\ttrain_set's rmse: 25.8511\tvalid_set's rmse: 38.4189\n",
      "[3000]\ttrain_set's rmse: 23.1096\tvalid_set's rmse: 37.9895\n",
      "[4000]\ttrain_set's rmse: 21.1965\tvalid_set's rmse: 37.8563\n",
      "[5000]\ttrain_set's rmse: 19.6606\tvalid_set's rmse: 37.7866\n",
      "[6000]\ttrain_set's rmse: 18.4473\tvalid_set's rmse: 37.6621\n",
      "[7000]\ttrain_set's rmse: 17.3839\tvalid_set's rmse: 37.6104\n",
      "[8000]\ttrain_set's rmse: 16.4545\tvalid_set's rmse: 37.5852\n",
      "[9000]\ttrain_set's rmse: 15.6263\tvalid_set's rmse: 37.5827\n",
      "[10000]\ttrain_set's rmse: 14.8962\tvalid_set's rmse: 37.5647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 30.8715\tvalid_set's rmse: 41.6983\n",
      "[2000]\ttrain_set's rmse: 25.7864\tvalid_set's rmse: 40.4701\n",
      "[3000]\ttrain_set's rmse: 23.0438\tvalid_set's rmse: 40.2124\n",
      "[4000]\ttrain_set's rmse: 21.1102\tvalid_set's rmse: 40.1203\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 31.2128\tvalid_set's rmse: 37.7181\n",
      "[2000]\ttrain_set's rmse: 26.193\tvalid_set's rmse: 35.5644\n",
      "[3000]\ttrain_set's rmse: 23.4812\tvalid_set's rmse: 34.97\n",
      "[4000]\ttrain_set's rmse: 21.5353\tvalid_set's rmse: 34.57\n",
      "[5000]\ttrain_set's rmse: 20.0592\tvalid_set's rmse: 34.4469\n",
      "[6000]\ttrain_set's rmse: 18.7743\tvalid_set's rmse: 34.367\n",
      "[7000]\ttrain_set's rmse: 17.7047\tvalid_set's rmse: 34.3081\n",
      "[8000]\ttrain_set's rmse: 16.7867\tvalid_set's rmse: 34.3073\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-36.9415\t = Validation score   (root_mean_squared_error)\n",
      "\t97.25s\t = Training   runtime\n",
      "\t5.49s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L1 ... Training model for up to 283.6s of the 483.58s of remaining time.\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 21.1363\tvalid_set's rmse: 35.5572\n",
      "[2000]\ttrain_set's rmse: 15.5114\tvalid_set's rmse: 35.4473\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 21.0431\tvalid_set's rmse: 36.9373\n",
      "[2000]\ttrain_set's rmse: 15.5393\tvalid_set's rmse: 36.1173\n",
      "[3000]\ttrain_set's rmse: 12.204\tvalid_set's rmse: 35.918\n",
      "[4000]\ttrain_set's rmse: 9.99922\tvalid_set's rmse: 35.8485\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 21.4766\tvalid_set's rmse: 32.789\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 21.4591\tvalid_set's rmse: 38.1004\n",
      "[2000]\ttrain_set's rmse: 15.6967\tvalid_set's rmse: 37.1149\n",
      "[3000]\ttrain_set's rmse: 12.2044\tvalid_set's rmse: 36.7233\n",
      "[4000]\ttrain_set's rmse: 9.95476\tvalid_set's rmse: 36.6799\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 21.3995\tvalid_set's rmse: 37.384\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 21.6619\tvalid_set's rmse: 35.9544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 20.9449\tvalid_set's rmse: 35.7483\n",
      "[2000]\ttrain_set's rmse: 15.4023\tvalid_set's rmse: 35.1407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 21.3287\tvalid_set's rmse: 36.3231\n",
      "[2000]\ttrain_set's rmse: 15.6319\tvalid_set's rmse: 35.6071\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-36.5792\t = Validation score   (root_mean_squared_error)\n",
      "\t30.24s\t = Training   runtime\n",
      "\t1.23s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1 ... Training model for up to 248.03s of the 448.01s of remaining time.\n",
      "\t-41.3075\t = Validation score   (root_mean_squared_error)\n",
      "\t9.54s\t = Training   runtime\n",
      "\t0.4s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L1 ... Training model for up to 235.97s of the 435.95s of remaining time.\n",
      "\tTime limit exceeded... Skipping CatBoost_BAG_L1.\n",
      "Fitting model: ExtraTreesMSE_BAG_L1 ... Training model for up to 206.21s of the 406.19s of remaining time.\n",
      "\t-40.8242\t = Validation score   (root_mean_squared_error)\n",
      "\t3.93s\t = Training   runtime\n",
      "\t0.4s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1 ... Training model for up to 199.47s of the 399.45s of remaining time.\n",
      "\t-46.4964\t = Validation score   (root_mean_squared_error)\n",
      "\t133.28s\t = Training   runtime\n",
      "\t0.35s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L1 ... Training model for up to 64.8s of the 264.78s of remaining time.\n",
      "\t-37.3995\t = Validation score   (root_mean_squared_error)\n",
      "\t31.76s\t = Training   runtime\n",
      "\t0.32s\t = Validation runtime\n",
      "Fitting model: NeuralNetMXNet_BAG_L1 ... Training model for up to 29.14s of the 229.11s of remaining time.\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 0)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 1)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 1)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 1)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 1)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 1)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 1)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 1)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 2)\n",
      "\t-246.3477\t = Validation score   (root_mean_squared_error)\n",
      "\t24.58s\t = Training   runtime\n",
      "\t2.26s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 1.57s of the 201.54s of remaining time.\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\tRan out of time, early stopping on iteration 1. Best iteration is:\n",
      "\t[1]\ttrain_set's rmse: 176.302\tvalid_set's rmse: 174.678\n",
      "\tTime limit exceeded... Skipping LightGBMLarge_BAG_L1.\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 201.2s of remaining time.\n",
      "\t-35.1865\t = Validation score   (root_mean_squared_error)\n",
      "\t0.46s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 9 L2 models ...\n",
      "Fitting model: LightGBMXT_BAG_L2 ... Training model for up to 200.65s of the 200.62s of remaining time.\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\t-36.2666\t = Validation score   (root_mean_squared_error)\n",
      "\t8.33s\t = Training   runtime\n",
      "\t0.16s\t = Validation runtime\n",
      "Fitting model: LightGBM_BAG_L2 ... Training model for up to 191.37s of the 191.34s of remaining time.\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\t-35.7631\t = Validation score   (root_mean_squared_error)\n",
      "\t7.18s\t = Training   runtime\n",
      "\t0.1s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2 ... Training model for up to 183.5s of the 183.47s of remaining time.\n",
      "\t-36.0173\t = Validation score   (root_mean_squared_error)\n",
      "\t27.79s\t = Training   runtime\n",
      "\t0.46s\t = Validation runtime\n",
      "Fitting model: CatBoost_BAG_L2 ... Training model for up to 153.2s of the 153.17s of remaining time.\n",
      "\t-35.5778\t = Validation score   (root_mean_squared_error)\n",
      "\t42.17s\t = Training   runtime\n",
      "\t0.06s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L2 ... Training model for up to 110.26s of the 110.23s of remaining time.\n",
      "\t-35.6953\t = Validation score   (root_mean_squared_error)\n",
      "\t6.85s\t = Training   runtime\n",
      "\t0.46s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2 ... Training model for up to 100.78s of the 100.75s of remaining time.\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 16)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 17)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 17)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 18)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 19)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 19)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 20)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 22)\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 25)\n",
      "\t-35.9772\t = Validation score   (root_mean_squared_error)\n",
      "\t95.36s\t = Training   runtime\n",
      "\t0.39s\t = Validation runtime\n",
      "Fitting model: XGBoost_BAG_L2 ... Training model for up to 4.07s of the 4.04s of remaining time.\n",
      "\tTime limit exceeded... Skipping XGBoost_BAG_L2.\n",
      "Fitting model: NeuralNetMXNet_BAG_L2 ... Training model for up to 3.56s of the 3.53s of remaining time.\n",
      "\tTime limit exceeded... Skipping NeuralNetMXNet_BAG_L2.\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 2.6s of the 2.57s of remaining time.\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\tRan out of time, early stopping on iteration 3. Best iteration is:\n",
      "\t[3]\ttrain_set's rmse: 166.12\tvalid_set's rmse: 163.088\n",
      "\tTime limit exceeded... Skipping LightGBMLarge_BAG_L2.\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the 2.2s of remaining time.\n",
      "\t-35.2458\t = Validation score   (root_mean_squared_error)\n",
      "\t0.32s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 598.41s ...\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20220202_193923/\")\n"
     ]
    }
   ],
   "source": [
    "predictor_new_features = TabularPredictor(label=\"count\",eval_metric=\"root_mean_squared_error\").fit(train_data=train.loc[:, ~train.columns.isin(['casual', 'registered'])], time_limit=600, presets=\"best_quality\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                     model   score_val  pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0      WeightedEnsemble_L2  -35.186484       7.796752  302.526579                0.000757           0.457028            2       True         10\n",
      "1      WeightedEnsemble_L3  -35.245798      12.343088  510.300913                0.000722           0.320552            3       True         17\n",
      "2          CatBoost_BAG_L2  -35.577795      10.931376  372.807618                0.064953          42.170421            2       True         14\n",
      "3     ExtraTreesMSE_BAG_L2  -35.695318      11.330737  337.485581                0.464314           6.848383            2       True         15\n",
      "4          LightGBM_BAG_L2  -35.763061      10.963635  337.815503                0.097212           7.178305            2       True         12\n",
      "5   NeuralNetFastAI_BAG_L2  -35.977238      11.253547  425.997911                0.387123          95.360714            2       True         16\n",
      "6   RandomForestMSE_BAG_L2  -36.017326      11.328764  358.422538                0.462340          27.785340            2       True         13\n",
      "7        LightGBMXT_BAG_L2  -36.266589      11.023179  338.969662                0.156755           8.332464            2       True         11\n",
      "8          LightGBM_BAG_L1  -36.579153       1.228561   30.240943                1.228561          30.240943            1       True          4\n",
      "9        LightGBMXT_BAG_L1  -36.941451       5.490897   97.249026                5.490897          97.249026            1       True          3\n",
      "10          XGBoost_BAG_L1  -37.399540       0.324223   31.757440                0.324223          31.757440            1       True          8\n",
      "11    ExtraTreesMSE_BAG_L1  -40.824193       0.402815    3.931156                0.402815           3.931156            1       True          6\n",
      "12  RandomForestMSE_BAG_L1  -41.307546       0.400761    9.544247                0.400761           9.544247            1       True          5\n",
      "13  NeuralNetFastAI_BAG_L1  -46.496409       0.351554  133.277896                0.351554         133.277896            1       True          7\n",
      "14   KNeighborsDist_BAG_L1 -119.373394       0.203787    0.021954                0.203787           0.021954            1       True          2\n",
      "15   KNeighborsUnif_BAG_L1 -123.922640       0.203114    0.031451                0.203114           0.031451            1       True          1\n",
      "16   NeuralNetMXNet_BAG_L1 -246.347726       2.260711   24.583085                2.260711          24.583085            1       True          9\n",
      "Number of models trained: 17\n",
      "Types of models trained:\n",
      "{'StackerEnsembleModel_XGBoost', 'StackerEnsembleModel_NNFastAiTabular', 'StackerEnsembleModel_TabularNeuralNet', 'StackerEnsembleModel_KNN', 'StackerEnsembleModel_RF', 'WeightedEnsembleModel', 'StackerEnsembleModel_LGB', 'StackerEnsembleModel_CatBoost', 'StackerEnsembleModel_XT'}\n",
      "Bagging used: True  (with 10 folds)\n",
      "Multi-layer stack-ensembling used: True  (with 3 levels)\n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('category', [])             : 2 | ['season', 'weather']\n",
      "('float', [])                : 3 | ['temp', 'atemp', 'windspeed']\n",
      "('int', [])                  : 4 | ['humidity', 'hour', 'month', 'day']\n",
      "('int', ['bool'])            : 2 | ['holiday', 'workingday']\n",
      "('int', ['datetime_as_int']) : 1 | ['datetime']\n",
      "Plot summary of models saved to file: AutogluonModels/ag-20220202_193923/SummaryOfModels.html\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_types': {'KNeighborsUnif_BAG_L1': 'StackerEnsembleModel_KNN',\n",
       "  'KNeighborsDist_BAG_L1': 'StackerEnsembleModel_KNN',\n",
       "  'LightGBMXT_BAG_L1': 'StackerEnsembleModel_LGB',\n",
       "  'LightGBM_BAG_L1': 'StackerEnsembleModel_LGB',\n",
       "  'RandomForestMSE_BAG_L1': 'StackerEnsembleModel_RF',\n",
       "  'ExtraTreesMSE_BAG_L1': 'StackerEnsembleModel_XT',\n",
       "  'NeuralNetFastAI_BAG_L1': 'StackerEnsembleModel_NNFastAiTabular',\n",
       "  'XGBoost_BAG_L1': 'StackerEnsembleModel_XGBoost',\n",
       "  'NeuralNetMXNet_BAG_L1': 'StackerEnsembleModel_TabularNeuralNet',\n",
       "  'WeightedEnsemble_L2': 'WeightedEnsembleModel',\n",
       "  'LightGBMXT_BAG_L2': 'StackerEnsembleModel_LGB',\n",
       "  'LightGBM_BAG_L2': 'StackerEnsembleModel_LGB',\n",
       "  'RandomForestMSE_BAG_L2': 'StackerEnsembleModel_RF',\n",
       "  'CatBoost_BAG_L2': 'StackerEnsembleModel_CatBoost',\n",
       "  'ExtraTreesMSE_BAG_L2': 'StackerEnsembleModel_XT',\n",
       "  'NeuralNetFastAI_BAG_L2': 'StackerEnsembleModel_NNFastAiTabular',\n",
       "  'WeightedEnsemble_L3': 'WeightedEnsembleModel'},\n",
       " 'model_performance': {'KNeighborsUnif_BAG_L1': -123.92263994015985,\n",
       "  'KNeighborsDist_BAG_L1': -119.37339448003198,\n",
       "  'LightGBMXT_BAG_L1': -36.94145052645896,\n",
       "  'LightGBM_BAG_L1': -36.57915349216572,\n",
       "  'RandomForestMSE_BAG_L1': -41.30754583307658,\n",
       "  'ExtraTreesMSE_BAG_L1': -40.82419267315799,\n",
       "  'NeuralNetFastAI_BAG_L1': -46.496408507095865,\n",
       "  'XGBoost_BAG_L1': -37.399539785297726,\n",
       "  'NeuralNetMXNet_BAG_L1': -246.34772551162342,\n",
       "  'WeightedEnsemble_L2': -35.18648360094959,\n",
       "  'LightGBMXT_BAG_L2': -36.26658924441224,\n",
       "  'LightGBM_BAG_L2': -35.76306120693435,\n",
       "  'RandomForestMSE_BAG_L2': -36.01732638289419,\n",
       "  'CatBoost_BAG_L2': -35.57779531385168,\n",
       "  'ExtraTreesMSE_BAG_L2': -35.69531814760805,\n",
       "  'NeuralNetFastAI_BAG_L2': -35.977238288726454,\n",
       "  'WeightedEnsemble_L3': -35.245798327402916},\n",
       " 'model_best': 'WeightedEnsemble_L2',\n",
       " 'model_paths': {'KNeighborsUnif_BAG_L1': 'AutogluonModels/ag-20220202_193923/models/KNeighborsUnif_BAG_L1/',\n",
       "  'KNeighborsDist_BAG_L1': 'AutogluonModels/ag-20220202_193923/models/KNeighborsDist_BAG_L1/',\n",
       "  'LightGBMXT_BAG_L1': 'AutogluonModels/ag-20220202_193923/models/LightGBMXT_BAG_L1/',\n",
       "  'LightGBM_BAG_L1': 'AutogluonModels/ag-20220202_193923/models/LightGBM_BAG_L1/',\n",
       "  'RandomForestMSE_BAG_L1': 'AutogluonModels/ag-20220202_193923/models/RandomForestMSE_BAG_L1/',\n",
       "  'ExtraTreesMSE_BAG_L1': 'AutogluonModels/ag-20220202_193923/models/ExtraTreesMSE_BAG_L1/',\n",
       "  'NeuralNetFastAI_BAG_L1': 'AutogluonModels/ag-20220202_193923/models/NeuralNetFastAI_BAG_L1/',\n",
       "  'XGBoost_BAG_L1': 'AutogluonModels/ag-20220202_193923/models/XGBoost_BAG_L1/',\n",
       "  'NeuralNetMXNet_BAG_L1': 'AutogluonModels/ag-20220202_193923/models/NeuralNetMXNet_BAG_L1/',\n",
       "  'WeightedEnsemble_L2': 'AutogluonModels/ag-20220202_193923/models/WeightedEnsemble_L2/',\n",
       "  'LightGBMXT_BAG_L2': 'AutogluonModels/ag-20220202_193923/models/LightGBMXT_BAG_L2/',\n",
       "  'LightGBM_BAG_L2': 'AutogluonModels/ag-20220202_193923/models/LightGBM_BAG_L2/',\n",
       "  'RandomForestMSE_BAG_L2': 'AutogluonModels/ag-20220202_193923/models/RandomForestMSE_BAG_L2/',\n",
       "  'CatBoost_BAG_L2': 'AutogluonModels/ag-20220202_193923/models/CatBoost_BAG_L2/',\n",
       "  'ExtraTreesMSE_BAG_L2': 'AutogluonModels/ag-20220202_193923/models/ExtraTreesMSE_BAG_L2/',\n",
       "  'NeuralNetFastAI_BAG_L2': 'AutogluonModels/ag-20220202_193923/models/NeuralNetFastAI_BAG_L2/',\n",
       "  'WeightedEnsemble_L3': 'AutogluonModels/ag-20220202_193923/models/WeightedEnsemble_L3/'},\n",
       " 'model_fit_times': {'KNeighborsUnif_BAG_L1': 0.03145098686218262,\n",
       "  'KNeighborsDist_BAG_L1': 0.021954059600830078,\n",
       "  'LightGBMXT_BAG_L1': 97.24902582168579,\n",
       "  'LightGBM_BAG_L1': 30.24094319343567,\n",
       "  'RandomForestMSE_BAG_L1': 9.544246673583984,\n",
       "  'ExtraTreesMSE_BAG_L1': 3.9311561584472656,\n",
       "  'NeuralNetFastAI_BAG_L1': 133.27789568901062,\n",
       "  'XGBoost_BAG_L1': 31.757440090179443,\n",
       "  'NeuralNetMXNet_BAG_L1': 24.58308506011963,\n",
       "  'WeightedEnsemble_L2': 0.4570279121398926,\n",
       "  'LightGBMXT_BAG_L2': 8.33246397972107,\n",
       "  'LightGBM_BAG_L2': 7.178305387496948,\n",
       "  'RandomForestMSE_BAG_L2': 27.785339832305908,\n",
       "  'CatBoost_BAG_L2': 42.17042064666748,\n",
       "  'ExtraTreesMSE_BAG_L2': 6.848382949829102,\n",
       "  'NeuralNetFastAI_BAG_L2': 95.36071372032166,\n",
       "  'WeightedEnsemble_L3': 0.32055234909057617},\n",
       " 'model_pred_times': {'KNeighborsUnif_BAG_L1': 0.20311427116394043,\n",
       "  'KNeighborsDist_BAG_L1': 0.20378732681274414,\n",
       "  'LightGBMXT_BAG_L1': 5.490896940231323,\n",
       "  'LightGBM_BAG_L1': 1.2285606861114502,\n",
       "  'RandomForestMSE_BAG_L1': 0.4007608890533447,\n",
       "  'ExtraTreesMSE_BAG_L1': 0.4028153419494629,\n",
       "  'NeuralNetFastAI_BAG_L1': 0.35155415534973145,\n",
       "  'XGBoost_BAG_L1': 0.3242228031158447,\n",
       "  'NeuralNetMXNet_BAG_L1': 2.2607109546661377,\n",
       "  'WeightedEnsemble_L2': 0.0007569789886474609,\n",
       "  'LightGBMXT_BAG_L2': 0.1567552089691162,\n",
       "  'LightGBM_BAG_L2': 0.09721159934997559,\n",
       "  'RandomForestMSE_BAG_L2': 0.4623403549194336,\n",
       "  'CatBoost_BAG_L2': 0.06495308876037598,\n",
       "  'ExtraTreesMSE_BAG_L2': 0.4643137454986572,\n",
       "  'NeuralNetFastAI_BAG_L2': 0.38712334632873535,\n",
       "  'WeightedEnsemble_L3': 0.0007224082946777344},\n",
       " 'num_bag_folds': 10,\n",
       " 'max_stack_level': 3,\n",
       " 'model_hyperparams': {'KNeighborsUnif_BAG_L1': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True,\n",
       "   'use_child_oof': True},\n",
       "  'KNeighborsDist_BAG_L1': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True,\n",
       "   'use_child_oof': True},\n",
       "  'LightGBMXT_BAG_L1': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'LightGBM_BAG_L1': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'RandomForestMSE_BAG_L1': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True,\n",
       "   'use_child_oof': True},\n",
       "  'ExtraTreesMSE_BAG_L1': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True,\n",
       "   'use_child_oof': True},\n",
       "  'NeuralNetFastAI_BAG_L1': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'XGBoost_BAG_L1': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'NeuralNetMXNet_BAG_L1': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'WeightedEnsemble_L2': {'use_orig_features': False,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'LightGBMXT_BAG_L2': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'LightGBM_BAG_L2': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'RandomForestMSE_BAG_L2': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True,\n",
       "   'use_child_oof': True},\n",
       "  'CatBoost_BAG_L2': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'ExtraTreesMSE_BAG_L2': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True,\n",
       "   'use_child_oof': True},\n",
       "  'NeuralNetFastAI_BAG_L2': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'WeightedEnsemble_L3': {'use_orig_features': False,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True}},\n",
       " 'leaderboard':                      model   score_val  pred_time_val    fit_time  \\\n",
       " 0      WeightedEnsemble_L2  -35.186484       7.796752  302.526579   \n",
       " 1      WeightedEnsemble_L3  -35.245798      12.343088  510.300913   \n",
       " 2          CatBoost_BAG_L2  -35.577795      10.931376  372.807618   \n",
       " 3     ExtraTreesMSE_BAG_L2  -35.695318      11.330737  337.485581   \n",
       " 4          LightGBM_BAG_L2  -35.763061      10.963635  337.815503   \n",
       " 5   NeuralNetFastAI_BAG_L2  -35.977238      11.253547  425.997911   \n",
       " 6   RandomForestMSE_BAG_L2  -36.017326      11.328764  358.422538   \n",
       " 7        LightGBMXT_BAG_L2  -36.266589      11.023179  338.969662   \n",
       " 8          LightGBM_BAG_L1  -36.579153       1.228561   30.240943   \n",
       " 9        LightGBMXT_BAG_L1  -36.941451       5.490897   97.249026   \n",
       " 10          XGBoost_BAG_L1  -37.399540       0.324223   31.757440   \n",
       " 11    ExtraTreesMSE_BAG_L1  -40.824193       0.402815    3.931156   \n",
       " 12  RandomForestMSE_BAG_L1  -41.307546       0.400761    9.544247   \n",
       " 13  NeuralNetFastAI_BAG_L1  -46.496409       0.351554  133.277896   \n",
       " 14   KNeighborsDist_BAG_L1 -119.373394       0.203787    0.021954   \n",
       " 15   KNeighborsUnif_BAG_L1 -123.922640       0.203114    0.031451   \n",
       " 16   NeuralNetMXNet_BAG_L1 -246.347726       2.260711   24.583085   \n",
       " \n",
       "     pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
       " 0                 0.000757           0.457028            2       True   \n",
       " 1                 0.000722           0.320552            3       True   \n",
       " 2                 0.064953          42.170421            2       True   \n",
       " 3                 0.464314           6.848383            2       True   \n",
       " 4                 0.097212           7.178305            2       True   \n",
       " 5                 0.387123          95.360714            2       True   \n",
       " 6                 0.462340          27.785340            2       True   \n",
       " 7                 0.156755           8.332464            2       True   \n",
       " 8                 1.228561          30.240943            1       True   \n",
       " 9                 5.490897          97.249026            1       True   \n",
       " 10                0.324223          31.757440            1       True   \n",
       " 11                0.402815           3.931156            1       True   \n",
       " 12                0.400761           9.544247            1       True   \n",
       " 13                0.351554         133.277896            1       True   \n",
       " 14                0.203787           0.021954            1       True   \n",
       " 15                0.203114           0.031451            1       True   \n",
       " 16                2.260711          24.583085            1       True   \n",
       " \n",
       "     fit_order  \n",
       " 0          10  \n",
       " 1          17  \n",
       " 2          14  \n",
       " 3          15  \n",
       " 4          12  \n",
       " 5          16  \n",
       " 6          13  \n",
       " 7          11  \n",
       " 8           4  \n",
       " 9           3  \n",
       " 10          8  \n",
       " 11          6  \n",
       " 12          5  \n",
       " 13          7  \n",
       " 14          2  \n",
       " 15          1  \n",
       " 16          9  }"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor_new_features.fit_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remember to set all negative values to zero\n",
    "predictions_new_features = predictor_new_features.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "107"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many negative values do we have after removal?\n",
    "counter=0\n",
    "for pred in predictions_new_features:\n",
    "    if pred < 0:\n",
    "        counter=counter+1\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set them to zero\n",
    "for index in predictions_new_features.keys():\n",
    "    if predictions_new_features[index] < 0:\n",
    "        predictions_new_features[index]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many negative values do we have after removal?\n",
    "counter=0\n",
    "for pred in predictions_new_features:\n",
    "    if pred < 0:\n",
    "        counter=counter+1\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-20 00:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-20 01:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-20 02:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-01-20 03:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-01-20 04:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              datetime  count\n",
       "0  2011-01-20 00:00:00      0\n",
       "1  2011-01-20 01:00:00      0\n",
       "2  2011-01-20 02:00:00      0\n",
       "3  2011-01-20 03:00:00      0\n",
       "4  2011-01-20 04:00:00      0"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize values\n",
    "submission_new_features = pd.read_csv(\"sampleSubmission.csv\")\n",
    "submission_new_features.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same submitting predictions\n",
    "submission_new_features[\"count\"] = predictions_new_features.values\n",
    "submission_new_features.to_csv(\"submission_new_features.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 188k/188k [00:00<00:00, 459kB/s]\n",
      "Successfully submitted to Bike Sharing Demand"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c bike-sharing-demand -f submission_new_features.csv -m \"new features\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fileName                     date                 description                             status    publicScore  privateScore  \n",
      "---------------------------  -------------------  --------------------------------------  --------  -----------  ------------  \n",
      "submission_new_features.csv  2022-02-02 19:54:58  new features                            complete  0.55149      0.55149       \n",
      "submission.csv               2022-02-02 19:29:26  first raw submission                    complete  1.39487      1.39487       \n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submissions -c bike-sharing-demand | tail -n +1 | head -n 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New Score of `0.55149`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Hyper parameter optimization\n",
    "* There are many options for hyper parameter optimization.\n",
    "* Options are to change the AutoGluon higher level parameters or the individual model hyperparameters.\n",
    "* The hyperparameters of the models themselves that are in AutoGluon. Those need the `hyperparameter` and `hyperparameter_tune_kwargs` arguments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20220202_195656/\"\n",
      "Presets specified: ['best_quality']\n",
      "Warning: hyperparameter tuning is currently experimental and may cause the process to hang.\n",
      "Beginning AutoGluon training ... Time limit = 600s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20220202_195656/\"\n",
      "AutoGluon Version:  0.3.1\n",
      "Train Data Rows:    10886\n",
      "Train Data Columns: 12\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2269.53 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.89 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 2 | ['season', 'weather']\n",
      "\t\t('datetime', []) : 1 | ['datetime']\n",
      "\t\t('float', [])    : 3 | ['temp', 'atemp', 'windspeed']\n",
      "\t\t('int', [])      : 6 | ['holiday', 'workingday', 'humidity', 'hour', 'month', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])             : 2 | ['season', 'weather']\n",
      "\t\t('float', [])                : 3 | ['temp', 'atemp', 'windspeed']\n",
      "\t\t('int', [])                  : 4 | ['humidity', 'hour', 'month', 'day']\n",
      "\t\t('int', ['bool'])            : 2 | ['holiday', 'workingday']\n",
      "\t\t('int', ['datetime_as_int']) : 1 | ['datetime']\n",
      "\t0.1s = Fit runtime\n",
      "\t12 features in original data used to generate 12 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.74 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.15s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tTo change this, specify the eval_metric argument of fit()\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 11 L1 models ...\n",
      "Hyperparameter tuning model: KNeighborsUnif_BAG_L1 ...\n",
      "Fitted model: KNeighborsUnif_BAG_L1/T0 ...\n",
      "\t-123.9226\t = Validation score   (root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Hyperparameter tuning model: KNeighborsDist_BAG_L1 ...\n",
      "Fitted model: KNeighborsDist_BAG_L1/T0 ...\n",
      "\t-119.3734\t = Validation score   (root_mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Hyperparameter tuning model: LightGBMXT_BAG_L1 ...\n",
      "Warning: Exception caused LightGBMXT_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 103, in status_printer\n",
      "    pbar = IntProgress(min=0, max=total)\n",
      "NameError: name 'IntProgress' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 1153, in _train_single_full\n",
      "    hpo_models, hpo_model_performances, hpo_results = model.hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=hyperparameter_tune_kwargs, **model_fit_kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 179, in _hyperparameter_tune\n",
      "    return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=scheduler_options, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 801, in _hyperparameter_tune\n",
      "    hpo_models, hpo_model_performances, hpo_results = self.model_base.hyperparameter_tune(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 391, in _hyperparameter_tune\n",
      "    scheduler.run()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/scheduler/seq_scheduler.py\", line 152, in run\n",
      "    for i in (tqdm(r) if self.num_trials < 1000 else r):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 213, in __init__\n",
      "    self.fp, total, self.desc, self.ncols)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 111, in status_printer\n",
      "    \"IntProgress not found. Please update jupyter and ipywidgets.\"\n",
      "ImportError: IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "Hyperparameter tuning model: LightGBM_BAG_L1 ...\n",
      "Warning: Exception caused LightGBM_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 103, in status_printer\n",
      "    pbar = IntProgress(min=0, max=total)\n",
      "NameError: name 'IntProgress' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 1153, in _train_single_full\n",
      "    hpo_models, hpo_model_performances, hpo_results = model.hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=hyperparameter_tune_kwargs, **model_fit_kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 179, in _hyperparameter_tune\n",
      "    return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=scheduler_options, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 801, in _hyperparameter_tune\n",
      "    hpo_models, hpo_model_performances, hpo_results = self.model_base.hyperparameter_tune(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 391, in _hyperparameter_tune\n",
      "    scheduler.run()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/scheduler/seq_scheduler.py\", line 152, in run\n",
      "    for i in (tqdm(r) if self.num_trials < 1000 else r):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 213, in __init__\n",
      "    self.fp, total, self.desc, self.ncols)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 111, in status_printer\n",
      "    \"IntProgress not found. Please update jupyter and ipywidgets.\"\n",
      "ImportError: IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "Hyperparameter tuning model: RandomForestMSE_BAG_L1 ...\n",
      "Fitted model: RandomForestMSE_BAG_L1/T0 ...\n",
      "\t-41.3075\t = Validation score   (root_mean_squared_error)\n",
      "\t9.54s\t = Training   runtime\n",
      "\t0.42s\t = Validation runtime\n",
      "Hyperparameter tuning model: CatBoost_BAG_L1 ...\n",
      "Warning: Exception caused CatBoost_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 103, in status_printer\n",
      "    pbar = IntProgress(min=0, max=total)\n",
      "NameError: name 'IntProgress' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 1153, in _train_single_full\n",
      "    hpo_models, hpo_model_performances, hpo_results = model.hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=hyperparameter_tune_kwargs, **model_fit_kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 179, in _hyperparameter_tune\n",
      "    return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=scheduler_options, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 801, in _hyperparameter_tune\n",
      "    hpo_models, hpo_model_performances, hpo_results = self.model_base.hyperparameter_tune(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 887, in _hyperparameter_tune\n",
      "    scheduler.run()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/scheduler/seq_scheduler.py\", line 152, in run\n",
      "    for i in (tqdm(r) if self.num_trials < 1000 else r):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 213, in __init__\n",
      "    self.fp, total, self.desc, self.ncols)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 111, in status_printer\n",
      "    \"IntProgress not found. Please update jupyter and ipywidgets.\"\n",
      "ImportError: IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "Hyperparameter tuning model: ExtraTreesMSE_BAG_L1 ...\n",
      "Fitted model: ExtraTreesMSE_BAG_L1/T0 ...\n",
      "\t-40.8242\t = Validation score   (root_mean_squared_error)\n",
      "\t3.83s\t = Training   runtime\n",
      "\t0.4s\t = Validation runtime\n",
      "Hyperparameter tuning model: NeuralNetFastAI_BAG_L1 ...\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 5)\n",
      "Fitted model: NeuralNetFastAI_BAG_L1/T0 ...\n",
      "\t-126.4374\t = Validation score   (root_mean_squared_error)\n",
      "\t2.46s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Hyperparameter tuning model: XGBoost_BAG_L1 ...\n",
      "Warning: Exception caused XGBoost_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 103, in status_printer\n",
      "    pbar = IntProgress(min=0, max=total)\n",
      "NameError: name 'IntProgress' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 1153, in _train_single_full\n",
      "    hpo_models, hpo_model_performances, hpo_results = model.hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=hyperparameter_tune_kwargs, **model_fit_kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 179, in _hyperparameter_tune\n",
      "    return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=scheduler_options, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 801, in _hyperparameter_tune\n",
      "    hpo_models, hpo_model_performances, hpo_results = self.model_base.hyperparameter_tune(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 887, in _hyperparameter_tune\n",
      "    scheduler.run()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/scheduler/seq_scheduler.py\", line 152, in run\n",
      "    for i in (tqdm(r) if self.num_trials < 1000 else r):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 213, in __init__\n",
      "    self.fp, total, self.desc, self.ncols)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 111, in status_printer\n",
      "    \"IntProgress not found. Please update jupyter and ipywidgets.\"\n",
      "ImportError: IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "Hyperparameter tuning model: NeuralNetMXNet_BAG_L1 ...\n",
      "Warning: Exception caused NeuralNetMXNet_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 103, in status_printer\n",
      "    pbar = IntProgress(min=0, max=total)\n",
      "NameError: name 'IntProgress' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 1153, in _train_single_full\n",
      "    hpo_models, hpo_model_performances, hpo_results = model.hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=hyperparameter_tune_kwargs, **model_fit_kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 179, in _hyperparameter_tune\n",
      "    return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=scheduler_options, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 801, in _hyperparameter_tune\n",
      "    hpo_models, hpo_model_performances, hpo_results = self.model_base.hyperparameter_tune(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/tabular/models/tabular_nn/tabular_nn_model.py\", line 749, in _hyperparameter_tune\n",
      "    scheduler.run()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/scheduler/seq_scheduler.py\", line 152, in run\n",
      "    for i in (tqdm(r) if self.num_trials < 1000 else r):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 213, in __init__\n",
      "    self.fp, total, self.desc, self.ncols)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 111, in status_printer\n",
      "    \"IntProgress not found. Please update jupyter and ipywidgets.\"\n",
      "ImportError: IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 3.27s of the 569.29s of remaining time.\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\tRan out of time, early stopping on iteration 767. Best iteration is:\n",
      "\t[767]\ttrain_set's rmse: 11.1185\tvalid_set's rmse: 34.7573\n",
      "\t-34.7573\t = Validation score   (root_mean_squared_error)\n",
      "\t3.12s\t = Training   runtime\n",
      "\t0.07s\t = Validation runtime\n",
      "Fitting model: KNeighborsUnif_BAG_L1/T0 ... Training model for up to 365.39s of the 565.43s of remaining time.\n",
      "\t-123.9226\t = Validation score   (root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1/T0 ... Training model for up to 365.29s of the 565.34s of remaining time.\n",
      "\t-119.3734\t = Validation score   (root_mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1/T0 ... Training model for up to 365.21s of the 565.26s of remaining time.\n",
      "\t-41.3075\t = Validation score   (root_mean_squared_error)\n",
      "\t9.54s\t = Training   runtime\n",
      "\t0.42s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1/T0 ... Training model for up to 365.13s of the 565.18s of remaining time.\n",
      "\t-40.8242\t = Validation score   (root_mean_squared_error)\n",
      "\t3.83s\t = Training   runtime\n",
      "\t0.4s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1/T0 ... Training model for up to 365.05s of the 565.09s of remaining time.\n",
      "\t-59.6008\t = Validation score   (root_mean_squared_error)\n",
      "\t121.93s\t = Training   runtime\n",
      "\t0.35s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 244.35s of the 444.39s of remaining time.\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 8.84519\tvalid_set's rmse: 36.334\n",
      "[2000]\ttrain_set's rmse: 4.14753\tvalid_set's rmse: 36.122\n",
      "[3000]\ttrain_set's rmse: 2.20644\tvalid_set's rmse: 36.0657\n",
      "[4000]\ttrain_set's rmse: 1.26036\tvalid_set's rmse: 36.0607\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 9.0955\tvalid_set's rmse: 33.0768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 8.80966\tvalid_set's rmse: 36.8276\n",
      "[2000]\ttrain_set's rmse: 4.28416\tvalid_set's rmse: 36.6523\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 8.90564\tvalid_set's rmse: 36.7088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 8.82007\tvalid_set's rmse: 34.5278\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 9.09881\tvalid_set's rmse: 36.8336\n",
      "[2000]\ttrain_set's rmse: 4.36071\tvalid_set's rmse: 36.4808\n",
      "[3000]\ttrain_set's rmse: 2.40468\tvalid_set's rmse: 36.4265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-36.2112\t = Validation score   (root_mean_squared_error)\n",
      "\t65.47s\t = Training   runtime\n",
      "\t1.38s\t = Validation runtime\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 371.3s of remaining time.\n",
      "\t-36.0912\t = Validation score   (root_mean_squared_error)\n",
      "\t0.32s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 9 L2 models ...\n",
      "Hyperparameter tuning model: LightGBMXT_BAG_L2 ...\n",
      "Warning: Exception caused LightGBMXT_BAG_L2 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 103, in status_printer\n",
      "    pbar = IntProgress(min=0, max=total)\n",
      "NameError: name 'IntProgress' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 1153, in _train_single_full\n",
      "    hpo_models, hpo_model_performances, hpo_results = model.hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=hyperparameter_tune_kwargs, **model_fit_kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 179, in _hyperparameter_tune\n",
      "    return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=scheduler_options, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 801, in _hyperparameter_tune\n",
      "    hpo_models, hpo_model_performances, hpo_results = self.model_base.hyperparameter_tune(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 391, in _hyperparameter_tune\n",
      "    scheduler.run()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/scheduler/seq_scheduler.py\", line 152, in run\n",
      "    for i in (tqdm(r) if self.num_trials < 1000 else r):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 213, in __init__\n",
      "    self.fp, total, self.desc, self.ncols)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 111, in status_printer\n",
      "    \"IntProgress not found. Please update jupyter and ipywidgets.\"\n",
      "ImportError: IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "Hyperparameter tuning model: LightGBM_BAG_L2 ...\n",
      "Warning: Exception caused LightGBM_BAG_L2 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 103, in status_printer\n",
      "    pbar = IntProgress(min=0, max=total)\n",
      "NameError: name 'IntProgress' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 1153, in _train_single_full\n",
      "    hpo_models, hpo_model_performances, hpo_results = model.hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=hyperparameter_tune_kwargs, **model_fit_kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 179, in _hyperparameter_tune\n",
      "    return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=scheduler_options, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 801, in _hyperparameter_tune\n",
      "    hpo_models, hpo_model_performances, hpo_results = self.model_base.hyperparameter_tune(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 391, in _hyperparameter_tune\n",
      "    scheduler.run()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/scheduler/seq_scheduler.py\", line 152, in run\n",
      "    for i in (tqdm(r) if self.num_trials < 1000 else r):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 213, in __init__\n",
      "    self.fp, total, self.desc, self.ncols)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 111, in status_printer\n",
      "    \"IntProgress not found. Please update jupyter and ipywidgets.\"\n",
      "ImportError: IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "Hyperparameter tuning model: RandomForestMSE_BAG_L2 ...\n",
      "Fitted model: RandomForestMSE_BAG_L2/T0 ...\n",
      "\t-37.345\t = Validation score   (root_mean_squared_error)\n",
      "\t22.47s\t = Training   runtime\n",
      "\t0.46s\t = Validation runtime\n",
      "Hyperparameter tuning model: CatBoost_BAG_L2 ...\n",
      "Warning: Exception caused CatBoost_BAG_L2 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 103, in status_printer\n",
      "    pbar = IntProgress(min=0, max=total)\n",
      "NameError: name 'IntProgress' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 1153, in _train_single_full\n",
      "    hpo_models, hpo_model_performances, hpo_results = model.hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=hyperparameter_tune_kwargs, **model_fit_kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 179, in _hyperparameter_tune\n",
      "    return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=scheduler_options, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 801, in _hyperparameter_tune\n",
      "    hpo_models, hpo_model_performances, hpo_results = self.model_base.hyperparameter_tune(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 887, in _hyperparameter_tune\n",
      "    scheduler.run()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/scheduler/seq_scheduler.py\", line 152, in run\n",
      "    for i in (tqdm(r) if self.num_trials < 1000 else r):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 213, in __init__\n",
      "    self.fp, total, self.desc, self.ncols)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 111, in status_printer\n",
      "    \"IntProgress not found. Please update jupyter and ipywidgets.\"\n",
      "ImportError: IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "Hyperparameter tuning model: ExtraTreesMSE_BAG_L2 ...\n",
      "Fitted model: ExtraTreesMSE_BAG_L2/T0 ...\n",
      "\t-36.5949\t = Validation score   (root_mean_squared_error)\n",
      "\t6.04s\t = Training   runtime\n",
      "\t0.45s\t = Validation runtime\n",
      "Hyperparameter tuning model: NeuralNetFastAI_BAG_L2 ...\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 5)\n",
      "Fitted model: NeuralNetFastAI_BAG_L2/T0 ...\n",
      "\t-39.5282\t = Validation score   (root_mean_squared_error)\n",
      "\t2.64s\t = Training   runtime\n",
      "\t0.04s\t = Validation runtime\n",
      "Hyperparameter tuning model: XGBoost_BAG_L2 ...\n",
      "Warning: Exception caused XGBoost_BAG_L2 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 103, in status_printer\n",
      "    pbar = IntProgress(min=0, max=total)\n",
      "NameError: name 'IntProgress' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 1153, in _train_single_full\n",
      "    hpo_models, hpo_model_performances, hpo_results = model.hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=hyperparameter_tune_kwargs, **model_fit_kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 179, in _hyperparameter_tune\n",
      "    return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=scheduler_options, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 801, in _hyperparameter_tune\n",
      "    hpo_models, hpo_model_performances, hpo_results = self.model_base.hyperparameter_tune(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 887, in _hyperparameter_tune\n",
      "    scheduler.run()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/scheduler/seq_scheduler.py\", line 152, in run\n",
      "    for i in (tqdm(r) if self.num_trials < 1000 else r):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 213, in __init__\n",
      "    self.fp, total, self.desc, self.ncols)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 111, in status_printer\n",
      "    \"IntProgress not found. Please update jupyter and ipywidgets.\"\n",
      "ImportError: IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "Hyperparameter tuning model: NeuralNetMXNet_BAG_L2 ...\n",
      "Warning: Exception caused NeuralNetMXNet_BAG_L2 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 103, in status_printer\n",
      "    pbar = IntProgress(min=0, max=total)\n",
      "NameError: name 'IntProgress' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 1153, in _train_single_full\n",
      "    hpo_models, hpo_model_performances, hpo_results = model.hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=hyperparameter_tune_kwargs, **model_fit_kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 179, in _hyperparameter_tune\n",
      "    return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=scheduler_options, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 801, in _hyperparameter_tune\n",
      "    hpo_models, hpo_model_performances, hpo_results = self.model_base.hyperparameter_tune(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/tabular/models/tabular_nn/tabular_nn_model.py\", line 749, in _hyperparameter_tune\n",
      "    scheduler.run()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/scheduler/seq_scheduler.py\", line 152, in run\n",
      "    for i in (tqdm(r) if self.num_trials < 1000 else r):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 213, in __init__\n",
      "    self.fp, total, self.desc, self.ncols)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 111, in status_printer\n",
      "    \"IntProgress not found. Please update jupyter and ipywidgets.\"\n",
      "ImportError: IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 3.71s of the 326.03s of remaining time.\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\t-39.0273\t = Validation score   (root_mean_squared_error)\n",
      "\t1.72s\t = Training   runtime\n",
      "\t0.01s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2/T0 ... Training model for up to 324.07s of the 324.05s of remaining time.\n",
      "\t-37.345\t = Validation score   (root_mean_squared_error)\n",
      "\t22.47s\t = Training   runtime\n",
      "\t0.46s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L2/T0 ... Training model for up to 323.99s of the 323.97s of remaining time.\n",
      "\t-36.5949\t = Validation score   (root_mean_squared_error)\n",
      "\t6.04s\t = Training   runtime\n",
      "\t0.45s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2/T0 ... Training model for up to 323.91s of the 323.89s of remaining time.\n",
      "\t-36.7725\t = Validation score   (root_mean_squared_error)\n",
      "\t123.58s\t = Training   runtime\n",
      "\t0.38s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 201.67s of the 201.65s of remaining time.\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\t-37.2856\t = Validation score   (root_mean_squared_error)\n",
      "\t17.12s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Completed 1/20 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the 184.59s of remaining time.\n",
      "\t-36.2383\t = Validation score   (root_mean_squared_error)\n",
      "\t0.23s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 415.92s ...\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20220202_195656/\")\n"
     ]
    }
   ],
   "source": [
    "num_trials = 5  # try at most 5 different hyperparameter configurations for each type of model\n",
    "search_strategy = 'auto'  # to tune hyperparameters using random search routine\n",
    "\n",
    "hyperparameter_tune_kwargs = {  # HPO is not performed unless hyperparameter_tune_kwargs is specified\n",
    "    'num_trials': num_trials,\n",
    "    'scheduler' : 'local',\n",
    "    'searcher': search_strategy\n",
    "}\n",
    "predictor_new_hpo = TabularPredictor(label=\"count\",eval_metric=\"root_mean_squared_error\",problem_type=\"regression\").fit(train_data=train.loc[:, ~train.columns.isin(['casual', 'registered'])], time_limit=600, presets=\"best_quality\",hyperparameter_tune_kwargs=hyperparameter_tune_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                        model   score_val  pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0         WeightedEnsemble_L2  -36.091199       2.129336  191.550136                0.000601           0.317499            2       True          7\n",
      "1        LightGBMLarge_BAG_L1  -36.211158       1.381140   65.467891                1.381140          65.467891            1       True          6\n",
      "2         WeightedEnsemble_L3  -36.238301       3.926142  347.788011                0.000679           0.226422            3       True         12\n",
      "3     ExtraTreesMSE_BAG_L2/T0  -36.594864       3.407551  206.862270                0.452794           6.036594            2       True          9\n",
      "4   NeuralNetFastAI_BAG_L2/T0  -36.772529       3.331507  324.408333                0.376751         123.582657            2       True         10\n",
      "5        LightGBMLarge_BAG_L2  -37.285577       3.095918  217.942338                0.141162          17.116662            2       True         11\n",
      "6   RandomForestMSE_BAG_L2/T0  -37.345021       3.410032  223.291251                0.455276          22.465575            2       True          8\n",
      "7     ExtraTreesMSE_BAG_L1/T0  -40.824193       0.399886    3.831477                0.399886           3.831477            1       True          4\n",
      "8   RandomForestMSE_BAG_L1/T0  -41.307546       0.417946    9.542601                0.417946           9.542601            1       True          3\n",
      "9   NeuralNetFastAI_BAG_L1/T0  -59.600840       0.347709  121.933269                0.347709         121.933269            1       True          5\n",
      "10   KNeighborsDist_BAG_L1/T0 -119.373394       0.204067    0.019544                0.204067           0.019544            1       True          2\n",
      "11   KNeighborsUnif_BAG_L1/T0 -123.922640       0.204008    0.030894                0.204008           0.030894            1       True          1\n",
      "Number of models trained: 12\n",
      "Types of models trained:\n",
      "{'StackerEnsembleModel_NNFastAiTabular', 'StackerEnsembleModel_KNN', 'StackerEnsembleModel_RF', 'WeightedEnsembleModel', 'StackerEnsembleModel_LGB', 'StackerEnsembleModel_XT'}\n",
      "Bagging used: True  (with 10 folds)\n",
      "Multi-layer stack-ensembling used: True  (with 3 levels)\n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('category', [])             : 2 | ['season', 'weather']\n",
      "('float', [])                : 3 | ['temp', 'atemp', 'windspeed']\n",
      "('int', [])                  : 4 | ['humidity', 'hour', 'month', 'day']\n",
      "('int', ['bool'])            : 2 | ['holiday', 'workingday']\n",
      "('int', ['datetime_as_int']) : 1 | ['datetime']\n",
      "Plot summary of models saved to file: AutogluonModels/ag-20220202_195656/SummaryOfModels.html\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_types': {'KNeighborsUnif_BAG_L1/T0': 'StackerEnsembleModel_KNN',\n",
       "  'KNeighborsDist_BAG_L1/T0': 'StackerEnsembleModel_KNN',\n",
       "  'RandomForestMSE_BAG_L1/T0': 'StackerEnsembleModel_RF',\n",
       "  'ExtraTreesMSE_BAG_L1/T0': 'StackerEnsembleModel_XT',\n",
       "  'NeuralNetFastAI_BAG_L1/T0': 'StackerEnsembleModel_NNFastAiTabular',\n",
       "  'LightGBMLarge_BAG_L1': 'StackerEnsembleModel_LGB',\n",
       "  'WeightedEnsemble_L2': 'WeightedEnsembleModel',\n",
       "  'RandomForestMSE_BAG_L2/T0': 'StackerEnsembleModel_RF',\n",
       "  'ExtraTreesMSE_BAG_L2/T0': 'StackerEnsembleModel_XT',\n",
       "  'NeuralNetFastAI_BAG_L2/T0': 'StackerEnsembleModel_NNFastAiTabular',\n",
       "  'LightGBMLarge_BAG_L2': 'StackerEnsembleModel_LGB',\n",
       "  'WeightedEnsemble_L3': 'WeightedEnsembleModel'},\n",
       " 'model_performance': {'KNeighborsUnif_BAG_L1/T0': -123.92263994015985,\n",
       "  'KNeighborsDist_BAG_L1/T0': -119.37339448003198,\n",
       "  'RandomForestMSE_BAG_L1/T0': -41.30754583307658,\n",
       "  'ExtraTreesMSE_BAG_L1/T0': -40.82419267315799,\n",
       "  'NeuralNetFastAI_BAG_L1/T0': -59.600839937880465,\n",
       "  'LightGBMLarge_BAG_L1': -36.211157542265255,\n",
       "  'WeightedEnsemble_L2': -36.09119895644588,\n",
       "  'RandomForestMSE_BAG_L2/T0': -37.34502141968657,\n",
       "  'ExtraTreesMSE_BAG_L2/T0': -36.59486413945162,\n",
       "  'NeuralNetFastAI_BAG_L2/T0': -36.77252941525855,\n",
       "  'LightGBMLarge_BAG_L2': -37.285576936052784,\n",
       "  'WeightedEnsemble_L3': -36.23830051860186},\n",
       " 'model_best': 'WeightedEnsemble_L2',\n",
       " 'model_paths': {'KNeighborsUnif_BAG_L1/T0': 'AutogluonModels/ag-20220202_195656/models/KNeighborsUnif_BAG_L1/T0/',\n",
       "  'KNeighborsDist_BAG_L1/T0': 'AutogluonModels/ag-20220202_195656/models/KNeighborsDist_BAG_L1/T0/',\n",
       "  'RandomForestMSE_BAG_L1/T0': 'AutogluonModels/ag-20220202_195656/models/RandomForestMSE_BAG_L1/T0/',\n",
       "  'ExtraTreesMSE_BAG_L1/T0': 'AutogluonModels/ag-20220202_195656/models/ExtraTreesMSE_BAG_L1/T0/',\n",
       "  'NeuralNetFastAI_BAG_L1/T0': 'AutogluonModels/ag-20220202_195656/models/NeuralNetFastAI_BAG_L1/T0/',\n",
       "  'LightGBMLarge_BAG_L1': 'AutogluonModels/ag-20220202_195656/models/LightGBMLarge_BAG_L1/',\n",
       "  'WeightedEnsemble_L2': 'AutogluonModels/ag-20220202_195656/models/WeightedEnsemble_L2/',\n",
       "  'RandomForestMSE_BAG_L2/T0': 'AutogluonModels/ag-20220202_195656/models/RandomForestMSE_BAG_L2/T0/',\n",
       "  'ExtraTreesMSE_BAG_L2/T0': 'AutogluonModels/ag-20220202_195656/models/ExtraTreesMSE_BAG_L2/T0/',\n",
       "  'NeuralNetFastAI_BAG_L2/T0': 'AutogluonModels/ag-20220202_195656/models/NeuralNetFastAI_BAG_L2/T0/',\n",
       "  'LightGBMLarge_BAG_L2': 'AutogluonModels/ag-20220202_195656/models/LightGBMLarge_BAG_L2/',\n",
       "  'WeightedEnsemble_L3': 'AutogluonModels/ag-20220202_195656/models/WeightedEnsemble_L3/'},\n",
       " 'model_fit_times': {'KNeighborsUnif_BAG_L1/T0': 0.030893564224243164,\n",
       "  'KNeighborsDist_BAG_L1/T0': 0.019544124603271484,\n",
       "  'RandomForestMSE_BAG_L1/T0': 9.542601108551025,\n",
       "  'ExtraTreesMSE_BAG_L1/T0': 3.831476926803589,\n",
       "  'NeuralNetFastAI_BAG_L1/T0': 121.93326878547668,\n",
       "  'LightGBMLarge_BAG_L1': 65.46789121627808,\n",
       "  'WeightedEnsemble_L2': 0.31749892234802246,\n",
       "  'RandomForestMSE_BAG_L2/T0': 22.465575456619263,\n",
       "  'ExtraTreesMSE_BAG_L2/T0': 6.036593914031982,\n",
       "  'NeuralNetFastAI_BAG_L2/T0': 123.58265733718872,\n",
       "  'LightGBMLarge_BAG_L2': 17.11666202545166,\n",
       "  'WeightedEnsemble_L3': 0.22642159461975098},\n",
       " 'model_pred_times': {'KNeighborsUnif_BAG_L1/T0': 0.20400786399841309,\n",
       "  'KNeighborsDist_BAG_L1/T0': 0.20406723022460938,\n",
       "  'RandomForestMSE_BAG_L1/T0': 0.41794633865356445,\n",
       "  'ExtraTreesMSE_BAG_L1/T0': 0.3998861312866211,\n",
       "  'NeuralNetFastAI_BAG_L1/T0': 0.34770917892456055,\n",
       "  'LightGBMLarge_BAG_L1': 1.3811397552490234,\n",
       "  'WeightedEnsemble_L2': 0.0006012916564941406,\n",
       "  'RandomForestMSE_BAG_L2/T0': 0.4552755355834961,\n",
       "  'ExtraTreesMSE_BAG_L2/T0': 0.45279431343078613,\n",
       "  'NeuralNetFastAI_BAG_L2/T0': 0.3767507076263428,\n",
       "  'LightGBMLarge_BAG_L2': 0.14116168022155762,\n",
       "  'WeightedEnsemble_L3': 0.0006792545318603516},\n",
       " 'num_bag_folds': 10,\n",
       " 'max_stack_level': 3,\n",
       " 'model_hyperparams': {'KNeighborsUnif_BAG_L1/T0': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True,\n",
       "   'use_child_oof': True},\n",
       "  'KNeighborsDist_BAG_L1/T0': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True,\n",
       "   'use_child_oof': True},\n",
       "  'RandomForestMSE_BAG_L1/T0': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True,\n",
       "   'use_child_oof': True},\n",
       "  'ExtraTreesMSE_BAG_L1/T0': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True,\n",
       "   'use_child_oof': True},\n",
       "  'NeuralNetFastAI_BAG_L1/T0': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'LightGBMLarge_BAG_L1': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'WeightedEnsemble_L2': {'use_orig_features': False,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'RandomForestMSE_BAG_L2/T0': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True,\n",
       "   'use_child_oof': True},\n",
       "  'ExtraTreesMSE_BAG_L2/T0': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True,\n",
       "   'use_child_oof': True},\n",
       "  'NeuralNetFastAI_BAG_L2/T0': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'LightGBMLarge_BAG_L2': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'WeightedEnsemble_L3': {'use_orig_features': False,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True}},\n",
       " 'leaderboard':                         model   score_val  pred_time_val    fit_time  \\\n",
       " 0         WeightedEnsemble_L2  -36.091199       2.129336  191.550136   \n",
       " 1        LightGBMLarge_BAG_L1  -36.211158       1.381140   65.467891   \n",
       " 2         WeightedEnsemble_L3  -36.238301       3.926142  347.788011   \n",
       " 3     ExtraTreesMSE_BAG_L2/T0  -36.594864       3.407551  206.862270   \n",
       " 4   NeuralNetFastAI_BAG_L2/T0  -36.772529       3.331507  324.408333   \n",
       " 5        LightGBMLarge_BAG_L2  -37.285577       3.095918  217.942338   \n",
       " 6   RandomForestMSE_BAG_L2/T0  -37.345021       3.410032  223.291251   \n",
       " 7     ExtraTreesMSE_BAG_L1/T0  -40.824193       0.399886    3.831477   \n",
       " 8   RandomForestMSE_BAG_L1/T0  -41.307546       0.417946    9.542601   \n",
       " 9   NeuralNetFastAI_BAG_L1/T0  -59.600840       0.347709  121.933269   \n",
       " 10   KNeighborsDist_BAG_L1/T0 -119.373394       0.204067    0.019544   \n",
       " 11   KNeighborsUnif_BAG_L1/T0 -123.922640       0.204008    0.030894   \n",
       " \n",
       "     pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
       " 0                 0.000601           0.317499            2       True   \n",
       " 1                 1.381140          65.467891            1       True   \n",
       " 2                 0.000679           0.226422            3       True   \n",
       " 3                 0.452794           6.036594            2       True   \n",
       " 4                 0.376751         123.582657            2       True   \n",
       " 5                 0.141162          17.116662            2       True   \n",
       " 6                 0.455276          22.465575            2       True   \n",
       " 7                 0.399886           3.831477            1       True   \n",
       " 8                 0.417946           9.542601            1       True   \n",
       " 9                 0.347709         121.933269            1       True   \n",
       " 10                0.204067           0.019544            1       True   \n",
       " 11                0.204008           0.030894            1       True   \n",
       " \n",
       "     fit_order  \n",
       " 0           7  \n",
       " 1           6  \n",
       " 2          12  \n",
       " 3           9  \n",
       " 4          10  \n",
       " 5          11  \n",
       " 6           8  \n",
       " 7           4  \n",
       " 8           3  \n",
       " 9           5  \n",
       " 10          2  \n",
       " 11          1  }"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor_new_hpo.fit_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_new_hpo = predictor_new_hpo.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "41"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Remember to set all negative values to zero\n",
    "# How many negative values do we have after removal?\n",
    "counter=0\n",
    "for pred in predictions_new_hpo:\n",
    "    if pred < 0:\n",
    "        counter=counter+1\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set them to zero\n",
    "for index in predictions_new_hpo.keys():\n",
    "    if predictions_new_hpo[index] < 0:\n",
    "        predictions_new_hpo[index]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-20 00:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-20 01:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-20 02:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-01-20 03:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-01-20 04:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              datetime  count\n",
       "0  2011-01-20 00:00:00      0\n",
       "1  2011-01-20 01:00:00      0\n",
       "2  2011-01-20 02:00:00      0\n",
       "3  2011-01-20 03:00:00      0\n",
       "4  2011-01-20 04:00:00      0"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize dataframe\n",
    "submission_new_hpo = pd.read_csv(\"sampleSubmission.csv\")\n",
    "submission_new_hpo.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same submitting predictions\n",
    "submission_new_hpo[\"count\"] = predictions_new_hpo.values\n",
    "submission_new_hpo.to_csv(\"submission_new_hpo.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 188k/188k [00:00<00:00, 374kB/s]\n",
      "Successfully submitted to Bike Sharing Demand"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c bike-sharing-demand -f submission_new_hpo.csv -m \"new features with hyperparameters\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fileName                     date                 description                             status    publicScore  privateScore  \n",
      "---------------------------  -------------------  --------------------------------------  --------  -----------  ------------  \n",
      "submission_new_hpo.csv       2022-02-02 20:08:13  new features with hyperparameters       complete  0.49953      0.49953       \n",
      "submission_new_features.csv  2022-02-02 19:54:58  new features                            complete  0.55149      0.55149       \n",
      "submission.csv               2022-02-02 19:29:26  first raw submission                    complete  1.39487      1.39487       \n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submissions -c bike-sharing-demand | tail -n +1 | head -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New Score of `0.49953`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Updating hyperparameters further"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trials = 5  # try at most 5 different hyperparameter configurations for each type of model\n",
    "search_strategy = 'auto'  # to tune hyperparameters using random search routine\n",
    "\n",
    "hyperparameter_tune_kwargs = {  # HPO is not performed unless hyperparameter_tune_kwargs is specified\n",
    "    'num_trials': num_trials,\n",
    "    'scheduler' : 'local',\n",
    "    'searcher': search_strategy\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20220202_214942/\"\n",
      "Presets specified: ['best_quality']\n",
      "Warning: hyperparameter tuning is currently experimental and may cause the process to hang.\n",
      "Beginning AutoGluon training ... Time limit = 1000s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20220202_214942/\"\n",
      "AutoGluon Version:  0.3.1\n",
      "Train Data Rows:    10886\n",
      "Train Data Columns: 12\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2401.82 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.89 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 2 | ['season', 'weather']\n",
      "\t\t('datetime', []) : 1 | ['datetime']\n",
      "\t\t('float', [])    : 3 | ['temp', 'atemp', 'windspeed']\n",
      "\t\t('int', [])      : 6 | ['holiday', 'workingday', 'humidity', 'hour', 'month', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])             : 2 | ['season', 'weather']\n",
      "\t\t('float', [])                : 3 | ['temp', 'atemp', 'windspeed']\n",
      "\t\t('int', [])                  : 4 | ['humidity', 'hour', 'month', 'day']\n",
      "\t\t('int', ['bool'])            : 2 | ['holiday', 'workingday']\n",
      "\t\t('int', ['datetime_as_int']) : 1 | ['datetime']\n",
      "\t0.4s = Fit runtime\n",
      "\t12 features in original data used to generate 12 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.74 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.42s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tTo change this, specify the eval_metric argument of fit()\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 11 L1 models ...\n",
      "Hyperparameter tuning model: KNeighborsUnif_BAG_L1 ...\n",
      "Fitted model: KNeighborsUnif_BAG_L1/T0 ...\n",
      "\t-123.9226\t = Validation score   (root_mean_squared_error)\n",
      "\t0.06s\t = Training   runtime\n",
      "\t0.42s\t = Validation runtime\n",
      "Hyperparameter tuning model: KNeighborsDist_BAG_L1 ...\n",
      "Fitted model: KNeighborsDist_BAG_L1/T0 ...\n",
      "\t-119.3734\t = Validation score   (root_mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Hyperparameter tuning model: LightGBMXT_BAG_L1 ...\n",
      "Warning: Exception caused LightGBMXT_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 103, in status_printer\n",
      "    pbar = IntProgress(min=0, max=total)\n",
      "NameError: name 'IntProgress' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 1153, in _train_single_full\n",
      "    hpo_models, hpo_model_performances, hpo_results = model.hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=hyperparameter_tune_kwargs, **model_fit_kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 179, in _hyperparameter_tune\n",
      "    return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=scheduler_options, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 801, in _hyperparameter_tune\n",
      "    hpo_models, hpo_model_performances, hpo_results = self.model_base.hyperparameter_tune(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 391, in _hyperparameter_tune\n",
      "    scheduler.run()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/scheduler/seq_scheduler.py\", line 152, in run\n",
      "    for i in (tqdm(r) if self.num_trials < 1000 else r):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 213, in __init__\n",
      "    self.fp, total, self.desc, self.ncols)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 111, in status_printer\n",
      "    \"IntProgress not found. Please update jupyter and ipywidgets.\"\n",
      "ImportError: IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "Hyperparameter tuning model: LightGBM_BAG_L1 ...\n",
      "Warning: Exception caused LightGBM_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 103, in status_printer\n",
      "    pbar = IntProgress(min=0, max=total)\n",
      "NameError: name 'IntProgress' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 1153, in _train_single_full\n",
      "    hpo_models, hpo_model_performances, hpo_results = model.hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=hyperparameter_tune_kwargs, **model_fit_kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 179, in _hyperparameter_tune\n",
      "    return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=scheduler_options, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 801, in _hyperparameter_tune\n",
      "    hpo_models, hpo_model_performances, hpo_results = self.model_base.hyperparameter_tune(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 391, in _hyperparameter_tune\n",
      "    scheduler.run()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/scheduler/seq_scheduler.py\", line 152, in run\n",
      "    for i in (tqdm(r) if self.num_trials < 1000 else r):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 213, in __init__\n",
      "    self.fp, total, self.desc, self.ncols)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 111, in status_printer\n",
      "    \"IntProgress not found. Please update jupyter and ipywidgets.\"\n",
      "ImportError: IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "Hyperparameter tuning model: RandomForestMSE_BAG_L1 ...\n",
      "Fitted model: RandomForestMSE_BAG_L1/T0 ...\n",
      "\t-41.3075\t = Validation score   (root_mean_squared_error)\n",
      "\t9.77s\t = Training   runtime\n",
      "\t0.43s\t = Validation runtime\n",
      "Hyperparameter tuning model: CatBoost_BAG_L1 ...\n",
      "Warning: Exception caused CatBoost_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 103, in status_printer\n",
      "    pbar = IntProgress(min=0, max=total)\n",
      "NameError: name 'IntProgress' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 1153, in _train_single_full\n",
      "    hpo_models, hpo_model_performances, hpo_results = model.hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=hyperparameter_tune_kwargs, **model_fit_kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 179, in _hyperparameter_tune\n",
      "    return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=scheduler_options, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 801, in _hyperparameter_tune\n",
      "    hpo_models, hpo_model_performances, hpo_results = self.model_base.hyperparameter_tune(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 887, in _hyperparameter_tune\n",
      "    scheduler.run()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/scheduler/seq_scheduler.py\", line 152, in run\n",
      "    for i in (tqdm(r) if self.num_trials < 1000 else r):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 213, in __init__\n",
      "    self.fp, total, self.desc, self.ncols)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 111, in status_printer\n",
      "    \"IntProgress not found. Please update jupyter and ipywidgets.\"\n",
      "ImportError: IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "Hyperparameter tuning model: ExtraTreesMSE_BAG_L1 ...\n",
      "Fitted model: ExtraTreesMSE_BAG_L1/T0 ...\n",
      "\t-40.8242\t = Validation score   (root_mean_squared_error)\n",
      "\t4.49s\t = Training   runtime\n",
      "\t0.41s\t = Validation runtime\n",
      "Hyperparameter tuning model: NeuralNetFastAI_BAG_L1 ...\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 23)\n",
      "Fitted model: NeuralNetFastAI_BAG_L1/T0 ...\n",
      "\t-89.8153\t = Validation score   (root_mean_squared_error)\n",
      "\t8.57s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Hyperparameter tuning model: XGBoost_BAG_L1 ...\n",
      "Warning: Exception caused XGBoost_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 103, in status_printer\n",
      "    pbar = IntProgress(min=0, max=total)\n",
      "NameError: name 'IntProgress' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 1153, in _train_single_full\n",
      "    hpo_models, hpo_model_performances, hpo_results = model.hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=hyperparameter_tune_kwargs, **model_fit_kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 179, in _hyperparameter_tune\n",
      "    return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=scheduler_options, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 801, in _hyperparameter_tune\n",
      "    hpo_models, hpo_model_performances, hpo_results = self.model_base.hyperparameter_tune(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 887, in _hyperparameter_tune\n",
      "    scheduler.run()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/scheduler/seq_scheduler.py\", line 152, in run\n",
      "    for i in (tqdm(r) if self.num_trials < 1000 else r):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 213, in __init__\n",
      "    self.fp, total, self.desc, self.ncols)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 111, in status_printer\n",
      "    \"IntProgress not found. Please update jupyter and ipywidgets.\"\n",
      "ImportError: IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "Hyperparameter tuning model: NeuralNetMXNet_BAG_L1 ...\n",
      "Warning: Exception caused NeuralNetMXNet_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 103, in status_printer\n",
      "    pbar = IntProgress(min=0, max=total)\n",
      "NameError: name 'IntProgress' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 1153, in _train_single_full\n",
      "    hpo_models, hpo_model_performances, hpo_results = model.hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=hyperparameter_tune_kwargs, **model_fit_kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 179, in _hyperparameter_tune\n",
      "    return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=scheduler_options, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 801, in _hyperparameter_tune\n",
      "    hpo_models, hpo_model_performances, hpo_results = self.model_base.hyperparameter_tune(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/tabular/models/tabular_nn/tabular_nn_model.py\", line 749, in _hyperparameter_tune\n",
      "    scheduler.run()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/scheduler/seq_scheduler.py\", line 152, in run\n",
      "    for i in (tqdm(r) if self.num_trials < 1000 else r):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 213, in __init__\n",
      "    self.fp, total, self.desc, self.ncols)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 111, in status_printer\n",
      "    \"IntProgress not found. Please update jupyter and ipywidgets.\"\n",
      "ImportError: IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 10.9s of the 958.93s of remaining time.\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 7.94246\tvalid_set's rmse: 36.1445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-36.0522\t = Validation score   (root_mean_squared_error)\n",
      "\t6.9s\t = Training   runtime\n",
      "\t0.27s\t = Validation runtime\n",
      "Fitting model: KNeighborsUnif_BAG_L1/T0 ... Training model for up to 617.36s of the 950.71s of remaining time.\n",
      "\t-123.9226\t = Validation score   (root_mean_squared_error)\n",
      "\t0.06s\t = Training   runtime\n",
      "\t0.42s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1/T0 ... Training model for up to 617.29s of the 950.64s of remaining time.\n",
      "\t-119.3734\t = Validation score   (root_mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1/T0 ... Training model for up to 617.22s of the 950.57s of remaining time.\n",
      "\t-41.3075\t = Validation score   (root_mean_squared_error)\n",
      "\t9.77s\t = Training   runtime\n",
      "\t0.43s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1/T0 ... Training model for up to 617.16s of the 950.5s of remaining time.\n",
      "\t-40.8242\t = Validation score   (root_mean_squared_error)\n",
      "\t4.49s\t = Training   runtime\n",
      "\t0.41s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1/T0 ... Training model for up to 617.09s of the 950.44s of remaining time.\n",
      "\t-58.2116\t = Validation score   (root_mean_squared_error)\n",
      "\t56.84s\t = Training   runtime\n",
      "\t0.24s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 568.24s of the 901.59s of remaining time.\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 8.19209\tvalid_set's rmse: 35.8467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 7.83644\tvalid_set's rmse: 38.136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 8.04423\tvalid_set's rmse: 36.4522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 7.95789\tvalid_set's rmse: 38.8895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-37.0626\t = Validation score   (root_mean_squared_error)\n",
      "\t30.07s\t = Training   runtime\n",
      "\t1.19s\t = Validation runtime\n",
      "Repeating k-fold bagging: 2/2\n",
      "Fitting model: NeuralNetFastAI_BAG_L1/T0 ... Training model for up to 540.87s of the 874.21s of remaining time.\n",
      "\t-37.7117\t = Validation score   (root_mean_squared_error)\n",
      "\t59.31s\t = Training   runtime\n",
      "\t0.26s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 671.8s of the 671.78s of remaining time.\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\t-37.1864\t = Validation score   (root_mean_squared_error)\n",
      "\t9.31s\t = Training   runtime\n",
      "\t0.13s\t = Validation runtime\n",
      "Repeating k-fold bagging: 2/2\n",
      "Fitting model: NeuralNetFastAI_BAG_L2/T0 ... Training model for up to 663.43s of the 663.42s of remaining time.\n",
      "\t-36.9404\t = Validation score   (root_mean_squared_error)\n",
      "\t119.34s\t = Training   runtime\n",
      "\t0.52s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 602.6s of the 602.58s of remaining time.\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\t-36.9541\t = Validation score   (root_mean_squared_error)\n",
      "\t18.29s\t = Training   runtime\n",
      "\t0.25s\t = Validation runtime\n",
      "Completed 2/2 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the 592.6s of remaining time.\n",
      "\t-36.2654\t = Validation score   (root_mean_squared_error)\n",
      "\t0.24s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 407.91s ...\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20220202_214942/\")\n"
     ]
    }
   ],
   "source": [
    "\n",
    "predictor_new_hpo_args = TabularPredictor(label=\"count\",eval_metric=\"root_mean_squared_error\",problem_type=\"regression\").fit(train_data=train.loc[:, ~train.columns.isin(['casual', 'registered'])], time_limit=1000, \n",
    "                                                                                                                        presets=\"best_quality\",\n",
    "                                                                                                                        hyperparameter_tune_kwargs=hyperparameter_tune_kwargs,\n",
    "                                                                                                                       num_bag_folds=5, num_bag_sets=2, num_stack_levels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                        model   score_val  pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0         WeightedEnsemble_L2  -36.263891       3.071317  179.708018                0.000681           0.305906            2       True          7\n",
      "1        LightGBMLarge_BAG_L1  -36.328919       2.137563   50.975962                2.137563          50.975962            1       True          6\n",
      "2         WeightedEnsemble_L3  -36.378999       5.116631  340.780307                0.000692           0.236300            3       True         12\n",
      "3     ExtraTreesMSE_BAG_L2/T0  -36.663420       4.339822  195.118741                0.455063           6.033159            2       True          9\n",
      "4        LightGBMLarge_BAG_L2  -37.094839       4.108869  206.117330                0.224110          17.031748            2       True         11\n",
      "5   NeuralNetFastAI_BAG_L2/T0  -37.143037       4.436766  317.479099                0.552007         128.393517            2       True         10\n",
      "6   RandomForestMSE_BAG_L2/T0  -37.466412       4.351661  211.973238                0.466902          22.887656            2       True          8\n",
      "7     ExtraTreesMSE_BAG_L1/T0  -40.824193       0.398009    3.832802                0.398009           3.832802            1       True          4\n",
      "8   RandomForestMSE_BAG_L1/T0  -41.307546       0.406440    9.638735                0.406440           9.638735            1       True          3\n",
      "9   NeuralNetFastAI_BAG_L1/T0  -56.120611       0.535063  124.593348                0.535063         124.593348            1       True          5\n",
      "10   KNeighborsDist_BAG_L1/T0 -119.373394       0.203817    0.019309                0.203817           0.019309            1       True          2\n",
      "11   KNeighborsUnif_BAG_L1/T0 -123.922640       0.203866    0.025426                0.203866           0.025426            1       True          1\n",
      "Number of models trained: 12\n",
      "Types of models trained:\n",
      "{'StackerEnsembleModel_NNFastAiTabular', 'StackerEnsembleModel_KNN', 'StackerEnsembleModel_RF', 'WeightedEnsembleModel', 'StackerEnsembleModel_LGB', 'StackerEnsembleModel_XT'}\n",
      "Bagging used: True  (with 5 folds)\n",
      "Multi-layer stack-ensembling used: True  (with 3 levels)\n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('category', [])             : 2 | ['season', 'weather']\n",
      "('float', [])                : 3 | ['temp', 'atemp', 'windspeed']\n",
      "('int', [])                  : 4 | ['humidity', 'hour', 'month', 'day']\n",
      "('int', ['bool'])            : 2 | ['holiday', 'workingday']\n",
      "('int', ['datetime_as_int']) : 1 | ['datetime']\n",
      "Plot summary of models saved to file: AutogluonModels/ag-20220202_201004/SummaryOfModels.html\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_types': {'KNeighborsUnif_BAG_L1/T0': 'StackerEnsembleModel_KNN',\n",
       "  'KNeighborsDist_BAG_L1/T0': 'StackerEnsembleModel_KNN',\n",
       "  'RandomForestMSE_BAG_L1/T0': 'StackerEnsembleModel_RF',\n",
       "  'ExtraTreesMSE_BAG_L1/T0': 'StackerEnsembleModel_XT',\n",
       "  'NeuralNetFastAI_BAG_L1/T0': 'StackerEnsembleModel_NNFastAiTabular',\n",
       "  'LightGBMLarge_BAG_L1': 'StackerEnsembleModel_LGB',\n",
       "  'WeightedEnsemble_L2': 'WeightedEnsembleModel',\n",
       "  'RandomForestMSE_BAG_L2/T0': 'StackerEnsembleModel_RF',\n",
       "  'ExtraTreesMSE_BAG_L2/T0': 'StackerEnsembleModel_XT',\n",
       "  'NeuralNetFastAI_BAG_L2/T0': 'StackerEnsembleModel_NNFastAiTabular',\n",
       "  'LightGBMLarge_BAG_L2': 'StackerEnsembleModel_LGB',\n",
       "  'WeightedEnsemble_L3': 'WeightedEnsembleModel'},\n",
       " 'model_performance': {'KNeighborsUnif_BAG_L1/T0': -123.92263994015985,\n",
       "  'KNeighborsDist_BAG_L1/T0': -119.37339448003198,\n",
       "  'RandomForestMSE_BAG_L1/T0': -41.30754583307658,\n",
       "  'ExtraTreesMSE_BAG_L1/T0': -40.82419267315799,\n",
       "  'NeuralNetFastAI_BAG_L1/T0': -56.12061068073881,\n",
       "  'LightGBMLarge_BAG_L1': -36.32891895055124,\n",
       "  'WeightedEnsemble_L2': -36.26389050054423,\n",
       "  'RandomForestMSE_BAG_L2/T0': -37.466412428978344,\n",
       "  'ExtraTreesMSE_BAG_L2/T0': -36.66342022295629,\n",
       "  'NeuralNetFastAI_BAG_L2/T0': -37.143037291368515,\n",
       "  'LightGBMLarge_BAG_L2': -37.0948393856299,\n",
       "  'WeightedEnsemble_L3': -36.37899907824756},\n",
       " 'model_best': 'WeightedEnsemble_L2',\n",
       " 'model_paths': {'KNeighborsUnif_BAG_L1/T0': 'AutogluonModels/ag-20220202_201004/models/KNeighborsUnif_BAG_L1/T0/',\n",
       "  'KNeighborsDist_BAG_L1/T0': 'AutogluonModels/ag-20220202_201004/models/KNeighborsDist_BAG_L1/T0/',\n",
       "  'RandomForestMSE_BAG_L1/T0': 'AutogluonModels/ag-20220202_201004/models/RandomForestMSE_BAG_L1/T0/',\n",
       "  'ExtraTreesMSE_BAG_L1/T0': 'AutogluonModels/ag-20220202_201004/models/ExtraTreesMSE_BAG_L1/T0/',\n",
       "  'NeuralNetFastAI_BAG_L1/T0': 'AutogluonModels/ag-20220202_201004/models/NeuralNetFastAI_BAG_L1/T0/',\n",
       "  'LightGBMLarge_BAG_L1': 'AutogluonModels/ag-20220202_201004/models/LightGBMLarge_BAG_L1/',\n",
       "  'WeightedEnsemble_L2': 'AutogluonModels/ag-20220202_201004/models/WeightedEnsemble_L2/',\n",
       "  'RandomForestMSE_BAG_L2/T0': 'AutogluonModels/ag-20220202_201004/models/RandomForestMSE_BAG_L2/T0/',\n",
       "  'ExtraTreesMSE_BAG_L2/T0': 'AutogluonModels/ag-20220202_201004/models/ExtraTreesMSE_BAG_L2/T0/',\n",
       "  'NeuralNetFastAI_BAG_L2/T0': 'AutogluonModels/ag-20220202_201004/models/NeuralNetFastAI_BAG_L2/T0/',\n",
       "  'LightGBMLarge_BAG_L2': 'AutogluonModels/ag-20220202_201004/models/LightGBMLarge_BAG_L2/',\n",
       "  'WeightedEnsemble_L3': 'AutogluonModels/ag-20220202_201004/models/WeightedEnsemble_L3/'},\n",
       " 'model_fit_times': {'KNeighborsUnif_BAG_L1/T0': 0.025426387786865234,\n",
       "  'KNeighborsDist_BAG_L1/T0': 0.019308805465698242,\n",
       "  'RandomForestMSE_BAG_L1/T0': 9.638734817504883,\n",
       "  'ExtraTreesMSE_BAG_L1/T0': 3.8328018188476562,\n",
       "  'NeuralNetFastAI_BAG_L1/T0': 124.59334802627563,\n",
       "  'LightGBMLarge_BAG_L1': 50.97596216201782,\n",
       "  'WeightedEnsemble_L2': 0.3059060573577881,\n",
       "  'RandomForestMSE_BAG_L2/T0': 22.88765597343445,\n",
       "  'ExtraTreesMSE_BAG_L2/T0': 6.033159017562866,\n",
       "  'NeuralNetFastAI_BAG_L2/T0': 128.39351725578308,\n",
       "  'LightGBMLarge_BAG_L2': 17.031748056411743,\n",
       "  'WeightedEnsemble_L3': 0.23630023002624512},\n",
       " 'model_pred_times': {'KNeighborsUnif_BAG_L1/T0': 0.20386624336242676,\n",
       "  'KNeighborsDist_BAG_L1/T0': 0.20381665229797363,\n",
       "  'RandomForestMSE_BAG_L1/T0': 0.40644049644470215,\n",
       "  'ExtraTreesMSE_BAG_L1/T0': 0.3980090618133545,\n",
       "  'NeuralNetFastAI_BAG_L1/T0': 0.5350632667541504,\n",
       "  'LightGBMLarge_BAG_L1': 2.1375632286071777,\n",
       "  'WeightedEnsemble_L2': 0.0006811618804931641,\n",
       "  'RandomForestMSE_BAG_L2/T0': 0.466902494430542,\n",
       "  'ExtraTreesMSE_BAG_L2/T0': 0.4550628662109375,\n",
       "  'NeuralNetFastAI_BAG_L2/T0': 0.552006721496582,\n",
       "  'LightGBMLarge_BAG_L2': 0.22411012649536133,\n",
       "  'WeightedEnsemble_L3': 0.0006923675537109375},\n",
       " 'num_bag_folds': 5,\n",
       " 'max_stack_level': 3,\n",
       " 'model_hyperparams': {'KNeighborsUnif_BAG_L1/T0': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True,\n",
       "   'use_child_oof': True},\n",
       "  'KNeighborsDist_BAG_L1/T0': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True,\n",
       "   'use_child_oof': True},\n",
       "  'RandomForestMSE_BAG_L1/T0': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True,\n",
       "   'use_child_oof': True},\n",
       "  'ExtraTreesMSE_BAG_L1/T0': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True,\n",
       "   'use_child_oof': True},\n",
       "  'NeuralNetFastAI_BAG_L1/T0': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'LightGBMLarge_BAG_L1': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'WeightedEnsemble_L2': {'use_orig_features': False,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'RandomForestMSE_BAG_L2/T0': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True,\n",
       "   'use_child_oof': True},\n",
       "  'ExtraTreesMSE_BAG_L2/T0': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True,\n",
       "   'use_child_oof': True},\n",
       "  'NeuralNetFastAI_BAG_L2/T0': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'LightGBMLarge_BAG_L2': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'WeightedEnsemble_L3': {'use_orig_features': False,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True}},\n",
       " 'leaderboard':                         model   score_val  pred_time_val    fit_time  \\\n",
       " 0         WeightedEnsemble_L2  -36.263891       3.071317  179.708018   \n",
       " 1        LightGBMLarge_BAG_L1  -36.328919       2.137563   50.975962   \n",
       " 2         WeightedEnsemble_L3  -36.378999       5.116631  340.780307   \n",
       " 3     ExtraTreesMSE_BAG_L2/T0  -36.663420       4.339822  195.118741   \n",
       " 4        LightGBMLarge_BAG_L2  -37.094839       4.108869  206.117330   \n",
       " 5   NeuralNetFastAI_BAG_L2/T0  -37.143037       4.436766  317.479099   \n",
       " 6   RandomForestMSE_BAG_L2/T0  -37.466412       4.351661  211.973238   \n",
       " 7     ExtraTreesMSE_BAG_L1/T0  -40.824193       0.398009    3.832802   \n",
       " 8   RandomForestMSE_BAG_L1/T0  -41.307546       0.406440    9.638735   \n",
       " 9   NeuralNetFastAI_BAG_L1/T0  -56.120611       0.535063  124.593348   \n",
       " 10   KNeighborsDist_BAG_L1/T0 -119.373394       0.203817    0.019309   \n",
       " 11   KNeighborsUnif_BAG_L1/T0 -123.922640       0.203866    0.025426   \n",
       " \n",
       "     pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
       " 0                 0.000681           0.305906            2       True   \n",
       " 1                 2.137563          50.975962            1       True   \n",
       " 2                 0.000692           0.236300            3       True   \n",
       " 3                 0.455063           6.033159            2       True   \n",
       " 4                 0.224110          17.031748            2       True   \n",
       " 5                 0.552007         128.393517            2       True   \n",
       " 6                 0.466902          22.887656            2       True   \n",
       " 7                 0.398009           3.832802            1       True   \n",
       " 8                 0.406440           9.638735            1       True   \n",
       " 9                 0.535063         124.593348            1       True   \n",
       " 10                0.203817           0.019309            1       True   \n",
       " 11                0.203866           0.025426            1       True   \n",
       " \n",
       "     fit_order  \n",
       " 0           7  \n",
       " 1           6  \n",
       " 2          12  \n",
       " 3           9  \n",
       " 4          11  \n",
       " 5          10  \n",
       " 6           8  \n",
       " 7           4  \n",
       " 8           3  \n",
       " 9           5  \n",
       " 10          2  \n",
       " 11          1  }"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor_new_hpo_args.fit_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor_new_hpo_args = predictor_new_hpo_args.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many negative values do we have after removal?\n",
    "counter=0\n",
    "for pred in predictor_new_hpo_args:\n",
    "    if pred < 0:\n",
    "        counter=counter+1\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set them to zero\n",
    "for index in predictor_new_hpo_args.keys():\n",
    "    if predictor_new_hpo_args[index] < 0:\n",
    "        predictor_new_hpo_args[index]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-20 00:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-20 01:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-20 02:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-01-20 03:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-01-20 04:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              datetime  count\n",
       "0  2011-01-20 00:00:00      0\n",
       "1  2011-01-20 01:00:00      0\n",
       "2  2011-01-20 02:00:00      0\n",
       "3  2011-01-20 03:00:00      0\n",
       "4  2011-01-20 04:00:00      0"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize dataframe\n",
    "submission_new_hpo_args = pd.read_csv(\"sampleSubmission.csv\")\n",
    "submission_new_hpo_args.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same submitting predictions\n",
    "submission_new_hpo_args[\"count\"] = predictor_new_hpo_args.values\n",
    "submission_new_hpo_args.to_csv(\"submission_new_hpo_args.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 188k/188k [00:00<00:00, 444kB/s]\n",
      "Successfully submitted to Bike Sharing Demand"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c bike-sharing-demand -f submission_new_hpo_args.csv -m \"new features with hyperparameters args\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fileName                     date                 description                             status    publicScore  privateScore  \n",
      "---------------------------  -------------------  --------------------------------------  --------  -----------  ------------  \n",
      "submission_new_hpo_args.csv  2022-02-02 20:21:11  new features with hyperparameters args  complete  0.49958      0.49958       \n",
      "submission_new_hpo.csv       2022-02-02 20:08:13  new features with hyperparameters       complete  0.49953      0.49953       \n",
      "submission_new_features.csv  2022-02-02 19:54:58  new features                            complete  0.55149      0.55149       \n",
      "submission.csv               2022-02-02 19:29:26  first raw submission                    complete  1.39487      1.39487       \n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submissions -c bike-sharing-demand | tail -n +1 | head -n 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New Score of `0.49958`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with grid search as the hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20220202_212254/\"\n",
      "Presets specified: ['best_quality']\n",
      "Warning: hyperparameter tuning is currently experimental and may cause the process to hang.\n",
      "Beginning AutoGluon training ... Time limit = 1000s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20220202_212254/\"\n",
      "AutoGluon Version:  0.3.1\n",
      "Train Data Rows:    10886\n",
      "Train Data Columns: 12\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    3007.3 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.89 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 2 | ['season', 'weather']\n",
      "\t\t('datetime', []) : 1 | ['datetime']\n",
      "\t\t('float', [])    : 3 | ['temp', 'atemp', 'windspeed']\n",
      "\t\t('int', [])      : 6 | ['holiday', 'workingday', 'humidity', 'hour', 'month', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])             : 2 | ['season', 'weather']\n",
      "\t\t('float', [])                : 3 | ['temp', 'atemp', 'windspeed']\n",
      "\t\t('int', [])                  : 4 | ['humidity', 'hour', 'month', 'day']\n",
      "\t\t('int', ['bool'])            : 2 | ['holiday', 'workingday']\n",
      "\t\t('int', ['datetime_as_int']) : 1 | ['datetime']\n",
      "\t0.5s = Fit runtime\n",
      "\t12 features in original data used to generate 12 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.74 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.55s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tTo change this, specify the eval_metric argument of fit()\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 11 L1 models ...\n",
      "Hyperparameter tuning model: KNeighborsUnif_BAG_L1 ...\n",
      "Fitted model: KNeighborsUnif_BAG_L1/T0 ...\n",
      "\t-123.9226\t = Validation score   (root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.21s\t = Validation runtime\n",
      "Hyperparameter tuning model: KNeighborsDist_BAG_L1 ...\n",
      "Fitted model: KNeighborsDist_BAG_L1/T0 ...\n",
      "\t-119.3734\t = Validation score   (root_mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Hyperparameter tuning model: LightGBMXT_BAG_L1 ...\n",
      "Warning: Exception caused LightGBMXT_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 1153, in _train_single_full\n",
      "    hpo_models, hpo_model_performances, hpo_results = model.hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=hyperparameter_tune_kwargs, **model_fit_kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 179, in _hyperparameter_tune\n",
      "    return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=scheduler_options, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 801, in _hyperparameter_tune\n",
      "    hpo_models, hpo_model_performances, hpo_results = self.model_base.hyperparameter_tune(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 382, in _hyperparameter_tune\n",
      "    scheduler = scheduler_cls(lgb_trial, **scheduler_params)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/scheduler/seq_scheduler.py\", line 97, in __init__\n",
      "    self.searcher: BaseSearcher = self.get_searcher_(searcher, train_fn, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/scheduler/seq_scheduler.py\", line 135, in get_searcher_\n",
      "    searcher = searcher_factory(searcher, **{**scheduler_opts, **_search_options})\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/searcher/searcher_factory.py\", line 104, in searcher_factory\n",
      "    searcher = searcher_cls(**kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/searcher/grid_searcher.py\", line 33, in __init__\n",
      "    'Only Categorical is supported, but {} is {}'.format(hp, hp_type)\n",
      "AssertionError: Only Categorical is supported, but feature_fraction is <class 'configspace.hyperparameters.uniformfloathyperparameter'>\n",
      "Only Categorical is supported, but feature_fraction is <class 'configspace.hyperparameters.uniformfloathyperparameter'>\n",
      "Hyperparameter tuning model: LightGBM_BAG_L1 ...\n",
      "Warning: Exception caused LightGBM_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 1153, in _train_single_full\n",
      "    hpo_models, hpo_model_performances, hpo_results = model.hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=hyperparameter_tune_kwargs, **model_fit_kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 179, in _hyperparameter_tune\n",
      "    return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=scheduler_options, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 801, in _hyperparameter_tune\n",
      "    hpo_models, hpo_model_performances, hpo_results = self.model_base.hyperparameter_tune(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 382, in _hyperparameter_tune\n",
      "    scheduler = scheduler_cls(lgb_trial, **scheduler_params)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/scheduler/seq_scheduler.py\", line 97, in __init__\n",
      "    self.searcher: BaseSearcher = self.get_searcher_(searcher, train_fn, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/scheduler/seq_scheduler.py\", line 135, in get_searcher_\n",
      "    searcher = searcher_factory(searcher, **{**scheduler_opts, **_search_options})\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/searcher/searcher_factory.py\", line 104, in searcher_factory\n",
      "    searcher = searcher_cls(**kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/searcher/grid_searcher.py\", line 33, in __init__\n",
      "    'Only Categorical is supported, but {} is {}'.format(hp, hp_type)\n",
      "AssertionError: Only Categorical is supported, but feature_fraction is <class 'configspace.hyperparameters.uniformfloathyperparameter'>\n",
      "Only Categorical is supported, but feature_fraction is <class 'configspace.hyperparameters.uniformfloathyperparameter'>\n",
      "Hyperparameter tuning model: RandomForestMSE_BAG_L1 ...\n",
      "Fitted model: RandomForestMSE_BAG_L1/T0 ...\n",
      "\t-41.3075\t = Validation score   (root_mean_squared_error)\n",
      "\t11.8s\t = Training   runtime\n",
      "\t0.42s\t = Validation runtime\n",
      "Hyperparameter tuning model: CatBoost_BAG_L1 ...\n",
      "Warning: Exception caused CatBoost_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 1153, in _train_single_full\n",
      "    hpo_models, hpo_model_performances, hpo_results = model.hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=hyperparameter_tune_kwargs, **model_fit_kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 179, in _hyperparameter_tune\n",
      "    return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=scheduler_options, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 801, in _hyperparameter_tune\n",
      "    hpo_models, hpo_model_performances, hpo_results = self.model_base.hyperparameter_tune(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 878, in _hyperparameter_tune\n",
      "    scheduler: FIFOScheduler = scheduler_cls(model_trial, **scheduler_params)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/scheduler/seq_scheduler.py\", line 97, in __init__\n",
      "    self.searcher: BaseSearcher = self.get_searcher_(searcher, train_fn, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/scheduler/seq_scheduler.py\", line 135, in get_searcher_\n",
      "    searcher = searcher_factory(searcher, **{**scheduler_opts, **_search_options})\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/searcher/searcher_factory.py\", line 104, in searcher_factory\n",
      "    searcher = searcher_cls(**kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/searcher/grid_searcher.py\", line 33, in __init__\n",
      "    'Only Categorical is supported, but {} is {}'.format(hp, hp_type)\n",
      "AssertionError: Only Categorical is supported, but depth is <class 'configspace.hyperparameters.uniformintegerhyperparameter'>\n",
      "Only Categorical is supported, but depth is <class 'configspace.hyperparameters.uniformintegerhyperparameter'>\n",
      "Hyperparameter tuning model: ExtraTreesMSE_BAG_L1 ...\n",
      "Fitted model: ExtraTreesMSE_BAG_L1/T0 ...\n",
      "\t-40.8242\t = Validation score   (root_mean_squared_error)\n",
      "\t4.1s\t = Training   runtime\n",
      "\t0.4s\t = Validation runtime\n",
      "Hyperparameter tuning model: NeuralNetFastAI_BAG_L1 ...\n",
      "No improvement since epoch 0: early stopping\n",
      "Fitted model: NeuralNetFastAI_BAG_L1/T0 ...\n",
      "\t-133.7807\t = Validation score   (root_mean_squared_error)\n",
      "\t10.34s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Hyperparameter tuning model: XGBoost_BAG_L1 ...\n",
      "Warning: Exception caused XGBoost_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 1153, in _train_single_full\n",
      "    hpo_models, hpo_model_performances, hpo_results = model.hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=hyperparameter_tune_kwargs, **model_fit_kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 179, in _hyperparameter_tune\n",
      "    return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=scheduler_options, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 801, in _hyperparameter_tune\n",
      "    hpo_models, hpo_model_performances, hpo_results = self.model_base.hyperparameter_tune(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 878, in _hyperparameter_tune\n",
      "    scheduler: FIFOScheduler = scheduler_cls(model_trial, **scheduler_params)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/scheduler/seq_scheduler.py\", line 97, in __init__\n",
      "    self.searcher: BaseSearcher = self.get_searcher_(searcher, train_fn, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/scheduler/seq_scheduler.py\", line 135, in get_searcher_\n",
      "    searcher = searcher_factory(searcher, **{**scheduler_opts, **_search_options})\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/searcher/searcher_factory.py\", line 104, in searcher_factory\n",
      "    searcher = searcher_cls(**kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/searcher/grid_searcher.py\", line 33, in __init__\n",
      "    'Only Categorical is supported, but {} is {}'.format(hp, hp_type)\n",
      "AssertionError: Only Categorical is supported, but colsample_bytree is <class 'configspace.hyperparameters.uniformfloathyperparameter'>\n",
      "Only Categorical is supported, but colsample_bytree is <class 'configspace.hyperparameters.uniformfloathyperparameter'>\n",
      "Hyperparameter tuning model: NeuralNetMXNet_BAG_L1 ...\n",
      "Warning: Exception caused NeuralNetMXNet_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 1153, in _train_single_full\n",
      "    hpo_models, hpo_model_performances, hpo_results = model.hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=hyperparameter_tune_kwargs, **model_fit_kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 179, in _hyperparameter_tune\n",
      "    return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=scheduler_options, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 801, in _hyperparameter_tune\n",
      "    hpo_models, hpo_model_performances, hpo_results = self.model_base.hyperparameter_tune(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/tabular/models/tabular_nn/tabular_nn_model.py\", line 736, in _hyperparameter_tune\n",
      "    scheduler = scheduler_cls(tabular_nn_trial, **scheduler_params)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/scheduler/seq_scheduler.py\", line 97, in __init__\n",
      "    self.searcher: BaseSearcher = self.get_searcher_(searcher, train_fn, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/scheduler/seq_scheduler.py\", line 135, in get_searcher_\n",
      "    searcher = searcher_factory(searcher, **{**scheduler_opts, **_search_options})\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/searcher/searcher_factory.py\", line 104, in searcher_factory\n",
      "    searcher = searcher_cls(**kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/searcher/grid_searcher.py\", line 33, in __init__\n",
      "    'Only Categorical is supported, but {} is {}'.format(hp, hp_type)\n",
      "AssertionError: Only Categorical is supported, but dropout_prob is <class 'configspace.hyperparameters.uniformfloathyperparameter'>\n",
      "Only Categorical is supported, but dropout_prob is <class 'configspace.hyperparameters.uniformfloathyperparameter'>\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 10.9s of the 951.76s of remaining time.\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 7.94246\tvalid_set's rmse: 36.1445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-36.0522\t = Validation score   (root_mean_squared_error)\n",
      "\t6.89s\t = Training   runtime\n",
      "\t0.3s\t = Validation runtime\n",
      "Fitting model: KNeighborsUnif_BAG_L1/T0 ... Training model for up to 610.17s of the 943.48s of remaining time.\n",
      "\t-123.9226\t = Validation score   (root_mean_squared_error)\n",
      "\t0.03s\t = Training   runtime\n",
      "\t0.21s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1/T0 ... Training model for up to 610.09s of the 943.4s of remaining time.\n",
      "\t-119.3734\t = Validation score   (root_mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1/T0 ... Training model for up to 610.0s of the 943.31s of remaining time.\n",
      "\t-41.3075\t = Validation score   (root_mean_squared_error)\n",
      "\t11.8s\t = Training   runtime\n",
      "\t0.42s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1/T0 ... Training model for up to 609.92s of the 943.23s of remaining time.\n",
      "\t-40.8242\t = Validation score   (root_mean_squared_error)\n",
      "\t4.1s\t = Training   runtime\n",
      "\t0.4s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1/T0 ... Training model for up to 609.85s of the 943.16s of remaining time.\n",
      "\t-73.064\t = Validation score   (root_mean_squared_error)\n",
      "\t59.14s\t = Training   runtime\n",
      "\t0.25s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 560.44s of the 893.75s of remaining time.\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 8.19209\tvalid_set's rmse: 35.8467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 7.83644\tvalid_set's rmse: 38.136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 8.04423\tvalid_set's rmse: 36.4522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 7.95789\tvalid_set's rmse: 38.8895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-37.0626\t = Validation score   (root_mean_squared_error)\n",
      "\t30.06s\t = Training   runtime\n",
      "\t1.32s\t = Validation runtime\n",
      "Repeating k-fold bagging: 2/2\n",
      "Fitting model: NeuralNetFastAI_BAG_L1/T0 ... Training model for up to 531.57s of the 864.89s of remaining time.\n",
      "Fitted model: RandomForestMSE_BAG_L2/T0 ...\n",
      "\t-37.5421\t = Validation score   (root_mean_squared_error)\n",
      "\t24.12s\t = Training   runtime\n",
      "\t0.48s\t = Validation runtime\n",
      "Hyperparameter tuning model: CatBoost_BAG_L2 ...\n",
      "Warning: Exception caused CatBoost_BAG_L2 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 1153, in _train_single_full\n",
      "    hpo_models, hpo_model_performances, hpo_results = model.hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=hyperparameter_tune_kwargs, **model_fit_kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 179, in _hyperparameter_tune\n",
      "    return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=scheduler_options, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 801, in _hyperparameter_tune\n",
      "    hpo_models, hpo_model_performances, hpo_results = self.model_base.hyperparameter_tune(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 878, in _hyperparameter_tune\n",
      "    scheduler: FIFOScheduler = scheduler_cls(model_trial, **scheduler_params)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/scheduler/seq_scheduler.py\", line 97, in __init__\n",
      "    self.searcher: BaseSearcher = self.get_searcher_(searcher, train_fn, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/scheduler/seq_scheduler.py\", line 135, in get_searcher_\n",
      "    searcher = searcher_factory(searcher, **{**scheduler_opts, **_search_options})\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/searcher/searcher_factory.py\", line 104, in searcher_factory\n",
      "    searcher = searcher_cls(**kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/searcher/grid_searcher.py\", line 33, in __init__\n",
      "    'Only Categorical is supported, but {} is {}'.format(hp, hp_type)\n",
      "AssertionError: Only Categorical is supported, but depth is <class 'configspace.hyperparameters.uniformintegerhyperparameter'>\n",
      "Only Categorical is supported, but depth is <class 'configspace.hyperparameters.uniformintegerhyperparameter'>\n",
      "Hyperparameter tuning model: ExtraTreesMSE_BAG_L2 ...\n",
      "Fitted model: ExtraTreesMSE_BAG_L2/T0 ...\n",
      "\t-36.7399\t = Validation score   (root_mean_squared_error)\n",
      "\t6.36s\t = Training   runtime\n",
      "\t0.46s\t = Validation runtime\n",
      "Hyperparameter tuning model: NeuralNetFastAI_BAG_L2 ...\n",
      "No improvement since epoch 4: early stopping\n",
      "Fitted model: NeuralNetFastAI_BAG_L2/T0 ...\n",
      "\t-41.3628\t = Validation score   (root_mean_squared_error)\n",
      "\t8.73s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Hyperparameter tuning model: XGBoost_BAG_L2 ...\n",
      "Warning: Exception caused XGBoost_BAG_L2 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 1153, in _train_single_full\n",
      "    hpo_models, hpo_model_performances, hpo_results = model.hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=hyperparameter_tune_kwargs, **model_fit_kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 179, in _hyperparameter_tune\n",
      "    return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=scheduler_options, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 801, in _hyperparameter_tune\n",
      "    hpo_models, hpo_model_performances, hpo_results = self.model_base.hyperparameter_tune(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 878, in _hyperparameter_tune\n",
      "    scheduler: FIFOScheduler = scheduler_cls(model_trial, **scheduler_params)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/scheduler/seq_scheduler.py\", line 97, in __init__\n",
      "    self.searcher: BaseSearcher = self.get_searcher_(searcher, train_fn, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/scheduler/seq_scheduler.py\", line 135, in get_searcher_\n",
      "    searcher = searcher_factory(searcher, **{**scheduler_opts, **_search_options})\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/searcher/searcher_factory.py\", line 104, in searcher_factory\n",
      "    searcher = searcher_cls(**kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/searcher/grid_searcher.py\", line 33, in __init__\n",
      "    'Only Categorical is supported, but {} is {}'.format(hp, hp_type)\n",
      "AssertionError: Only Categorical is supported, but colsample_bytree is <class 'configspace.hyperparameters.uniformfloathyperparameter'>\n",
      "Only Categorical is supported, but colsample_bytree is <class 'configspace.hyperparameters.uniformfloathyperparameter'>\n",
      "Hyperparameter tuning model: NeuralNetMXNet_BAG_L2 ...\n",
      "Warning: Exception caused NeuralNetMXNet_BAG_L2 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 1153, in _train_single_full\n",
      "    hpo_models, hpo_model_performances, hpo_results = model.hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=hyperparameter_tune_kwargs, **model_fit_kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 179, in _hyperparameter_tune\n",
      "    return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=scheduler_options, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 801, in _hyperparameter_tune\n",
      "    hpo_models, hpo_model_performances, hpo_results = self.model_base.hyperparameter_tune(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/tabular/models/tabular_nn/tabular_nn_model.py\", line 736, in _hyperparameter_tune\n",
      "    scheduler = scheduler_cls(tabular_nn_trial, **scheduler_params)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/scheduler/seq_scheduler.py\", line 97, in __init__\n",
      "    self.searcher: BaseSearcher = self.get_searcher_(searcher, train_fn, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/scheduler/seq_scheduler.py\", line 135, in get_searcher_\n",
      "    searcher = searcher_factory(searcher, **{**scheduler_opts, **_search_options})\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/searcher/searcher_factory.py\", line 104, in searcher_factory\n",
      "    searcher = searcher_cls(**kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/searcher/grid_searcher.py\", line 33, in __init__\n",
      "    'Only Categorical is supported, but {} is {}'.format(hp, hp_type)\n",
      "AssertionError: Only Categorical is supported, but dropout_prob is <class 'configspace.hyperparameters.uniformfloathyperparameter'>\n",
      "Only Categorical is supported, but dropout_prob is <class 'configspace.hyperparameters.uniformfloathyperparameter'>\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 15.5s of the 721.12s of remaining time.\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\t-39.7905\t = Validation score   (root_mean_squared_error)\n",
      "\t2.06s\t = Training   runtime\n",
      "\t0.02s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2/T0 ... Training model for up to 718.78s of the 718.75s of remaining time.\n",
      "\t-37.5421\t = Validation score   (root_mean_squared_error)\n",
      "\t24.12s\t = Training   runtime\n",
      "\t0.48s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L2/T0 ... Training model for up to 718.69s of the 718.67s of remaining time.\n",
      "\t-36.7399\t = Validation score   (root_mean_squared_error)\n",
      "\t6.36s\t = Training   runtime\n",
      "\t0.46s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2/T0 ... Training model for up to 718.61s of the 718.59s of remaining time.\n",
      "\t-37.444\t = Validation score   (root_mean_squared_error)\n",
      "\t56.98s\t = Training   runtime\n",
      "\t0.26s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 669.74s of the 669.72s of remaining time.\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\t-37.4698\t = Validation score   (root_mean_squared_error)\n",
      "\t9.1s\t = Training   runtime\n",
      "\t0.12s\t = Validation runtime\n",
      "Repeating k-fold bagging: 2/2\n",
      "Fitting model: NeuralNetFastAI_BAG_L2/T0 ... Training model for up to 661.88s of the 661.86s of remaining time.\n",
      "\t-36.7677\t = Validation score   (root_mean_squared_error)\n",
      "\t116.93s\t = Training   runtime\n",
      "\t0.5s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 601.15s of the 601.13s of remaining time.\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\t-37.16\t = Validation score   (root_mean_squared_error)\n",
      "\t18.01s\t = Training   runtime\n",
      "\t0.24s\t = Validation runtime\n",
      "Completed 2/2 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the 591.21s of remaining time.\n",
      "\t-36.3495\t = Validation score   (root_mean_squared_error)\n",
      "\t0.26s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 409.33s ...\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20220202_212254/\")\n"
     ]
    }
   ],
   "source": [
    "num_trials_new = 5  # try at most 5 different hyperparameter configurations for each type of model\n",
    "\n",
    "search_strategy_new = 'grid'  # to tune hyperparameters using random search routine\n",
    "\n",
    "hyperparameter_tune_kwargs_2 = {  # HPO is not performed unless hyperparameter_tune_kwargs is specified\n",
    "    'num_trials': num_trials_new,\n",
    "    'scheduler' : 'local',\n",
    "    'searcher': search_strategy_new\n",
    "}\n",
    "predictor_new_hpo_arg = TabularPredictor(label=\"count\",eval_metric=\"root_mean_squared_error\",problem_type=\"regression\").fit(train_data=train.loc[:, ~train.columns.isin(['casual', 'registered'])], time_limit=1000, \n",
    "                                                                                                                        presets=\"best_quality\",\n",
    "                                                                                                                        hyperparameter_tune_kwargs=hyperparameter_tune_kwargs_2,\n",
    "                                                                                                                       num_bag_folds=5, num_bag_sets=2, num_stack_levels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Summary of fit() ***\n",
      "Estimated performance of each model:\n",
      "                        model   score_val  pred_time_val    fit_time  pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  fit_order\n",
      "0         WeightedEnsemble_L2  -36.264821       2.671351   57.990563                0.000737           0.320481            2       True          7\n",
      "1        LightGBMLarge_BAG_L1  -36.328919       2.267190   53.566014                2.267190          53.566014            1       True          6\n",
      "2         WeightedEnsemble_L3  -36.349476       5.189835  330.821703                0.000677           0.262129            3       True         12\n",
      "3     ExtraTreesMSE_BAG_L2/T0  -36.739943       4.446917  195.627054                0.463181           6.362641            2       True          9\n",
      "4   NeuralNetFastAI_BAG_L2/T0  -36.767718       4.487311  306.189519                0.503575         116.925106            2       True         10\n",
      "5        LightGBMLarge_BAG_L2  -37.159992       4.222401  207.271828                0.238665          18.007415            2       True         11\n",
      "6   RandomForestMSE_BAG_L2/T0  -37.542091       4.460377  213.385688                0.476641          24.121275            2       True          8\n",
      "7     ExtraTreesMSE_BAG_L1/T0  -40.824193       0.403424    4.104068                0.403424           4.104068            1       True          4\n",
      "8   RandomForestMSE_BAG_L1/T0  -41.307546       0.421493   11.804996                0.421493          11.804996            1       True          3\n",
      "9   NeuralNetFastAI_BAG_L1/T0  -55.671420       0.481257  119.737127                0.481257         119.737127            1       True          5\n",
      "10   KNeighborsDist_BAG_L1/T0 -119.373394       0.203778    0.021951                0.203778           0.021951            1       True          2\n",
      "11   KNeighborsUnif_BAG_L1/T0 -123.922640       0.206594    0.030258                0.206594           0.030258            1       True          1\n",
      "Number of models trained: 12\n",
      "Types of models trained:\n",
      "{'StackerEnsembleModel_RF', 'StackerEnsembleModel_KNN', 'StackerEnsembleModel_LGB', 'StackerEnsembleModel_NNFastAiTabular', 'StackerEnsembleModel_XT', 'WeightedEnsembleModel'}\n",
      "Bagging used: True  (with 5 folds)\n",
      "Multi-layer stack-ensembling used: True  (with 3 levels)\n",
      "Feature Metadata (Processed):\n",
      "(raw dtype, special dtypes):\n",
      "('category', [])             : 2 | ['season', 'weather']\n",
      "('float', [])                : 3 | ['temp', 'atemp', 'windspeed']\n",
      "('int', [])                  : 4 | ['humidity', 'hour', 'month', 'day']\n",
      "('int', ['bool'])            : 2 | ['holiday', 'workingday']\n",
      "('int', ['datetime_as_int']) : 1 | ['datetime']\n",
      "Plot summary of models saved to file: AutogluonModels/ag-20220202_212254/SummaryOfModels.html\n",
      "*** End of fit() summary ***\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'model_types': {'KNeighborsUnif_BAG_L1/T0': 'StackerEnsembleModel_KNN',\n",
       "  'KNeighborsDist_BAG_L1/T0': 'StackerEnsembleModel_KNN',\n",
       "  'RandomForestMSE_BAG_L1/T0': 'StackerEnsembleModel_RF',\n",
       "  'ExtraTreesMSE_BAG_L1/T0': 'StackerEnsembleModel_XT',\n",
       "  'NeuralNetFastAI_BAG_L1/T0': 'StackerEnsembleModel_NNFastAiTabular',\n",
       "  'LightGBMLarge_BAG_L1': 'StackerEnsembleModel_LGB',\n",
       "  'WeightedEnsemble_L2': 'WeightedEnsembleModel',\n",
       "  'RandomForestMSE_BAG_L2/T0': 'StackerEnsembleModel_RF',\n",
       "  'ExtraTreesMSE_BAG_L2/T0': 'StackerEnsembleModel_XT',\n",
       "  'NeuralNetFastAI_BAG_L2/T0': 'StackerEnsembleModel_NNFastAiTabular',\n",
       "  'LightGBMLarge_BAG_L2': 'StackerEnsembleModel_LGB',\n",
       "  'WeightedEnsemble_L3': 'WeightedEnsembleModel'},\n",
       " 'model_performance': {'KNeighborsUnif_BAG_L1/T0': -123.92263994015985,\n",
       "  'KNeighborsDist_BAG_L1/T0': -119.37339448003198,\n",
       "  'RandomForestMSE_BAG_L1/T0': -41.30754583307658,\n",
       "  'ExtraTreesMSE_BAG_L1/T0': -40.82419267315799,\n",
       "  'NeuralNetFastAI_BAG_L1/T0': -55.67142008367457,\n",
       "  'LightGBMLarge_BAG_L1': -36.32891895055124,\n",
       "  'WeightedEnsemble_L2': -36.26482147815258,\n",
       "  'RandomForestMSE_BAG_L2/T0': -37.54209125973882,\n",
       "  'ExtraTreesMSE_BAG_L2/T0': -36.7399433751296,\n",
       "  'NeuralNetFastAI_BAG_L2/T0': -36.76771786047803,\n",
       "  'LightGBMLarge_BAG_L2': -37.15999153330295,\n",
       "  'WeightedEnsemble_L3': -36.349476275837425},\n",
       " 'model_best': 'WeightedEnsemble_L2',\n",
       " 'model_paths': {'KNeighborsUnif_BAG_L1/T0': 'AutogluonModels/ag-20220202_212254/models/KNeighborsUnif_BAG_L1/T0/',\n",
       "  'KNeighborsDist_BAG_L1/T0': 'AutogluonModels/ag-20220202_212254/models/KNeighborsDist_BAG_L1/T0/',\n",
       "  'RandomForestMSE_BAG_L1/T0': 'AutogluonModels/ag-20220202_212254/models/RandomForestMSE_BAG_L1/T0/',\n",
       "  'ExtraTreesMSE_BAG_L1/T0': 'AutogluonModels/ag-20220202_212254/models/ExtraTreesMSE_BAG_L1/T0/',\n",
       "  'NeuralNetFastAI_BAG_L1/T0': 'AutogluonModels/ag-20220202_212254/models/NeuralNetFastAI_BAG_L1/T0/',\n",
       "  'LightGBMLarge_BAG_L1': 'AutogluonModels/ag-20220202_212254/models/LightGBMLarge_BAG_L1/',\n",
       "  'WeightedEnsemble_L2': 'AutogluonModels/ag-20220202_212254/models/WeightedEnsemble_L2/',\n",
       "  'RandomForestMSE_BAG_L2/T0': 'AutogluonModels/ag-20220202_212254/models/RandomForestMSE_BAG_L2/T0/',\n",
       "  'ExtraTreesMSE_BAG_L2/T0': 'AutogluonModels/ag-20220202_212254/models/ExtraTreesMSE_BAG_L2/T0/',\n",
       "  'NeuralNetFastAI_BAG_L2/T0': 'AutogluonModels/ag-20220202_212254/models/NeuralNetFastAI_BAG_L2/T0/',\n",
       "  'LightGBMLarge_BAG_L2': 'AutogluonModels/ag-20220202_212254/models/LightGBMLarge_BAG_L2/',\n",
       "  'WeightedEnsemble_L3': 'AutogluonModels/ag-20220202_212254/models/WeightedEnsemble_L3/'},\n",
       " 'model_fit_times': {'KNeighborsUnif_BAG_L1/T0': 0.030257701873779297,\n",
       "  'KNeighborsDist_BAG_L1/T0': 0.021950960159301758,\n",
       "  'RandomForestMSE_BAG_L1/T0': 11.804996013641357,\n",
       "  'ExtraTreesMSE_BAG_L1/T0': 4.10406756401062,\n",
       "  'NeuralNetFastAI_BAG_L1/T0': 119.73712682723999,\n",
       "  'LightGBMLarge_BAG_L1': 53.56601428985596,\n",
       "  'WeightedEnsemble_L2': 0.3204808235168457,\n",
       "  'RandomForestMSE_BAG_L2/T0': 24.121274948120117,\n",
       "  'ExtraTreesMSE_BAG_L2/T0': 6.362640619277954,\n",
       "  'NeuralNetFastAI_BAG_L2/T0': 116.92510557174683,\n",
       "  'LightGBMLarge_BAG_L2': 18.00741457939148,\n",
       "  'WeightedEnsemble_L3': 0.2621288299560547},\n",
       " 'model_pred_times': {'KNeighborsUnif_BAG_L1/T0': 0.20659351348876953,\n",
       "  'KNeighborsDist_BAG_L1/T0': 0.20377826690673828,\n",
       "  'RandomForestMSE_BAG_L1/T0': 0.4214930534362793,\n",
       "  'ExtraTreesMSE_BAG_L1/T0': 0.4034240245819092,\n",
       "  'NeuralNetFastAI_BAG_L1/T0': 0.48125672340393066,\n",
       "  'LightGBMLarge_BAG_L1': 2.2671902179718018,\n",
       "  'WeightedEnsemble_L2': 0.0007369518280029297,\n",
       "  'RandomForestMSE_BAG_L2/T0': 0.4766414165496826,\n",
       "  'ExtraTreesMSE_BAG_L2/T0': 0.4631812572479248,\n",
       "  'NeuralNetFastAI_BAG_L2/T0': 0.503575325012207,\n",
       "  'LightGBMLarge_BAG_L2': 0.23866486549377441,\n",
       "  'WeightedEnsemble_L3': 0.0006773471832275391},\n",
       " 'num_bag_folds': 5,\n",
       " 'max_stack_level': 3,\n",
       " 'model_hyperparams': {'KNeighborsUnif_BAG_L1/T0': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True,\n",
       "   'use_child_oof': True},\n",
       "  'KNeighborsDist_BAG_L1/T0': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True,\n",
       "   'use_child_oof': True},\n",
       "  'RandomForestMSE_BAG_L1/T0': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True,\n",
       "   'use_child_oof': True},\n",
       "  'ExtraTreesMSE_BAG_L1/T0': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True,\n",
       "   'use_child_oof': True},\n",
       "  'NeuralNetFastAI_BAG_L1/T0': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'LightGBMLarge_BAG_L1': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'WeightedEnsemble_L2': {'use_orig_features': False,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'RandomForestMSE_BAG_L2/T0': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True,\n",
       "   'use_child_oof': True},\n",
       "  'ExtraTreesMSE_BAG_L2/T0': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True,\n",
       "   'use_child_oof': True},\n",
       "  'NeuralNetFastAI_BAG_L2/T0': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'LightGBMLarge_BAG_L2': {'use_orig_features': True,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True},\n",
       "  'WeightedEnsemble_L3': {'use_orig_features': False,\n",
       "   'max_base_models': 25,\n",
       "   'max_base_models_per_type': 5,\n",
       "   'save_bag_folds': True}},\n",
       " 'leaderboard':                         model   score_val  pred_time_val    fit_time  \\\n",
       " 0         WeightedEnsemble_L2  -36.264821       2.671351   57.990563   \n",
       " 1        LightGBMLarge_BAG_L1  -36.328919       2.267190   53.566014   \n",
       " 2         WeightedEnsemble_L3  -36.349476       5.189835  330.821703   \n",
       " 3     ExtraTreesMSE_BAG_L2/T0  -36.739943       4.446917  195.627054   \n",
       " 4   NeuralNetFastAI_BAG_L2/T0  -36.767718       4.487311  306.189519   \n",
       " 5        LightGBMLarge_BAG_L2  -37.159992       4.222401  207.271828   \n",
       " 6   RandomForestMSE_BAG_L2/T0  -37.542091       4.460377  213.385688   \n",
       " 7     ExtraTreesMSE_BAG_L1/T0  -40.824193       0.403424    4.104068   \n",
       " 8   RandomForestMSE_BAG_L1/T0  -41.307546       0.421493   11.804996   \n",
       " 9   NeuralNetFastAI_BAG_L1/T0  -55.671420       0.481257  119.737127   \n",
       " 10   KNeighborsDist_BAG_L1/T0 -119.373394       0.203778    0.021951   \n",
       " 11   KNeighborsUnif_BAG_L1/T0 -123.922640       0.206594    0.030258   \n",
       " \n",
       "     pred_time_val_marginal  fit_time_marginal  stack_level  can_infer  \\\n",
       " 0                 0.000737           0.320481            2       True   \n",
       " 1                 2.267190          53.566014            1       True   \n",
       " 2                 0.000677           0.262129            3       True   \n",
       " 3                 0.463181           6.362641            2       True   \n",
       " 4                 0.503575         116.925106            2       True   \n",
       " 5                 0.238665          18.007415            2       True   \n",
       " 6                 0.476641          24.121275            2       True   \n",
       " 7                 0.403424           4.104068            1       True   \n",
       " 8                 0.421493          11.804996            1       True   \n",
       " 9                 0.481257         119.737127            1       True   \n",
       " 10                0.203778           0.021951            1       True   \n",
       " 11                0.206594           0.030258            1       True   \n",
       " \n",
       "     fit_order  \n",
       " 0           7  \n",
       " 1           6  \n",
       " 2          12  \n",
       " 3           9  \n",
       " 4          10  \n",
       " 5          11  \n",
       " 6           8  \n",
       " 7           4  \n",
       " 8           3  \n",
       " 9           5  \n",
       " 10          2  \n",
       " 11          1  }"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictor_new_hpo_arg.fit_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_new_hpo_arg = predictor_new_hpo_arg.predict(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many negative values do we have after removal?\n",
    "counter=0\n",
    "for pred in predictions_new_hpo_arg:\n",
    "    if pred < 0:\n",
    "        counter=counter+1\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set them to zero\n",
    "for index in predictions_new_hpo_arg.keys():\n",
    "    if predictions_new_hpo_arg[index] < 0:\n",
    "        predictions_new_hpo_arg[index]=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many negative values do we have after removal?\n",
    "counter=0\n",
    "for pred in predictions_new_hpo_arg:\n",
    "    if pred < 0:\n",
    "        counter=counter+1\n",
    "counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>datetime</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2011-01-20 00:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2011-01-20 01:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2011-01-20 02:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2011-01-20 03:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2011-01-20 04:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              datetime  count\n",
       "0  2011-01-20 00:00:00      0\n",
       "1  2011-01-20 01:00:00      0\n",
       "2  2011-01-20 02:00:00      0\n",
       "3  2011-01-20 03:00:00      0\n",
       "4  2011-01-20 04:00:00      0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Initialize dataframe\n",
    "submission_new_hpo_arg_grid = pd.read_csv(\"sampleSubmission.csv\")\n",
    "submission_new_hpo_arg_grid.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same submitting predictions\n",
    "submission_new_hpo_arg_grid[\"count\"] = predictions_new_hpo_arg.values\n",
    "submission_new_hpo_arg_grid.to_csv(\"submission_new_hpo_arg_grid.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 188k/188k [00:00<00:00, 410kB/s]\n",
      "Successfully submitted to Bike Sharing Demand"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submit -c bike-sharing-demand -f submission_new_hpo_arg_grid.csv -m \"new features with hyperparameters grid search\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fileName                         date                 description                                    status    publicScore  privateScore  \n",
      "-------------------------------  -------------------  ---------------------------------------------  --------  -----------  ------------  \n",
      "submission_new_hpo_arg_grid.csv  2022-02-02 21:32:07  new features with hyperparameters grid search  complete  0.49982      0.49982       \n",
      "submission_new_hpo_arg_grid.csv  2022-02-02 20:41:08  new features with hyperparameters grid search  complete  0.49981      0.49981       \n",
      "submission_new_hpo_arg_grid.csv  2022-02-02 20:40:08  new features with hyperparameters grid search  error     None         None          \n",
      "submission_new_hpo_arg_grid.csv  2022-02-02 20:39:03  new features with hyperparameters grid search  error     None         None          \n",
      "submission_new_hpo_args.csv      2022-02-02 20:21:11  new features with hyperparameters args         complete  0.49958      0.49958       \n",
      "submission_new_hpo.csv           2022-02-02 20:08:13  new features with hyperparameters              complete  0.49953      0.49953       \n",
      "submission_new_features.csv      2022-02-02 19:54:58  new features                                   complete  0.55149      0.55149       \n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submissions -c bike-sharing-demand | tail -n +1 | head -n 9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New Score of `0.49982`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing with increased number of trials as the hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20220202_213429/\"\n",
      "Presets specified: ['best_quality']\n",
      "Warning: hyperparameter tuning is currently experimental and may cause the process to hang.\n",
      "Beginning AutoGluon training ... Time limit = 1000s\n",
      "AutoGluon will save models to \"AutogluonModels/ag-20220202_213429/\"\n",
      "AutoGluon Version:  0.3.1\n",
      "Train Data Rows:    10886\n",
      "Train Data Columns: 12\n",
      "Preprocessing data ...\n",
      "Using Feature Generators to preprocess the data ...\n",
      "Fitting AutoMLPipelineFeatureGenerator...\n",
      "\tAvailable Memory:                    2493.66 MB\n",
      "\tTrain Data (Original)  Memory Usage: 0.89 MB (0.0% of available memory)\n",
      "\tInferring data type of each feature based on column values. Set feature_metadata_in to manually specify special dtypes of the features.\n",
      "\tStage 1 Generators:\n",
      "\t\tFitting AsTypeFeatureGenerator...\n",
      "\t\t\tNote: Converting 2 features to boolean dtype as they only contain 2 unique values.\n",
      "\tStage 2 Generators:\n",
      "\t\tFitting FillNaFeatureGenerator...\n",
      "\tStage 3 Generators:\n",
      "\t\tFitting IdentityFeatureGenerator...\n",
      "\t\tFitting CategoryFeatureGenerator...\n",
      "\t\t\tFitting CategoryMemoryMinimizeFeatureGenerator...\n",
      "\t\tFitting DatetimeFeatureGenerator...\n",
      "\tStage 4 Generators:\n",
      "\t\tFitting DropUniqueFeatureGenerator...\n",
      "\tTypes of features in original data (raw dtype, special dtypes):\n",
      "\t\t('category', []) : 2 | ['season', 'weather']\n",
      "\t\t('datetime', []) : 1 | ['datetime']\n",
      "\t\t('float', [])    : 3 | ['temp', 'atemp', 'windspeed']\n",
      "\t\t('int', [])      : 6 | ['holiday', 'workingday', 'humidity', 'hour', 'month', ...]\n",
      "\tTypes of features in processed data (raw dtype, special dtypes):\n",
      "\t\t('category', [])             : 2 | ['season', 'weather']\n",
      "\t\t('float', [])                : 3 | ['temp', 'atemp', 'windspeed']\n",
      "\t\t('int', [])                  : 4 | ['humidity', 'hour', 'month', 'day']\n",
      "\t\t('int', ['bool'])            : 2 | ['holiday', 'workingday']\n",
      "\t\t('int', ['datetime_as_int']) : 1 | ['datetime']\n",
      "\t0.2s = Fit runtime\n",
      "\t12 features in original data used to generate 12 features in processed data.\n",
      "\tTrain Data (Processed) Memory Usage: 0.74 MB (0.0% of available memory)\n",
      "Data preprocessing and feature engineering runtime = 0.25s ...\n",
      "AutoGluon will gauge predictive performance using evaluation metric: 'root_mean_squared_error'\n",
      "\tTo change this, specify the eval_metric argument of fit()\n",
      "AutoGluon will fit 2 stack levels (L1 to L2) ...\n",
      "Fitting 11 L1 models ...\n",
      "Hyperparameter tuning model: KNeighborsUnif_BAG_L1 ...\n",
      "Fitted model: KNeighborsUnif_BAG_L1/T0 ...\n",
      "\t-123.9226\t = Validation score   (root_mean_squared_error)\n",
      "\t0.05s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Hyperparameter tuning model: KNeighborsDist_BAG_L1 ...\n",
      "Fitted model: KNeighborsDist_BAG_L1/T0 ...\n",
      "\t-119.3734\t = Validation score   (root_mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Hyperparameter tuning model: LightGBMXT_BAG_L1 ...\n",
      "Warning: Exception caused LightGBMXT_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 103, in status_printer\n",
      "    pbar = IntProgress(min=0, max=total)\n",
      "NameError: name 'IntProgress' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 1153, in _train_single_full\n",
      "    hpo_models, hpo_model_performances, hpo_results = model.hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=hyperparameter_tune_kwargs, **model_fit_kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 179, in _hyperparameter_tune\n",
      "    return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=scheduler_options, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 801, in _hyperparameter_tune\n",
      "    hpo_models, hpo_model_performances, hpo_results = self.model_base.hyperparameter_tune(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 391, in _hyperparameter_tune\n",
      "    scheduler.run()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/scheduler/seq_scheduler.py\", line 152, in run\n",
      "    for i in (tqdm(r) if self.num_trials < 1000 else r):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 213, in __init__\n",
      "    self.fp, total, self.desc, self.ncols)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 111, in status_printer\n",
      "    \"IntProgress not found. Please update jupyter and ipywidgets.\"\n",
      "ImportError: IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "Hyperparameter tuning model: LightGBM_BAG_L1 ...\n",
      "Warning: Exception caused LightGBM_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 103, in status_printer\n",
      "    pbar = IntProgress(min=0, max=total)\n",
      "NameError: name 'IntProgress' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 1153, in _train_single_full\n",
      "    hpo_models, hpo_model_performances, hpo_results = model.hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=hyperparameter_tune_kwargs, **model_fit_kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 179, in _hyperparameter_tune\n",
      "    return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=scheduler_options, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 801, in _hyperparameter_tune\n",
      "    hpo_models, hpo_model_performances, hpo_results = self.model_base.hyperparameter_tune(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 391, in _hyperparameter_tune\n",
      "    scheduler.run()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/scheduler/seq_scheduler.py\", line 152, in run\n",
      "    for i in (tqdm(r) if self.num_trials < 1000 else r):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 213, in __init__\n",
      "    self.fp, total, self.desc, self.ncols)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 111, in status_printer\n",
      "    \"IntProgress not found. Please update jupyter and ipywidgets.\"\n",
      "ImportError: IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "Hyperparameter tuning model: RandomForestMSE_BAG_L1 ...\n",
      "Fitted model: RandomForestMSE_BAG_L1/T0 ...\n",
      "\t-41.3075\t = Validation score   (root_mean_squared_error)\n",
      "\t9.88s\t = Training   runtime\n",
      "\t0.41s\t = Validation runtime\n",
      "Hyperparameter tuning model: CatBoost_BAG_L1 ...\n",
      "Warning: Exception caused CatBoost_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 103, in status_printer\n",
      "    pbar = IntProgress(min=0, max=total)\n",
      "NameError: name 'IntProgress' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 1153, in _train_single_full\n",
      "    hpo_models, hpo_model_performances, hpo_results = model.hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=hyperparameter_tune_kwargs, **model_fit_kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 179, in _hyperparameter_tune\n",
      "    return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=scheduler_options, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 801, in _hyperparameter_tune\n",
      "    hpo_models, hpo_model_performances, hpo_results = self.model_base.hyperparameter_tune(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 887, in _hyperparameter_tune\n",
      "    scheduler.run()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/scheduler/seq_scheduler.py\", line 152, in run\n",
      "    for i in (tqdm(r) if self.num_trials < 1000 else r):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 213, in __init__\n",
      "    self.fp, total, self.desc, self.ncols)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 111, in status_printer\n",
      "    \"IntProgress not found. Please update jupyter and ipywidgets.\"\n",
      "ImportError: IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "Hyperparameter tuning model: ExtraTreesMSE_BAG_L1 ...\n",
      "Fitted model: ExtraTreesMSE_BAG_L1/T0 ...\n",
      "\t-40.8242\t = Validation score   (root_mean_squared_error)\n",
      "\t4.08s\t = Training   runtime\n",
      "\t0.4s\t = Validation runtime\n",
      "Hyperparameter tuning model: NeuralNetFastAI_BAG_L1 ...\n",
      "\tRan out of time, stopping training early. (Stopping on epoch 15)\n",
      "Fitted model: NeuralNetFastAI_BAG_L1/T0 ...\n",
      "\t-83.1454\t = Validation score   (root_mean_squared_error)\n",
      "\t8.36s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Hyperparameter tuning model: XGBoost_BAG_L1 ...\n",
      "Warning: Exception caused XGBoost_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 103, in status_printer\n",
      "    pbar = IntProgress(min=0, max=total)\n",
      "NameError: name 'IntProgress' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 1153, in _train_single_full\n",
      "    hpo_models, hpo_model_performances, hpo_results = model.hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=hyperparameter_tune_kwargs, **model_fit_kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 179, in _hyperparameter_tune\n",
      "    return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=scheduler_options, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 801, in _hyperparameter_tune\n",
      "    hpo_models, hpo_model_performances, hpo_results = self.model_base.hyperparameter_tune(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 887, in _hyperparameter_tune\n",
      "    scheduler.run()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/scheduler/seq_scheduler.py\", line 152, in run\n",
      "    for i in (tqdm(r) if self.num_trials < 1000 else r):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 213, in __init__\n",
      "    self.fp, total, self.desc, self.ncols)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 111, in status_printer\n",
      "    \"IntProgress not found. Please update jupyter and ipywidgets.\"\n",
      "ImportError: IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "Hyperparameter tuning model: NeuralNetMXNet_BAG_L1 ...\n",
      "Warning: Exception caused NeuralNetMXNet_BAG_L1 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 103, in status_printer\n",
      "    pbar = IntProgress(min=0, max=total)\n",
      "NameError: name 'IntProgress' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 1153, in _train_single_full\n",
      "    hpo_models, hpo_model_performances, hpo_results = model.hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=hyperparameter_tune_kwargs, **model_fit_kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 179, in _hyperparameter_tune\n",
      "    return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=scheduler_options, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 801, in _hyperparameter_tune\n",
      "    hpo_models, hpo_model_performances, hpo_results = self.model_base.hyperparameter_tune(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/tabular/models/tabular_nn/tabular_nn_model.py\", line 749, in _hyperparameter_tune\n",
      "    scheduler.run()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/scheduler/seq_scheduler.py\", line 152, in run\n",
      "    for i in (tqdm(r) if self.num_trials < 1000 else r):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 213, in __init__\n",
      "    self.fp, total, self.desc, self.ncols)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 111, in status_printer\n",
      "    \"IntProgress not found. Please update jupyter and ipywidgets.\"\n",
      "ImportError: IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 10.9s of the 959.88s of remaining time.\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 7.94246\tvalid_set's rmse: 36.1445\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-36.0522\t = Validation score   (root_mean_squared_error)\n",
      "\t6.82s\t = Training   runtime\n",
      "\t0.26s\t = Validation runtime\n",
      "Fitting model: KNeighborsUnif_BAG_L1/T0 ... Training model for up to 618.36s of the 951.76s of remaining time.\n",
      "\t-123.9226\t = Validation score   (root_mean_squared_error)\n",
      "\t0.05s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Fitting model: KNeighborsDist_BAG_L1/T0 ... Training model for up to 618.29s of the 951.7s of remaining time.\n",
      "\t-119.3734\t = Validation score   (root_mean_squared_error)\n",
      "\t0.02s\t = Training   runtime\n",
      "\t0.2s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L1/T0 ... Training model for up to 618.23s of the 951.63s of remaining time.\n",
      "\t-41.3075\t = Validation score   (root_mean_squared_error)\n",
      "\t9.88s\t = Training   runtime\n",
      "\t0.41s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L1/T0 ... Training model for up to 618.16s of the 951.57s of remaining time.\n",
      "\t-40.8242\t = Validation score   (root_mean_squared_error)\n",
      "\t4.08s\t = Training   runtime\n",
      "\t0.4s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L1/T0 ... Training model for up to 618.1s of the 951.51s of remaining time.\n",
      "\t-55.895\t = Validation score   (root_mean_squared_error)\n",
      "\t56.19s\t = Training   runtime\n",
      "\t0.24s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 569.68s of the 903.09s of remaining time.\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 8.19209\tvalid_set's rmse: 35.8467\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 7.83644\tvalid_set's rmse: 38.136\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 8.04423\tvalid_set's rmse: 36.4522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 7.95789\tvalid_set's rmse: 38.8895\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\t-37.0626\t = Validation score   (root_mean_squared_error)\n",
      "\t29.02s\t = Training   runtime\n",
      "\t1.19s\t = Validation runtime\n",
      "Repeating k-fold bagging: 2/2\n",
      "Fitting model: NeuralNetFastAI_BAG_L1/T0 ... Training model for up to 543.23s of the 876.64s of remaining time.\n",
      "\t-49.8601\t = Validation score   (root_mean_squared_error)\n",
      "\t115.91s\t = Training   runtime\n",
      "\t0.48s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L1 ... Training model for up to 482.81s of the 816.21s of remaining time.\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 8.14136\tvalid_set's rmse: 38.505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 8.31363\tvalid_set's rmse: 38.0787\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 7.92407\tvalid_set's rmse: 36.4751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1000]\ttrain_set's rmse: 7.86947\tvalid_set's rmse: 34.7026\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\t-36.3289\t = Validation score   (root_mean_squared_error)\n",
      "\t54.29s\t = Training   runtime\n",
      "\t2.25s\t = Validation runtime\n",
      "Completed 2/2 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L2 ... Training model for up to 360.0s of the 786.29s of remaining time.\n",
      "\t-36.235\t = Validation score   (root_mean_squared_error)\n",
      "\t0.61s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "Fitting 9 L2 models ...\n",
      "Hyperparameter tuning model: LightGBMXT_BAG_L2 ...\n",
      "Warning: Exception caused LightGBMXT_BAG_L2 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 103, in status_printer\n",
      "    pbar = IntProgress(min=0, max=total)\n",
      "NameError: name 'IntProgress' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 1153, in _train_single_full\n",
      "    hpo_models, hpo_model_performances, hpo_results = model.hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=hyperparameter_tune_kwargs, **model_fit_kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 179, in _hyperparameter_tune\n",
      "    return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=scheduler_options, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 801, in _hyperparameter_tune\n",
      "    hpo_models, hpo_model_performances, hpo_results = self.model_base.hyperparameter_tune(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 391, in _hyperparameter_tune\n",
      "    scheduler.run()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/scheduler/seq_scheduler.py\", line 152, in run\n",
      "    for i in (tqdm(r) if self.num_trials < 1000 else r):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 213, in __init__\n",
      "    self.fp, total, self.desc, self.ncols)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 111, in status_printer\n",
      "    \"IntProgress not found. Please update jupyter and ipywidgets.\"\n",
      "ImportError: IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "Hyperparameter tuning model: LightGBM_BAG_L2 ...\n",
      "Warning: Exception caused LightGBM_BAG_L2 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 103, in status_printer\n",
      "    pbar = IntProgress(min=0, max=total)\n",
      "NameError: name 'IntProgress' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 1153, in _train_single_full\n",
      "    hpo_models, hpo_model_performances, hpo_results = model.hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=hyperparameter_tune_kwargs, **model_fit_kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 179, in _hyperparameter_tune\n",
      "    return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=scheduler_options, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 801, in _hyperparameter_tune\n",
      "    hpo_models, hpo_model_performances, hpo_results = self.model_base.hyperparameter_tune(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/tabular/models/lgb/lgb_model.py\", line 391, in _hyperparameter_tune\n",
      "    scheduler.run()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/scheduler/seq_scheduler.py\", line 152, in run\n",
      "    for i in (tqdm(r) if self.num_trials < 1000 else r):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 213, in __init__\n",
      "    self.fp, total, self.desc, self.ncols)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 111, in status_printer\n",
      "    \"IntProgress not found. Please update jupyter and ipywidgets.\"\n",
      "ImportError: IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "Hyperparameter tuning model: RandomForestMSE_BAG_L2 ...\n",
      "Fitted model: RandomForestMSE_BAG_L2/T0 ...\n",
      "\t-37.4492\t = Validation score   (root_mean_squared_error)\n",
      "\t22.88s\t = Training   runtime\n",
      "\t0.47s\t = Validation runtime\n",
      "Hyperparameter tuning model: CatBoost_BAG_L2 ...\n",
      "Warning: Exception caused CatBoost_BAG_L2 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 103, in status_printer\n",
      "    pbar = IntProgress(min=0, max=total)\n",
      "NameError: name 'IntProgress' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 1153, in _train_single_full\n",
      "    hpo_models, hpo_model_performances, hpo_results = model.hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=hyperparameter_tune_kwargs, **model_fit_kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 179, in _hyperparameter_tune\n",
      "    return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=scheduler_options, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 801, in _hyperparameter_tune\n",
      "    hpo_models, hpo_model_performances, hpo_results = self.model_base.hyperparameter_tune(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 887, in _hyperparameter_tune\n",
      "    scheduler.run()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/scheduler/seq_scheduler.py\", line 152, in run\n",
      "    for i in (tqdm(r) if self.num_trials < 1000 else r):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 213, in __init__\n",
      "    self.fp, total, self.desc, self.ncols)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 111, in status_printer\n",
      "    \"IntProgress not found. Please update jupyter and ipywidgets.\"\n",
      "ImportError: IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "Hyperparameter tuning model: ExtraTreesMSE_BAG_L2 ...\n",
      "Fitted model: ExtraTreesMSE_BAG_L2/T0 ...\n",
      "\t-36.579\t = Validation score   (root_mean_squared_error)\n",
      "\t6.32s\t = Training   runtime\n",
      "\t0.47s\t = Validation runtime\n",
      "Hyperparameter tuning model: NeuralNetFastAI_BAG_L2 ...\n",
      "Fitted model: NeuralNetFastAI_BAG_L2/T0 ...\n",
      "\t-40.0123\t = Validation score   (root_mean_squared_error)\n",
      "\t10.7s\t = Training   runtime\n",
      "\t0.05s\t = Validation runtime\n",
      "Hyperparameter tuning model: XGBoost_BAG_L2 ...\n",
      "Warning: Exception caused XGBoost_BAG_L2 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 103, in status_printer\n",
      "    pbar = IntProgress(min=0, max=total)\n",
      "NameError: name 'IntProgress' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 1153, in _train_single_full\n",
      "    hpo_models, hpo_model_performances, hpo_results = model.hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=hyperparameter_tune_kwargs, **model_fit_kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 179, in _hyperparameter_tune\n",
      "    return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=scheduler_options, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 801, in _hyperparameter_tune\n",
      "    hpo_models, hpo_model_performances, hpo_results = self.model_base.hyperparameter_tune(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 887, in _hyperparameter_tune\n",
      "    scheduler.run()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/scheduler/seq_scheduler.py\", line 152, in run\n",
      "    for i in (tqdm(r) if self.num_trials < 1000 else r):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 213, in __init__\n",
      "    self.fp, total, self.desc, self.ncols)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 111, in status_printer\n",
      "    \"IntProgress not found. Please update jupyter and ipywidgets.\"\n",
      "ImportError: IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "Hyperparameter tuning model: NeuralNetMXNet_BAG_L2 ...\n",
      "Warning: Exception caused NeuralNetMXNet_BAG_L2 to fail during hyperparameter tuning... Skipping this model.\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 103, in status_printer\n",
      "    pbar = IntProgress(min=0, max=total)\n",
      "NameError: name 'IntProgress' is not defined\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/tabular/trainer/abstract_trainer.py\", line 1153, in _train_single_full\n",
      "    hpo_models, hpo_model_performances, hpo_results = model.hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=hyperparameter_tune_kwargs, **model_fit_kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/stacker_ensemble_model.py\", line 179, in _hyperparameter_tune\n",
      "    return super()._hyperparameter_tune(X=X, y=y, k_fold=k_fold, scheduler_options=scheduler_options, preprocess_kwargs=preprocess_kwargs, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/ensemble/bagged_ensemble_model.py\", line 801, in _hyperparameter_tune\n",
      "    hpo_models, hpo_model_performances, hpo_results = self.model_base.hyperparameter_tune(X=X_fold, y=y_fold, X_val=X_val_fold, y_val=y_val_fold, scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/models/abstract/abstract_model.py\", line 830, in hyperparameter_tune\n",
      "    return self._hyperparameter_tune(scheduler_options=scheduler_options, **kwargs)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/tabular/models/tabular_nn/tabular_nn_model.py\", line 749, in _hyperparameter_tune\n",
      "    scheduler.run()\n",
      "  File \"/usr/local/lib/python3.7/site-packages/autogluon/core/scheduler/seq_scheduler.py\", line 152, in run\n",
      "    for i in (tqdm(r) if self.num_trials < 1000 else r):\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 213, in __init__\n",
      "    self.fp, total, self.desc, self.ncols)\n",
      "  File \"/usr/local/lib/python3.7/site-packages/tqdm/notebook.py\", line 111, in status_printer\n",
      "    \"IntProgress not found. Please update jupyter and ipywidgets.\"\n",
      "ImportError: IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "IntProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 15.71s of the 730.79s of remaining time.\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\t-39.8837\t = Validation score   (root_mean_squared_error)\n",
      "\t2.41s\t = Training   runtime\n",
      "\t0.03s\t = Validation runtime\n",
      "Fitting model: RandomForestMSE_BAG_L2/T0 ... Training model for up to 728.05s of the 728.02s of remaining time.\n",
      "\t-37.4492\t = Validation score   (root_mean_squared_error)\n",
      "\t22.88s\t = Training   runtime\n",
      "\t0.47s\t = Validation runtime\n",
      "Fitting model: ExtraTreesMSE_BAG_L2/T0 ... Training model for up to 727.97s of the 727.94s of remaining time.\n",
      "\t-36.579\t = Validation score   (root_mean_squared_error)\n",
      "\t6.32s\t = Training   runtime\n",
      "\t0.47s\t = Validation runtime\n",
      "Fitting model: NeuralNetFastAI_BAG_L2/T0 ... Training model for up to 727.88s of the 727.85s of remaining time.\n",
      "\t-37.1482\t = Validation score   (root_mean_squared_error)\n",
      "\t59.32s\t = Training   runtime\n",
      "\t0.26s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 678.58s of the 678.56s of remaining time.\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\t-37.3594\t = Validation score   (root_mean_squared_error)\n",
      "\t10.19s\t = Training   runtime\n",
      "\t0.14s\t = Validation runtime\n",
      "Repeating k-fold bagging: 2/2\n",
      "Fitting model: NeuralNetFastAI_BAG_L2/T0 ... Training model for up to 669.93s of the 669.9s of remaining time.\n",
      "\t-36.7572\t = Validation score   (root_mean_squared_error)\n",
      "\t122.49s\t = Training   runtime\n",
      "\t0.53s\t = Validation runtime\n",
      "Fitting model: LightGBMLarge_BAG_L2 ... Training model for up to 605.96s of the 605.93s of remaining time.\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "/usr/local/lib/python3.7/site-packages/lightgbm/engine.py:239: UserWarning: 'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. Pass 'log_evaluation()' callback via 'callbacks' argument instead.\n",
      "  _log_warning(\"'verbose_eval' argument is deprecated and will be removed in a future release of LightGBM. \"\n",
      "\t-37.0138\t = Validation score   (root_mean_squared_error)\n",
      "\t19.67s\t = Training   runtime\n",
      "\t0.25s\t = Validation runtime\n",
      "Completed 2/2 k-fold bagging repeats ...\n",
      "Fitting model: WeightedEnsemble_L3 ... Training model for up to 360.0s of the 595.43s of remaining time.\n",
      "\t-36.2639\t = Validation score   (root_mean_squared_error)\n",
      "\t0.23s\t = Training   runtime\n",
      "\t0.0s\t = Validation runtime\n",
      "AutoGluon training complete, total runtime = 405.1s ...\n",
      "TabularPredictor saved. To load, use: predictor = TabularPredictor.load(\"AutogluonModels/ag-20220202_213429/\")\n"
     ]
    }
   ],
   "source": [
    "num_trials_n = 10  # try at most 5 different hyperparameter configurations for each type of model\n",
    "\n",
    "search_strategy_n = 'auto'  # to tune hyperparameters using random search routine\n",
    "\n",
    "hyperparameter_tune_kwargs_3 = {  # HPO is not performed unless hyperparameter_tune_kwargs is specified\n",
    "    'num_trials': num_trials_n,\n",
    "    'scheduler' : 'local',\n",
    "    'searcher': search_strategy_n\n",
    "}\n",
    "predictor_new_hpo_trial = TabularPredictor(label=\"count\",eval_metric=\"root_mean_squared_error\",problem_type=\"regression\").fit(train_data=train.loc[:, ~train.columns.isin(['casual', 'registered'])], time_limit=1000, \n",
    "                                                                                                                        presets=\"best_quality\",\n",
    "                                                                                                                        hyperparameter_tune_kwargs=hyperparameter_tune_kwargs_3,\n",
    "                                                                                                                       num_bag_folds=5, num_bag_sets=2, num_stack_levels=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 188k/188k [00:00<00:00, 389kB/s]\n",
      "Successfully submitted to Bike Sharing DemandfileName                         date                 description                                    status    publicScore  privateScore  \n",
      "-------------------------------  -------------------  ---------------------------------------------  --------  -----------  ------------  \n",
      "submission_new_hpo_trial.csv     2022-02-02 21:44:37  new features with increased trials             complete  0.49960      0.49960       \n",
      "submission_new_hpo_arg_grid.csv  2022-02-02 21:41:56  new features with increased trials             complete  0.49982      0.49982       \n",
      "submission_new_hpo_arg_grid.csv  2022-02-02 21:32:07  new features with hyperparameters grid search  complete  0.49982      0.49982       \n",
      "submission_new_hpo_arg_grid.csv  2022-02-02 20:41:08  new features with hyperparameters grid search  complete  0.49981      0.49981       \n",
      "submission_new_hpo_arg_grid.csv  2022-02-02 20:40:08  new features with hyperparameters grid search  error     None         None          \n",
      "submission_new_hpo_arg_grid.csv  2022-02-02 20:39:03  new features with hyperparameters grid search  error     None         None          \n",
      "submission_new_hpo_args.csv      2022-02-02 20:21:11  new features with hyperparameters args         complete  0.49958      0.49958       \n"
     ]
    }
   ],
   "source": [
    "predictions_new_hpo_trial = predictor_new_hpo_trial.predict(test)\n",
    "# Set them to zero\n",
    "for index in predictions_new_hpo_trial.keys():\n",
    "    if predictions_new_hpo_trial[index] < 0:\n",
    "        predictions_new_hpo_trial[index]=0\n",
    "        \n",
    "# Initialize dataframe\n",
    "submission_new_hpo_trial = pd.read_csv(\"sampleSubmission.csv\")\n",
    "submission_new_hpo_trial.head()\n",
    "\n",
    "# Same submitting predictions\n",
    "submission_new_hpo_trial[\"count\"] = predictions_new_hpo_trial.values\n",
    "submission_new_hpo_trial.to_csv(\"submission_new_hpo_trial.csv\", index=False)\n",
    "\n",
    "!kaggle competitions submit -c bike-sharing-demand -f submission_new_hpo_trial.csv -m \"new features with increased trials\"\n",
    "!kaggle competitions submissions -c bike-sharing-demand | tail -n +1 | head -n 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fileName                         date                 description                                    status    publicScore  privateScore  \n",
      "-------------------------------  -------------------  ---------------------------------------------  --------  -----------  ------------  \n",
      "submission_new_hpo_trial.csv     2022-02-02 21:44:37  new features with increased trials             complete  0.49960      0.49960       \n",
      "submission_new_hpo_arg_grid.csv  2022-02-02 21:41:56  new features with increased trials             complete  0.49982      0.49982       \n",
      "submission_new_hpo_arg_grid.csv  2022-02-02 21:32:07  new features with hyperparameters grid search  complete  0.49982      0.49982       \n",
      "submission_new_hpo_arg_grid.csv  2022-02-02 20:41:08  new features with hyperparameters grid search  complete  0.49981      0.49981       \n",
      "submission_new_hpo_arg_grid.csv  2022-02-02 20:40:08  new features with hyperparameters grid search  error     None         None          \n",
      "submission_new_hpo_arg_grid.csv  2022-02-02 20:39:03  new features with hyperparameters grid search  error     None         None          \n",
      "submission_new_hpo_args.csv      2022-02-02 20:21:11  new features with hyperparameters args         complete  0.49958      0.49958       \n",
      "submission_new_hpo.csv           2022-02-02 20:08:13  new features with hyperparameters              complete  0.49953      0.49953       \n",
      "submission_new_features.csv      2022-02-02 19:54:58  new features                                   complete  0.55149      0.55149       \n",
      "submission.csv                   2022-02-02 19:29:26  first raw submission                           complete  1.39487      1.39487       \n"
     ]
    }
   ],
   "source": [
    "!kaggle competitions submissions -c bike-sharing-demand | tail -n +1 | head -n 12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### New Score of `0.49960`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Write a Report\n",
    "### Refer to the markdown file for the full report\n",
    "### Creating plots and table for report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Taking the top model score from each training run and creating a line plot to show improvement\n",
    "# You can create these in the notebook and save them to PNG or use some other tool (e.g. google sheets, excel)\n",
    "\n",
    "fig = pd.DataFrame(\n",
    "    {\n",
    "        \"model\": [\"initial\", \"add_features\", \"hpo\",\"hpo_2\",\"grid_search\",\"increased_trials\"],\n",
    "        \"score\": [-115.137560,-35.186484, -36.091199, -36.263891, -36.264821,  -36.0522]\n",
    "    }\n",
    ").plot(x=\"model\", y=\"score\", figsize=(8, 6)).get_figure()\n",
    "fig.savefig('model_train_score.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Take the 3 kaggle scores and creating a line plot to show improvement\n",
    "\n",
    "fig = pd.DataFrame(\n",
    "    {\n",
    "        \"model\": [\"initial\", \"add_features\", \"hpo\",\"hpo_2\",\"grid_search\",\"increased_trials\"],\n",
    "        \"score\": [1.39487,0.55149, 0.49953, 0.49958,0.49982,  0.49960]\n",
    "    }\n",
    ").plot(x=\"model\", y=\"score\", figsize=(8, 6)).get_figure()\n",
    "fig.savefig('model_test_score.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>model</th>\n",
       "      <th>num_bag_folds</th>\n",
       "      <th>num_trials</th>\n",
       "      <th>num_bag_sets</th>\n",
       "      <th>max_stack_levels</th>\n",
       "      <th>score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>initial</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1.39487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>add_features</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.55149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>hpo</td>\n",
       "      <td>10</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0.49953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>hpo_2</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.49958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>grid_search</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.49982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>increased_trials</td>\n",
       "      <td>5</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0.49960</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              model  num_bag_folds  num_trials  num_bag_sets  \\\n",
       "0           initial             10           1             0   \n",
       "1      add_features             10           1             0   \n",
       "2               hpo             10           5             0   \n",
       "3             hpo_2              5           5             2   \n",
       "4       grid_search              5           5             2   \n",
       "5  increased_trials              5          10             2   \n",
       "\n",
       "   max_stack_levels    score  \n",
       "0                 3  1.39487  \n",
       "1                 3  0.55149  \n",
       "2                 3  0.49953  \n",
       "3                 1  0.49958  \n",
       "4                 1  0.49982  \n",
       "5                 1  0.49960  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The 3 hyperparameters we tuned with the kaggle score as the result\n",
    "pd.DataFrame({\n",
    "    \"model\": [\"initial\", \"add_features\", \"hpo\",\"hpo_2\",\"grid_search\",\"increased_trials\"],\n",
    "   \"num_bag_folds\": [10, 10, 10,5,5,5],\n",
    "    \"num_trials\":[1,1,5,5,5,10],\n",
    "    \"num_bag_sets\": [0, 0, 0,2,2,2],\n",
    "    \"max_stack_levels\": [3, 3, 3,1,1,1],\n",
    "    \"score\": [1.39487,0.55149, 0.49953,0.49958,0.49982,0.49960]\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (MXNet 1.8 Python 3.7 CPU Optimized)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/mxnet-1.8-cpu-py37-ubuntu16.04-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
